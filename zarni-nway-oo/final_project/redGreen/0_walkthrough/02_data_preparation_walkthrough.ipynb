{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Pipeline Walkthrough\n",
    "\n",
    "This notebook explains our 4-stage data preparation process that transforms raw scraped articles into training-ready datasets for Myanmar news classification.\n",
    "\n",
    "## Environment Setup\n",
    "**Conda Environment:** nlp  \n",
    "**Purpose:** Clean, preprocess, tokenize, and label Myanmar news articles\n",
    "\n",
    "## Pipeline Overview\n",
    "```\n",
    "Raw JSON → Cleaning → Preprocessing → Tokenization → Labeling → Training Data\n",
    "```\n",
    "\n",
    "Each stage addresses specific challenges in Myanmar NLP and creates progressively refined datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Data Cleaning\n",
    "\n",
    "### Purpose\n",
    "Remove HTML artifacts, normalize Unicode, and standardize text format from scraped content.\n",
    "\n",
    "### Key Challenges in Myanmar Text\n",
    "- Mixed encoding issues from different websites\n",
    "- HTML entities and formatting artifacts\n",
    "- Inconsistent Unicode normalization\n",
    "- Special characters from web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "\n",
    "class MyanmarTextCleaner:\n",
    "    \"\"\"\n",
    "    Specialized text cleaner for Myanmar content.\n",
    "    \n",
    "    Key Functions:\n",
    "    - Unicode normalization (critical for Myanmar script)\n",
    "    - HTML artifact removal\n",
    "    - Whitespace standardization\n",
    "    - Character encoding fixes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Myanmar Unicode ranges\n",
    "        self.myanmar_ranges = [\n",
    "            (0x1000, 0x109F),  # Myanmar script\n",
    "            (0xAA60, 0xAA7F),  # Myanmar Extended-A\n",
    "            (0xA9E0, 0xA9FF),  # Myanmar Extended-B\n",
    "        ]\n",
    "        \n",
    "        # Common HTML entities in Myanmar websites\n",
    "        self.html_entities = {\n",
    "            '&nbsp;': ' ',\n",
    "            '&amp;': '&',\n",
    "            '&lt;': '<',\n",
    "            '&gt;': '>',\n",
    "            '&quot;': '\"',\n",
    "            '&#8217;': \"'\",\n",
    "            '&#8220;': '\"',\n",
    "            '&#8221;': '\"'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Comprehensive text cleaning for Myanmar content.\n",
    "        \n",
    "        Steps:\n",
    "        1. Unicode normalization\n",
    "        2. HTML entity replacement\n",
    "        3. Special character removal\n",
    "        4. Whitespace standardization\n",
    "        5. Myanmar script validation\n",
    "        \n",
    "        Args:\n",
    "            text (str): Raw text from scraping\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned text ready for preprocessing\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Step 1: Unicode normalization (critical for Myanmar)\n",
    "        # NFC = Canonical composition (standard for Myanmar)\n",
    "        text = unicodedata.normalize('NFC', text)\n",
    "        \n",
    "        # Step 2: HTML entity replacement\n",
    "        for entity, replacement in self.html_entities.items():\n",
    "            text = text.replace(entity, replacement)\n",
    "        \n",
    "        # Step 3: Remove HTML tags\n",
    "        text = re.sub(r'<[^>]+>', ' ', text)\n",
    "        \n",
    "        # Step 4: Clean unwanted characters\n",
    "        # Preserve Myanmar script, basic punctuation, spaces\n",
    "        allowed_pattern = r'[^\\u1000-\\u109F\\u0020-\\u007E\\u00A0-\\u00FF\\uAA60-\\uAA7F\\uA9E0-\\uA9FF\\u2000-\\u206F\\u2070-\\u209F\\u20A0-\\u20CF]'\n",
    "        text = re.sub(allowed_pattern, ' ', text)\n",
    "        \n",
    "        # Step 5: Whitespace standardization\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Multiple spaces → single space\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Step 6: Clean excessive punctuation\n",
    "        text = re.sub(r'[,.!?;:]{2,}', '.', text)  # Multiple punctuation → single period\n",
    "        text = re.sub(r'\\.{3,}', '...', text)      # Multiple periods → ellipsis\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def validate_myanmar_content(self, text):\n",
    "        \"\"\"\n",
    "        Validate that text contains meaningful Myanmar content.\n",
    "        \n",
    "        Criteria:\n",
    "        - Contains Myanmar Unicode characters\n",
    "        - Minimum length requirement\n",
    "        - Not mostly punctuation/numbers\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if text is valid Myanmar content\n",
    "        \"\"\"\n",
    "        if len(text) < 20:  # Minimum meaningful length\n",
    "            return False\n",
    "        \n",
    "        # Check for Myanmar script presence\n",
    "        myanmar_chars = sum(1 for char in text \n",
    "                          if any(start <= ord(char) <= end \n",
    "                               for start, end in self.myanmar_ranges))\n",
    "        \n",
    "        # At least 10% Myanmar characters\n",
    "        return myanmar_chars / len(text) > 0.1\n",
    "\n",
    "def clean_article_data(raw_json_path, output_path):\n",
    "    \"\"\"\n",
    "    Clean all articles in a JSON file.\n",
    "    \n",
    "    Process:\n",
    "    1. Load raw scraped data\n",
    "    2. Clean each article's title and content\n",
    "    3. Validate content quality\n",
    "    4. Save cleaned data with metadata\n",
    "    \n",
    "    Args:\n",
    "        raw_json_path (str): Path to raw scraped JSON\n",
    "        output_path (str): Path for cleaned JSON output\n",
    "    \"\"\"\n",
    "    cleaner = MyanmarTextCleaner()\n",
    "    \n",
    "    # Load raw data\n",
    "    with open(raw_json_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    \n",
    "    cleaned_articles = []\n",
    "    stats = {\n",
    "        'input_articles': len(raw_data.get('articles', [])),\n",
    "        'cleaned_articles': 0,\n",
    "        'rejected_articles': 0,\n",
    "        'cleaning_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    for article in raw_data.get('articles', []):\n",
    "        # Clean title and content\n",
    "        cleaned_title = cleaner.clean_text(article.get('title', ''))\n",
    "        cleaned_content = cleaner.clean_text(article.get('content', ''))\n",
    "        \n",
    "        # Validate cleaned content\n",
    "        if (cleaner.validate_myanmar_content(cleaned_title + ' ' + cleaned_content)):\n",
    "            cleaned_article = {\n",
    "                'url': article.get('url', ''),\n",
    "                'title': cleaned_title,\n",
    "                'content': cleaned_content,\n",
    "                'source': article.get('source', ''),\n",
    "                'original_scraped_at': article.get('scraped_at', ''),\n",
    "                'cleaned_at': datetime.now().isoformat(),\n",
    "                'content_length': len(cleaned_content),\n",
    "                'word_count': len(cleaned_content.split())\n",
    "            }\n",
    "            cleaned_articles.append(cleaned_article)\n",
    "            stats['cleaned_articles'] += 1\n",
    "        else:\n",
    "            stats['rejected_articles'] += 1\n",
    "    \n",
    "    # Save cleaned data\n",
    "    output_data = {\n",
    "        'articles': cleaned_articles,\n",
    "        'metadata': {\n",
    "            'stage': 'cleaned',\n",
    "            'source_file': raw_json_path,\n",
    "            'processing_stats': stats\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Cleaning complete:\")\n",
    "    print(f\"   Input: {stats['input_articles']} articles\")\n",
    "    print(f\"   Cleaned: {stats['cleaned_articles']} articles\")\n",
    "    print(f\"   Rejected: {stats['rejected_articles']} articles\")\n",
    "    print(f\"   Success rate: {stats['cleaned_articles']/stats['input_articles']*100:.1f}%\")\n",
    "\n",
    "print(\"✅ Data cleaning implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Text Preprocessing\n",
    "\n",
    "### Purpose\n",
    "Normalize text structure, handle Myanmar-specific formatting, and prepare for tokenization.\n",
    "\n",
    "### Myanmar-Specific Preprocessing\n",
    "- Syllable boundary handling\n",
    "- Number format standardization  \n",
    "- Punctuation normalization\n",
    "- Sentence boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyanmarPreprocessor:\n",
    "    \"\"\"\n",
    "    Myanmar-specific text preprocessing.\n",
    "    \n",
    "    Handles language-specific formatting issues:\n",
    "    - Myanmar number conversion\n",
    "    - Punctuation standardization\n",
    "    - Sentence boundary detection\n",
    "    - Text structure normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Myanmar digits to Arabic digits mapping\n",
    "        self.myanmar_digits = {\n",
    "            '၀': '0', '၁': '1', '၂': '2', '၃': '3', '၄': '4',\n",
    "            '၅': '5', '၆': '6', '၇': '7', '၈': '8', '၉': '9'\n",
    "        }\n",
    "        \n",
    "        # Common Myanmar punctuation standardization\n",
    "        self.punctuation_mapping = {\n",
    "            '။': '.',  # Myanmar period\n",
    "            '၊': ',',  # Myanmar comma\n",
    "            '၏': \"'s\", # Myanmar possessive\n",
    "            '၍': ' and ',  # Myanmar conjunction\n",
    "        }\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Apply Myanmar-specific preprocessing.\n",
    "        \n",
    "        Steps:\n",
    "        1. Convert Myanmar digits to Arabic\n",
    "        2. Standardize punctuation\n",
    "        3. Normalize sentence structure\n",
    "        4. Handle special formatting\n",
    "        \n",
    "        Args:\n",
    "            text (str): Cleaned text\n",
    "        \n",
    "        Returns:\n",
    "            str: Preprocessed text ready for tokenization\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Step 1: Convert Myanmar digits\n",
    "        for myanmar_digit, arabic_digit in self.myanmar_digits.items():\n",
    "            text = text.replace(myanmar_digit, arabic_digit)\n",
    "        \n",
    "        # Step 2: Standardize punctuation\n",
    "        for myanmar_punct, standard_punct in self.punctuation_mapping.items():\n",
    "            text = text.replace(myanmar_punct, standard_punct)\n",
    "        \n",
    "        # Step 3: Sentence boundary standardization\n",
    "        # Ensure proper spacing around sentence boundaries\n",
    "        text = re.sub(r'\\s*\\.\\s*', '. ', text)\n",
    "        text = re.sub(r'\\s*,\\s*', ', ', text)\n",
    "        \n",
    "        # Step 4: Handle quotes and parentheses\n",
    "        text = re.sub(r'\\s*\"\\s*', '\"', text)\n",
    "        text = re.sub(r'\\s*\\(\\s*', ' (', text)\n",
    "        text = re.sub(r'\\s*\\)\\s*', ') ', text)\n",
    "        \n",
    "        # Step 5: Final whitespace cleanup\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def extract_sentences(self, text):\n",
    "        \"\"\"\n",
    "        Split text into sentences for Myanmar content.\n",
    "        \n",
    "        Myanmar sentence boundaries:\n",
    "        - Period (.) \n",
    "        - Question mark (?)\n",
    "        - Exclamation mark (!)\n",
    "        - Myanmar period (။) - converted to .\n",
    "        \n",
    "        Returns:\n",
    "            list: Individual sentences\n",
    "        \"\"\"\n",
    "        # Split on sentence boundaries\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        \n",
    "        # Clean and filter sentences\n",
    "        clean_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 10:  # Minimum meaningful sentence length\n",
    "                clean_sentences.append(sentence)\n",
    "        \n",
    "        return clean_sentences\n",
    "\n",
    "def preprocess_article_data(cleaned_json_path, output_path):\n",
    "    \"\"\"\n",
    "    Preprocess all articles in cleaned JSON.\n",
    "    \n",
    "    Process:\n",
    "    1. Load cleaned data\n",
    "    2. Apply Myanmar-specific preprocessing\n",
    "    3. Extract sentences for better structure\n",
    "    4. Calculate text statistics\n",
    "    5. Save preprocessed data\n",
    "    \"\"\"\n",
    "    preprocessor = MyanmarPreprocessor()\n",
    "    \n",
    "    # Load cleaned data\n",
    "    with open(cleaned_json_path, 'r', encoding='utf-8') as f:\n",
    "        cleaned_data = json.load(f)\n",
    "    \n",
    "    preprocessed_articles = []\n",
    "    stats = {\n",
    "        'input_articles': len(cleaned_data.get('articles', [])),\n",
    "        'preprocessed_articles': 0,\n",
    "        'total_sentences': 0,\n",
    "        'avg_sentences_per_article': 0,\n",
    "        'preprocessing_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    for article in cleaned_data.get('articles', []):\n",
    "        # Preprocess title and content\n",
    "        preprocessed_title = preprocessor.preprocess_text(article.get('title', ''))\n",
    "        preprocessed_content = preprocessor.preprocess_text(article.get('content', ''))\n",
    "        \n",
    "        # Extract sentences for structure analysis\n",
    "        sentences = preprocessor.extract_sentences(preprocessed_content)\n",
    "        \n",
    "        if len(sentences) >= 2:  # Minimum sentences for meaningful article\n",
    "            preprocessed_article = {\n",
    "                'url': article.get('url', ''),\n",
    "                'title': preprocessed_title,\n",
    "                'content': preprocessed_content,\n",
    "                'sentences': sentences,\n",
    "                'source': article.get('source', ''),\n",
    "                'cleaned_at': article.get('cleaned_at', ''),\n",
    "                'preprocessed_at': datetime.now().isoformat(),\n",
    "                'sentence_count': len(sentences),\n",
    "                'character_count': len(preprocessed_content),\n",
    "                'word_count': len(preprocessed_content.split())\n",
    "            }\n",
    "            preprocessed_articles.append(preprocessed_article)\n",
    "            stats['preprocessed_articles'] += 1\n",
    "            stats['total_sentences'] += len(sentences)\n",
    "    \n",
    "    # Calculate averages\n",
    "    if stats['preprocessed_articles'] > 0:\n",
    "        stats['avg_sentences_per_article'] = stats['total_sentences'] / stats['preprocessed_articles']\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    output_data = {\n",
    "        'articles': preprocessed_articles,\n",
    "        'metadata': {\n",
    "            'stage': 'preprocessed',\n",
    "            'source_file': cleaned_json_path,\n",
    "            'processing_stats': stats\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Preprocessing complete:\")\n",
    "    print(f\"   Input: {stats['input_articles']} articles\")\n",
    "    print(f\"   Processed: {stats['preprocessed_articles']} articles\")\n",
    "    print(f\"   Total sentences: {stats['total_sentences']}\")\n",
    "    print(f\"   Avg sentences/article: {stats['avg_sentences_per_article']:.1f}\")\n",
    "\n",
    "print(\"✅ Text preprocessing implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Myanmar Text Tokenization\n",
    "\n",
    "### Purpose\n",
    "Split Myanmar text into meaningful units (words/syllables) using specialized Myanmar NLP tools.\n",
    "\n",
    "### Why Myanmar Tokenization is Challenging\n",
    "- No spaces between words in traditional Myanmar writing\n",
    "- Context-dependent word boundaries\n",
    "- Compound words and syllable structures\n",
    "- Mixed script content (Myanmar + English + numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This uses our custom Myanmar tokenizer (MyWord)\n",
    "# In production, we import from: from myWord import MyWord\n",
    "\n",
    "class MyanmarTokenizer:\n",
    "    \"\"\"\n",
    "    Myanmar text tokenization using MyWord library.\n",
    "    \n",
    "    MyWord provides:\n",
    "    - Word-level segmentation\n",
    "    - Syllable-level segmentation  \n",
    "    - N-gram phrase detection\n",
    "    - Statistical language models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize Myanmar tokenizer with statistical models.\"\"\"\n",
    "        try:\n",
    "            # Import MyWord tokenizer (custom Myanmar NLP library)\n",
    "            from myword import MyWord\n",
    "            self.tokenizer = MyWord()\n",
    "            self.initialized = True\n",
    "            print(\"✅ MyWord tokenizer initialized successfully\")\n",
    "        except ImportError as e:\n",
    "            print(f\"⚠️  MyWord not available: {e}\")\n",
    "            print(\"   Falling back to basic tokenization\")\n",
    "            self.tokenizer = None\n",
    "            self.initialized = False\n",
    "    \n",
    "    def tokenize_text(self, text, level='word'):\n",
    "        \"\"\"\n",
    "        Tokenize Myanmar text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Preprocessed Myanmar text\n",
    "            level (str): 'word' or 'syllable' tokenization\n",
    "        \n",
    "        Returns:\n",
    "            list: Tokens (words or syllables)\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        if self.tokenizer and self.initialized:\n",
    "            try:\n",
    "                # Use MyWord for sophisticated tokenization\n",
    "                if level == 'syllable':\n",
    "                    tokens = self.tokenizer.syllable_segment(text)\n",
    "                else:\n",
    "                    tokens = self.tokenizer.segment(text)  # Word-level by default\n",
    "                \n",
    "                return [token.strip() for token in tokens if token.strip()]\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  MyWord tokenization failed: {e}\")\n",
    "                return self._fallback_tokenize(text)\n",
    "        else:\n",
    "            # Fallback tokenization\n",
    "            return self._fallback_tokenize(text)\n",
    "    \n",
    "    def _fallback_tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Basic fallback tokenization for Myanmar text.\n",
    "        \n",
    "        Strategy:\n",
    "        - Split on spaces (works for mixed content)\n",
    "        - Handle punctuation boundaries\n",
    "        - Basic syllable boundaries for Myanmar script\n",
    "        \"\"\"\n",
    "        # Basic space-based splitting\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Further split on punctuation\n",
    "        refined_tokens = []\n",
    "        for token in tokens:\n",
    "            # Split punctuation while preserving Myanmar text\n",
    "            subtokens = re.findall(r'[\\u1000-\\u109F]+|[a-zA-Z0-9]+|[.,!?]', token)\n",
    "            refined_tokens.extend(subtokens)\n",
    "        \n",
    "        return [token for token in refined_tokens if token.strip()]\n",
    "    \n",
    "    def calculate_token_statistics(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculate useful statistics about tokenized content.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Token statistics for quality assessment\n",
    "        \"\"\"\n",
    "        if not tokens:\n",
    "            return {'token_count': 0, 'unique_tokens': 0, 'avg_token_length': 0}\n",
    "        \n",
    "        # Basic statistics\n",
    "        token_count = len(tokens)\n",
    "        unique_tokens = len(set(tokens))\n",
    "        avg_token_length = sum(len(token) for token in tokens) / len(tokens)\n",
    "        \n",
    "        # Myanmar-specific statistics\n",
    "        myanmar_tokens = [token for token in tokens \n",
    "                         if re.search(r'[\\u1000-\\u109F]', token)]\n",
    "        english_tokens = [token for token in tokens \n",
    "                         if re.search(r'[a-zA-Z]', token)]\n",
    "        number_tokens = [token for token in tokens \n",
    "                        if re.search(r'[0-9]', token)]\n",
    "        \n",
    "        return {\n",
    "            'token_count': token_count,\n",
    "            'unique_tokens': unique_tokens,\n",
    "            'type_token_ratio': unique_tokens / token_count if token_count > 0 else 0,\n",
    "            'avg_token_length': avg_token_length,\n",
    "            'myanmar_tokens': len(myanmar_tokens),\n",
    "            'english_tokens': len(english_tokens),\n",
    "            'number_tokens': len(number_tokens),\n",
    "            'myanmar_ratio': len(myanmar_tokens) / token_count if token_count > 0 else 0\n",
    "        }\n",
    "\n",
    "def tokenize_article_data(preprocessed_json_path, output_path):\n",
    "    \"\"\"\n",
    "    Tokenize all articles in preprocessed JSON.\n",
    "    \n",
    "    Process:\n",
    "    1. Load preprocessed data\n",
    "    2. Tokenize title and content\n",
    "    3. Calculate token statistics\n",
    "    4. Preserve sentence structure\n",
    "    5. Save tokenized data\n",
    "    \"\"\"\n",
    "    tokenizer = MyanmarTokenizer()\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    with open(preprocessed_json_path, 'r', encoding='utf-8') as f:\n",
    "        preprocessed_data = json.load(f)\n",
    "    \n",
    "    tokenized_articles = []\n",
    "    stats = {\n",
    "        'input_articles': len(preprocessed_data.get('articles', [])),\n",
    "        'tokenized_articles': 0,\n",
    "        'total_tokens': 0,\n",
    "        'total_unique_tokens': set(),\n",
    "        'avg_tokens_per_article': 0,\n",
    "        'tokenization_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    for article in preprocessed_data.get('articles', []):\n",
    "        # Tokenize title and content\n",
    "        title_tokens = tokenizer.tokenize_text(article.get('title', ''))\n",
    "        content_tokens = tokenizer.tokenize_text(article.get('content', ''))\n",
    "        \n",
    "        # Tokenize sentences separately for structure preservation\n",
    "        tokenized_sentences = []\n",
    "        for sentence in article.get('sentences', []):\n",
    "            sentence_tokens = tokenizer.tokenize_text(sentence)\n",
    "            if sentence_tokens:  # Only keep non-empty tokenized sentences\n",
    "                tokenized_sentences.append(sentence_tokens)\n",
    "        \n",
    "        # Calculate token statistics\n",
    "        all_tokens = title_tokens + content_tokens\n",
    "        token_stats = tokenizer.calculate_token_statistics(all_tokens)\n",
    "        \n",
    "        if token_stats['token_count'] >= 10:  # Minimum tokens for meaningful content\n",
    "            tokenized_article = {\n",
    "                'url': article.get('url', ''),\n",
    "                'title': article.get('title', ''),\n",
    "                'title_tokens': title_tokens,\n",
    "                'content': article.get('content', ''),\n",
    "                'content_tokens': content_tokens,\n",
    "                'tokenized_sentences': tokenized_sentences,\n",
    "                'all_tokens': all_tokens,\n",
    "                'source': article.get('source', ''),\n",
    "                'preprocessed_at': article.get('preprocessed_at', ''),\n",
    "                'tokenized_at': datetime.now().isoformat(),\n",
    "                'token_stats': token_stats\n",
    "            }\n",
    "            tokenized_articles.append(tokenized_article)\n",
    "            stats['tokenized_articles'] += 1\n",
    "            stats['total_tokens'] += token_stats['token_count']\n",
    "            stats['total_unique_tokens'].update(all_tokens)\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    if stats['tokenized_articles'] > 0:\n",
    "        stats['avg_tokens_per_article'] = stats['total_tokens'] / stats['tokenized_articles']\n",
    "    stats['vocabulary_size'] = len(stats['total_unique_tokens'])\n",
    "    stats['total_unique_tokens'] = len(stats['total_unique_tokens'])  # Convert set to count\n",
    "    \n",
    "    # Save tokenized data\n",
    "    output_data = {\n",
    "        'articles': tokenized_articles,\n",
    "        'metadata': {\n",
    "            'stage': 'tokenized',\n",
    "            'source_file': preprocessed_json_path,\n",
    "            'processing_stats': stats,\n",
    "            'tokenizer_used': 'MyWord' if tokenizer.initialized else 'fallback'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"✅ Tokenization complete:\")\n",
    "    print(f\"   Input: {stats['input_articles']} articles\")\n",
    "    print(f\"   Tokenized: {stats['tokenized_articles']} articles\")\n",
    "    print(f\"   Total tokens: {stats['total_tokens']}\")\n",
    "    print(f\"   Vocabulary size: {stats['vocabulary_size']}\")\n",
    "    print(f\"   Avg tokens/article: {stats['avg_tokens_per_article']:.1f}\")\n",
    "\n",
    "print(\"✅ Myanmar tokenization implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4: Data Labeling\n",
    "\n",
    "### Purpose\n",
    "Assign sentiment labels based on news source bias for supervised learning.\n",
    "\n",
    "### Labeling Strategy\n",
    "- **DVB News** → \"neutral\" (opposition/exile perspective)\n",
    "- **Myawady News** → \"green\" (government-friendly)\n",
    "- **Khitthit News** → \"red\" (critical/independent)\n",
    "\n",
    "This creates a balanced 3-class dataset representing different political perspectives in Myanmar media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MyanmarNewsLabeler:\n",
    "    \"\"\"\n",
    "    Label Myanmar news articles based on source bias.\n",
    "    \n",
    "    Label Mapping:\n",
    "    - dvb → 'neutral' (0) - Opposition/exile media\n",
    "    - myawady → 'green' (2) - Government-aligned\n",
    "    - khitthit → 'red' (1) - Independent/critical\n",
    "    \n",
    "    This creates training labels for political sentiment classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Source to label mapping\n",
    "        self.source_labels = {\n",
    "            'dvb': {'label': 'neutral', 'numeric': 0, 'description': 'Opposition/neutral perspective'},\n",
    "            'myawady': {'label': 'green', 'numeric': 2, 'description': 'Government-friendly perspective'},\n",
    "            'khitthit': {'label': 'red', 'numeric': 1, 'description': 'Independent/critical perspective'}\n",
    "        }\n",
    "        \n",
    "        # Reverse mapping for analysis\n",
    "        self.label_descriptions = {\n",
    "            0: 'neutral', 1: 'red', 2: 'green'\n",
    "        }\n",
    "    \n",
    "    def label_article(self, article_data):\n",
    "        \"\"\"\n",
    "        Assign label to article based on source.\n",
    "        \n",
    "        Args:\n",
    "            article_data (dict): Tokenized article data\n",
    "        \n",
    "        Returns:\n",
    "            dict: Article data with labels added\n",
    "        \"\"\"\n",
    "        source = article_data.get('source', '').lower()\n",
    "        \n",
    "        if source in self.source_labels:\n",
    "            label_info = self.source_labels[source]\n",
    "            article_data.update({\n",
    "                'label': label_info['label'],\n",
    "                'label_numeric': label_info['numeric'],\n",
    "                'label_description': label_info['description'],\n",
    "                'labeled_at': datetime.now().isoformat()\n",
    "            })\n",
    "        else:\n",
    "            # Unknown source - assign neutral by default\n",
    "            article_data.update({\n",
    "                'label': 'neutral',\n",
    "                'label_numeric': 0,\n",
    "                'label_description': 'Unknown source - neutral default',\n",
    "                'labeled_at': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        return article_data\n",
    "    \n",
    "    def create_training_dataset(self, labeled_articles):\n",
    "        \"\"\"\n",
    "        Convert labeled articles to training-ready format.\n",
    "        \n",
    "        Creates both token-based and text-based datasets:\n",
    "        - Token dataset: For models that need pre-tokenized input\n",
    "        - Text dataset: For models with built-in tokenization\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (DataFrame, token_dataset_dict)\n",
    "        \"\"\"\n",
    "        training_rows = []\n",
    "        token_dataset = {'texts': [], 'tokens': [], 'labels': []}\n",
    "        \n",
    "        for article in labeled_articles:\n",
    "            # Combine title and content for full text\n",
    "            full_text = f\"{article.get('title', '')} {article.get('content', '')}\"\n",
    "            combined_tokens = article.get('all_tokens', [])\n",
    "            \n",
    "            if len(combined_tokens) >= 10:  # Minimum tokens for training\n",
    "                # DataFrame row (for pandas-based analysis)\n",
    "                row = {\n",
    "                    'url': article.get('url', ''),\n",
    "                    'title': article.get('title', ''),\n",
    "                    'content': article.get('content', ''),\n",
    "                    'full_text': full_text.strip(),\n",
    "                    'tokens': ' '.join(combined_tokens),  # Space-separated tokens\n",
    "                    'token_count': len(combined_tokens),\n",
    "                    'source': article.get('source', ''),\n",
    "                    'label': article.get('label', 'neutral'),\n",
    "                    'label_numeric': article.get('label_numeric', 0),\n",
    "                    'character_count': len(full_text),\n",
    "                    'word_count': len(full_text.split())\n",
    "                }\n",
    "                training_rows.append(row)\n",
    "                \n",
    "                # Token-based dataset (for advanced models)\n",
    "                token_dataset['texts'].append(full_text)\n",
    "                token_dataset['tokens'].append(combined_tokens)\n",
    "                token_dataset['labels'].append(article.get('label_numeric', 0))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(training_rows)\n",
    "        \n",
    "        return df, token_dataset\n",
    "    \n",
    "    def analyze_label_distribution(self, df):\n",
    "        \"\"\"\n",
    "        Analyze the distribution of labels in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Label distribution statistics\n",
    "        \"\"\"\n",
    "        label_counts = df['label'].value_counts()\n",
    "        total_articles = len(df)\n",
    "        \n",
    "        distribution = {}\n",
    "        for label, count in label_counts.items():\n",
    "            distribution[label] = {\n",
    "                'count': count,\n",
    "                'percentage': (count / total_articles) * 100,\n",
    "                'description': self.source_labels.get(\n",
    "                    [k for k, v in self.source_labels.items() if v['label'] == label][0], \n",
    "                    {}\n",
    "                ).get('description', 'Unknown')\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'total_articles': total_articles,\n",
    "            'label_distribution': distribution,\n",
    "            'is_balanced': max(label_counts) / min(label_counts) <= 2.0  # Within 2x ratio\n",
    "        }\n",
    "\n",
    "def create_labeled_dataset(tokenized_json_files, output_csv_path):\n",
    "    \"\"\"\n",
    "    Process multiple tokenized JSON files and create combined labeled dataset.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_json_files (list): Paths to tokenized JSON files\n",
    "        output_csv_path (str): Path for output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (DataFrame, statistics)\n",
    "    \"\"\"\n",
    "    labeler = MyanmarNewsLabeler()\n",
    "    all_labeled_articles = []\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in tokenized_json_files:\n",
    "        print(f\"📂 Processing: {json_file}\")\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            tokenized_data = json.load(f)\n",
    "        \n",
    "        # Label all articles in this file\n",
    "        for article in tokenized_data.get('articles', []):\n",
    "            labeled_article = labeler.label_article(article)\n",
    "            all_labeled_articles.append(labeled_article)\n",
    "    \n",
    "    print(f\"\\n📊 Total articles collected: {len(all_labeled_articles)}\")\n",
    "    \n",
    "    # Create training dataset\n",
    "    df, token_dataset = labeler.create_training_dataset(all_labeled_articles)\n",
    "    \n",
    "    # Analyze distribution\n",
    "    distribution_stats = labeler.analyze_label_distribution(df)\n",
    "    \n",
    "    # Save combined dataset\n",
    "    df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    # Create train/validation split\n",
    "    train_df, val_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    \n",
    "    # Save splits\n",
    "    base_path = output_csv_path.replace('.csv', '')\n",
    "    train_df.to_csv(f\"{base_path}_train.csv\", index=False, encoding='utf-8')\n",
    "    val_df.to_csv(f\"{base_path}_val.csv\", index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n✅ Dataset creation complete:\")\n",
    "    print(f\"   Full dataset: {output_csv_path} ({len(df)} articles)\")\n",
    "    print(f\"   Training set: {base_path}_train.csv ({len(train_df)} articles)\")\n",
    "    print(f\"   Validation set: {base_path}_val.csv ({len(val_df)} articles)\")\n",
    "    \n",
    "    print(f\"\\n📊 Label Distribution:\")\n",
    "    for label, stats in distribution_stats['label_distribution'].items():\n",
    "        print(f\"   {label}: {stats['count']} articles ({stats['percentage']:.1f}%)\")\n",
    "    \n",
    "    return df, distribution_stats\n",
    "\n",
    "print(\"✅ Data labeling implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Integration and Data Flow\n",
    "\n",
    "### Complete Data Flow\n",
    "```\n",
    "Raw Scraped JSON\n",
    "      ↓\n",
    "[1] Cleaning: Unicode normalization, HTML removal\n",
    "      ↓\n",
    "[2] Preprocessing: Myanmar-specific text formatting\n",
    "      ↓  \n",
    "[3] Tokenization: MyWord segmentation → tokens\n",
    "      ↓\n",
    "[4] Labeling: Source-based sentiment labels\n",
    "      ↓\n",
    "Training CSV (ready for BiLSTM)\n",
    "```\n",
    "\n",
    "### Quality Controls at Each Stage\n",
    "- **Cleaning:** Content length, Myanmar script validation\n",
    "- **Preprocessing:** Sentence structure, text normalization  \n",
    "- **Tokenization:** Token count thresholds, vocabulary analysis\n",
    "- **Labeling:** Balanced dataset creation, train/val splits\n",
    "\n",
    "### Output Format for Training\n",
    "Final CSV contains:\n",
    "- `full_text`: Complete article (title + content)\n",
    "- `tokens`: Space-separated Myanmar tokens\n",
    "- `label_numeric`: 0=neutral, 1=red, 2=green\n",
    "- `token_count`: Number of tokens (for model input sizing)\n",
    "- Source and metadata for analysis\n",
    "\n",
    "This structured approach ensures high-quality training data for our BiLSTM Myanmar news classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n   "language": "python",\n   "name": "python3"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"version\": \"3.8.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}