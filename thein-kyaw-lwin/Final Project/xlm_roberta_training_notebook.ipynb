{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":""},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12803787,"sourceType":"datasetVersion","datasetId":8095575}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"id":"aec2cd70","cell_type":"markdown","source":"# ğŸŒ Myanmar Address Geocoding with XLM-RoBERTa\n\n## Project Overview\nTransform Myanmar address text into GPS coordinates using state-of-the-art XLM-RoBERTa transformer model.\n\n**Key Features:**\n- **XLM-RoBERTa**: Superior multilingual understanding (270M parameters)\n- **Full Dataset**: 610K+ Myanmar addresses with GPS coordinates\n- **Geographic Loss**: Haversine distance for real-world accuracy\n- **Production Ready**: Kaggle optimized with 4-6 hour training time\n\n**Architecture:**\n```\nMyanmar Text â†’ XLM-RoBERTa Encoder â†’ Geographic Regression Head â†’ GPS Coordinates\n```","metadata":{}},{"id":"9e99ca58","cell_type":"code","source":"# ğŸš€ Configuration & Setup\nimport os\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Configuration\nCONFIG = {\n    'model_name': 'xlm-roberta-base',\n    'max_length': 128,\n    'batch_size': 64,\n    'epochs': 2,\n    'learning_rate': 2e-5,\n    'weight_decay': 0.01,\n    'warmup_steps': 1000,\n    'dropout_rate': 0.1,\n}\n\n# Dataset path\nDATASET_PATH = '/kaggle/input/a2c-address/master_dataset_myanmar_address.csv'\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ–¥ï¸ Using device: {device}\")\nprint(f\"ğŸ“Š Configuration: {CONFIG}\")\nprint(\"âœ… Setup complete!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"199ff957","cell_type":"code","source":"# ğŸ“Š Data Loading & Preprocessing\ndef load_and_clean_data(file_path):\n    \"\"\"Load and clean Myanmar address dataset\"\"\"\n    print(\"ğŸ“‚ Loading dataset...\")\n    \n    # Load dataset\n    df = pd.read_csv(file_path)\n    print(f\"âœ… Loaded {len(df):,} records\")\n    \n    # Standardize column names\n    if 'lat' in df.columns:\n        df = df.rename(columns={'lat': 'latitude'})\n    if 'long' in df.columns:\n        df = df.rename(columns={'long': 'longitude'})\n    \n    # Clean data\n    initial_count = len(df)\n    df = df.dropna(subset=['address', 'latitude', 'longitude']).copy()\n    \n    print(f\"ğŸ§¹ Cleaned data: {len(df):,} records ({len(df)/initial_count*100:.1f}% retained)\")\n    print(f\"ğŸ“ Coordinate ranges:\")\n    print(f\"   Latitude: {df['latitude'].min():.3f} to {df['latitude'].max():.3f}\")\n    print(f\"   Longitude: {df['longitude'].min():.3f} to {df['longitude'].max():.3f}\")\n    \n    return df\n\n# Load the data\ndf = load_and_clean_data(DATASET_PATH)","metadata":{},"outputs":[],"execution_count":null},{"id":"b5758a9c","cell_type":"code","source":"# ğŸ¯ Coordinate Normalization & Data Split\nprint(\"ğŸ”§ Normalizing coordinates...\")\n\n# Create scalers\nlat_scaler = MinMaxScaler()\nlon_scaler = MinMaxScaler()\n\n# Normalize coordinates to [0,1] range\ndf['lat_normalized'] = lat_scaler.fit_transform(df[['latitude']])\ndf['lon_normalized'] = lon_scaler.fit_transform(df[['longitude']])\n\nprint(f\"âœ… Coordinates normalized to [0,1] range\")\n\n# Train/validation split\nprint(\"ğŸ“‹ Creating train/validation split (80%/20%)...\")\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True\n)\n\nprint(f\"ğŸ“Š Dataset split:\")\nprint(f\"   Training: {len(train_df):,} samples\")\nprint(f\"   Validation: {len(val_df):,} samples\")\nprint(f\"   Total: {len(df):,} samples\")","metadata":{},"outputs":[],"execution_count":null},{"id":"1af982a7","cell_type":"code","source":"# ğŸ¤— XLM-RoBERTa Tokenizer Setup\nprint(\"ğŸ”¤ Loading XLM-RoBERTa tokenizer...\")\nprint(f\"ğŸŒ Model: {CONFIG['model_name']} (270M parameters)\")\nprint(\"âœ¨ Features: 100+ languages, Myanmar script support\")\n\ntokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n\n# Test with Myanmar text\ntest_text = \"á€›á€”á€ºá€€á€¯á€”á€ºá€™á€¼á€­á€¯á€· á€á€¬á€™á€½á€± á€™á€¼á€­á€¯á€·á€”á€šá€º\"\ntokens = tokenizer.encode(test_text, add_special_tokens=True)\n\nprint(f\"âœ… Tokenizer loaded successfully!\")\nprint(f\"ğŸ“ Test: '{test_text}' â†’ {len(tokens)} tokens\")\nprint(f\"ğŸ”¢ Vocabulary size: {tokenizer.vocab_size:,}\")\nprint(f\"ğŸ“ Max length: {CONFIG['max_length']} tokens\")","metadata":{},"outputs":[],"execution_count":null},{"id":"b74eafff","cell_type":"code","source":"# ğŸ“¦ Dataset Class\nclass MyanmarAddressDataset(torch.utils.data.Dataset):\n    \"\"\"Myanmar Address Dataset for XLM-RoBERTa training\"\"\"\n    \n    def __init__(self, dataframe, tokenizer, max_length=128):\n        self.data = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        # Get address text and coordinates\n        address = str(self.data.iloc[idx]['address'])\n        lat = float(self.data.iloc[idx]['lat_normalized'])\n        lon = float(self.data.iloc[idx]['lon_normalized'])\n        \n        # Tokenize address\n        encoding = self.tokenizer(\n            address,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].squeeze(),\n            'attention_mask': encoding['attention_mask'].squeeze(),\n            'coordinates': torch.tensor([lat, lon], dtype=torch.float)\n        }\n\n# Create datasets\ntrain_dataset = MyanmarAddressDataset(train_df, tokenizer, CONFIG['max_length'])\nval_dataset = MyanmarAddressDataset(val_df, tokenizer, CONFIG['max_length'])\n\nprint(f\"ğŸ“¦ Datasets created:\")\nprint(f\"   Training: {len(train_dataset):,} samples\")\nprint(f\"   Validation: {len(val_dataset):,} samples\")","metadata":{},"outputs":[],"execution_count":null},{"id":"0c46f88a","cell_type":"code","source":"# ğŸŒ Haversine Distance Loss Function\ndef haversine_distance_loss(predictions, targets):\n    \"\"\"\n    Calculate Haversine distance loss for geographic accuracy\n    \n    Args:\n        predictions: Model predictions [batch_size, 2] in [0,1] range\n        targets: Ground truth coordinates [batch_size, 2] in [0,1] range\n    \n    Returns:\n        Mean Haversine distance in kilometers\n    \"\"\"\n    # Convert normalized coordinates back to actual lat/lon\n    # Myanmar bounds: lat(9.5-28.5), lon(92.0-101.5)\n    pred_lat = predictions[:, 0] * (28.5 - 9.5) + 9.5\n    pred_lon = predictions[:, 1] * (101.5 - 92.0) + 92.0\n    target_lat = targets[:, 0] * (28.5 - 9.5) + 9.5\n    target_lon = targets[:, 1] * (101.5 - 92.0) + 92.0\n    \n    # Convert to radians\n    pred_lat_rad = torch.deg2rad(pred_lat)\n    pred_lon_rad = torch.deg2rad(pred_lon)\n    target_lat_rad = torch.deg2rad(target_lat)\n    target_lon_rad = torch.deg2rad(target_lon)\n    \n    # Haversine formula\n    dlat = target_lat_rad - pred_lat_rad\n    dlon = target_lon_rad - pred_lon_rad\n    \n    a = torch.sin(dlat/2)**2 + torch.cos(pred_lat_rad) * torch.cos(target_lat_rad) * torch.sin(dlon/2)**2\n    c = 2 * torch.asin(torch.sqrt(torch.clamp(a, 0., 1.)))\n    \n    # Earth radius in kilometers\n    R = 6371.0\n    distance = R * c\n    \n    return torch.mean(distance)\n\nprint(\"ğŸŒ Haversine distance loss function ready!\")\nprint(\"âœ… Optimizes for real-world geographic accuracy\")","metadata":{},"outputs":[],"execution_count":null},{"id":"d4c39b98","cell_type":"code","source":"# ğŸ§  XLM-RoBERTa Geocoding Model\nclass MyanmarXLMRobertaGeocoder(nn.Module):\n    \"\"\"\n    XLM-RoBERTa-based geocoding model for Myanmar addresses\n    \n    Architecture:\n    - XLM-RoBERTa encoder (270M parameters)\n    - 4-layer regression head with dropout\n    - Sigmoid output constraint for [0,1] coordinates\n    \"\"\"\n    \n    def __init__(self, model_name, dropout_rate=0.1):\n        super().__init__()\n        \n        # Load pre-trained XLM-RoBERTa\n        self.xlm_roberta = AutoModel.from_pretrained(model_name)\n        hidden_size = self.xlm_roberta.config.hidden_size  # 768\n        \n        # Regression head for coordinate prediction\n        self.dropout = nn.Dropout(dropout_rate)\n        self.coordinate_regressor = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate * 3),  # 0.3 for stronger regularization\n            \n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate * 3),\n            \n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate * 1.5),  # 0.15\n            \n            nn.Linear(128, 2),\n            nn.Sigmoid()  # Constrain to [0,1] range\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        # Get XLM-RoBERTa embeddings\n        outputs = self.xlm_roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        # Use [CLS] token representation\n        cls_output = outputs.last_hidden_state[:, 0, :]\n        cls_output = self.dropout(cls_output)\n        \n        # Predict coordinates\n        coordinates = self.coordinate_regressor(cls_output)\n        \n        return coordinates\n\n# Initialize model\nmodel = MyanmarXLMRobertaGeocoder(\n    model_name=CONFIG['model_name'],\n    dropout_rate=CONFIG['dropout_rate']\n)\nmodel = model.to(device)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"ğŸ§  XLM-RoBERTa Geocoding Model initialized!\")\nprint(f\"ğŸ“Š Total parameters: {total_params:,}\")\nprint(f\"ğŸ¯ Trainable parameters: {trainable_params:,}\")\nprint(f\"ğŸ’¾ Model size: ~{total_params * 4 / 1024**2:.1f} MB\")\nprint(f\"ğŸ–¥ï¸ Device: {device}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"6fe5e1c2","cell_type":"code","source":"# ğŸ”„ Data Loaders\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    batch_size=CONFIG['batch_size'],\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"ğŸ”„ Data loaders created:\")\nprint(f\"   Training batches: {len(train_loader):,}\")\nprint(f\"   Validation batches: {len(val_loader):,}\")\nprint(f\"   Batch size: {CONFIG['batch_size']}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"ba2af5a5","cell_type":"code","source":"# âš™ï¸ Optimizer & Scheduler Setup\nprint(\"âš™ï¸ Setting up optimizer and scheduler...\")\n\n# Differential learning rates\nxlm_roberta_params = []\nregression_params = []\n\nfor name, param in model.named_parameters():\n    if 'xlm_roberta' in name:\n        xlm_roberta_params.append(param)\n    else:\n        regression_params.append(param)\n\n# AdamW optimizer with differential learning rates\noptimizer = AdamW([\n    {'params': xlm_roberta_params, 'lr': CONFIG['learning_rate']},\n    {'params': regression_params, 'lr': CONFIG['learning_rate'] * 10}  # Higher LR for new layers\n], weight_decay=CONFIG['weight_decay'])\n\n# Calculate total training steps\ntotal_steps = len(train_loader) * CONFIG['epochs']\n\n# Linear warmup scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=CONFIG['warmup_steps'],\n    num_training_steps=total_steps\n)\n\nprint(f\"âœ… Optimizer configured:\")\nprint(f\"   XLM-RoBERTa LR: {CONFIG['learning_rate']}\")\nprint(f\"   Regression head LR: {CONFIG['learning_rate'] * 10}\")\nprint(f\"   Weight decay: {CONFIG['weight_decay']}\")\nprint(f\"   Total steps: {total_steps:,}\")\nprint(f\"   Warmup steps: {CONFIG['warmup_steps']:,}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"3f517d4b","cell_type":"code","source":"# ğŸš€ Enhanced Training Functions with Progress Monitoring\nimport time\n\ndef train_epoch(model, train_loader, optimizer, scheduler, device, epoch_num):\n    \"\"\"Train for one epoch with detailed progress tracking\"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = len(train_loader)\n    start_time = time.time()\n    \n    # Enhanced progress bar with more details\n    progress_bar = tqdm(\n        train_loader, \n        desc=f\"ğŸš€ Epoch {epoch_num} Training\",\n        ncols=120,\n        leave=True,\n        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}] {postfix}'\n    )\n    \n    for batch_idx, batch in enumerate(progress_bar):\n        batch_start_time = time.time()\n        \n        # Move to device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        coordinates = batch['coordinates'].to(device)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        predictions = model(input_ids, attention_mask)\n        \n        # Calculate loss\n        loss = haversine_distance_loss(predictions, coordinates)\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        \n        total_loss += loss.item()\n        batch_time = time.time() - batch_start_time\n        \n        # Calculate ETA for current epoch\n        if batch_idx > 0:\n            elapsed_time = time.time() - start_time\n            avg_batch_time = elapsed_time / (batch_idx + 1)\n            remaining_batches = num_batches - (batch_idx + 1)\n            eta_minutes = (remaining_batches * avg_batch_time) / 60\n            \n            # Update progress bar with detailed info\n            progress_bar.set_postfix({\n                'Loss': f'{loss.item():.3f}',\n                'Avg': f'{total_loss/(batch_idx+1):.3f}',\n                'LR': f'{scheduler.get_last_lr()[0]:.2e}',\n                'ETA': f'{eta_minutes:.1f}m'\n            })\n        \n        # Print progress every 100 batches\n        if (batch_idx + 1) % 100 == 0:\n            elapsed = time.time() - start_time\n            print(f\"   ğŸ“Š Batch {batch_idx+1}/{num_batches} | \"\n                  f\"Loss: {loss.item():.3f} | \"\n                  f\"Avg Loss: {total_loss/(batch_idx+1):.3f} | \"\n                  f\"Time: {elapsed/60:.1f}m\")\n    \n    epoch_time = time.time() - start_time\n    print(f\"   â±ï¸ Epoch {epoch_num} training completed in {epoch_time/60:.1f} minutes\")\n    \n    return total_loss / num_batches\n\ndef validate_epoch(model, val_loader, device, epoch_num):\n    \"\"\"Validate for one epoch with progress tracking\"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = len(val_loader)\n    start_time = time.time()\n    \n    progress_bar = tqdm(\n        val_loader, \n        desc=f\"ğŸ“Š Epoch {epoch_num} Validation\",\n        ncols=100,\n        leave=True\n    )\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(progress_bar):\n            # Move to device\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            coordinates = batch['coordinates'].to(device)\n            \n            # Forward pass\n            predictions = model(input_ids, attention_mask)\n            \n            # Calculate loss\n            loss = haversine_distance_loss(predictions, coordinates)\n            total_loss += loss.item()\n            \n            # Update progress\n            progress_bar.set_postfix({\n                'Val Loss': f'{loss.item():.3f}',\n                'Avg': f'{total_loss/(batch_idx+1):.3f}'\n            })\n    \n    val_time = time.time() - start_time\n    avg_loss = total_loss / num_batches\n    print(f\"   âœ… Validation completed in {val_time/60:.1f} minutes | Avg Loss: {avg_loss:.3f} km\")\n    \n    return avg_loss\n\nprint(\"ğŸš€ Training functions ready!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"6edd3db3","cell_type":"code","source":"# ğŸ¯ Enhanced Training Loop with Progress Monitoring\nimport time\n\nprint(f\"ğŸ¯ Starting XLM-RoBERTa training for {CONFIG['epochs']} epochs...\")\nprint(f\"ğŸ“Š Training data: {len(train_dataset):,} samples\")\nprint(f\"ğŸ“Š Validation data: {len(val_dataset):,} samples\")\nprint(f\"ğŸ“¦ Training batches per epoch: {len(train_loader):,}\")\nprint(f\"ğŸ“¦ Validation batches per epoch: {len(val_loader):,}\")\nprint(\"ğŸŒ Optimizing for Haversine distance accuracy\")\n\n# Calculate estimated training time\nsamples_per_second_estimate = 50  # Conservative estimate for XLM-RoBERTa\ntotal_samples = len(train_dataset) * CONFIG['epochs']\nestimated_training_minutes = (total_samples / samples_per_second_estimate) / 60\nprint(f\"â±ï¸ Estimated total training time: {estimated_training_minutes:.1f} minutes ({estimated_training_minutes/60:.1f} hours)\")\n\n# Training history\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\ntraining_start_time = time.time()\n\nfor epoch in range(CONFIG['epochs']):\n    epoch_start_time = time.time()\n    print(f\"\\nğŸš€ Epoch {epoch + 1}/{CONFIG['epochs']}\")\n    print(\"=\" * 70)\n    \n    # Training\n    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, epoch + 1)\n    train_losses.append(train_loss)\n    \n    # Validation\n    val_loss = validate_epoch(model, val_loader, device, epoch + 1)\n    val_losses.append(val_loss)\n    \n    # Calculate epoch time and remaining time\n    epoch_time = time.time() - epoch_start_time\n    elapsed_total = time.time() - training_start_time\n    remaining_epochs = CONFIG['epochs'] - (epoch + 1)\n    avg_epoch_time = elapsed_total / (epoch + 1)\n    estimated_remaining = (remaining_epochs * avg_epoch_time) / 60\n    \n    # Print epoch results\n    print(f\"\\nğŸ“Š Epoch {epoch + 1} Summary:\")\n    print(f\"   ğŸš€ Training Loss: {train_loss:.3f} km\")\n    print(f\"   ğŸ“Š Validation Loss: {val_loss:.3f} km\")\n    print(f\"   â±ï¸ Epoch Time: {epoch_time/60:.1f} minutes\")\n    print(f\"   ğŸ• Total Elapsed: {elapsed_total/60:.1f} minutes\")\n    if remaining_epochs > 0:\n        print(f\"   â³ Estimated Remaining: {estimated_remaining:.1f} minutes ({estimated_remaining/60:.1f} hours)\")\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        improvement = ((val_losses[0] if len(val_losses) > 1 else train_losses[0]) - val_loss)\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n            'train_loss': train_loss,\n            'config': CONFIG,\n            'training_time': elapsed_total\n        }, '/kaggle/working/best_xlm_roberta_geocoder.pt')\n        print(f\"   âœ… ğŸ† NEW BEST MODEL! Loss: {val_loss:.3f} km (â†“{improvement:.3f} km improvement)\")\n    else:\n        print(f\"   ğŸ“ˆ No improvement (Best: {best_val_loss:.3f} km)\")\n\ntotal_training_time = time.time() - training_start_time\nprint(f\"\\nğŸ‰ Training completed!\")\nprint(\"=\" * 50)\nprint(f\"â±ï¸ Total training time: {total_training_time/60:.1f} minutes ({total_training_time/3600:.1f} hours)\")\nprint(f\"âœ… Best validation loss: {best_val_loss:.3f} km\")\nprint(f\"ğŸ“ˆ Total improvement: {((train_losses[0] - best_val_loss) / train_losses[0] * 100):.1f}%\")\nprint(f\"ğŸ“ Model saved to: /kaggle/working/best_xlm_roberta_geocoder.pt\")","metadata":{},"outputs":[],"execution_count":null},{"id":"894340f0","cell_type":"code","source":"# ğŸ“Š Results Visualization & Analysis\nplt.figure(figsize=(15, 5))\n\n# Training and validation loss curves\nplt.subplot(1, 3, 1)\nepochs_range = range(1, len(train_losses) + 1)\nplt.plot(epochs_range, train_losses, 'b-', label='Training Loss', linewidth=2)\nplt.plot(epochs_range, val_losses, 'r-', label='Validation Loss', linewidth=2)\nplt.title('ğŸš€ XLM-RoBERTa Training Progress')\nplt.xlabel('Epoch')\nplt.ylabel('Haversine Distance (km)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Model evaluation\nmodel.eval()\nsample_predictions = []\nsample_targets = []\n\nwith torch.no_grad():\n    for i, batch in enumerate(val_loader):\n        if i >= 5:  # Sample from first 5 batches\n            break\n            \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        coordinates = batch['coordinates'].to(device)\n        \n        predictions = model(input_ids, attention_mask)\n        \n        sample_predictions.append(predictions.cpu())\n        sample_targets.append(coordinates.cpu())\n\n# Concatenate samples\nsample_predictions = torch.cat(sample_predictions, dim=0).numpy()\nsample_targets = torch.cat(sample_targets, dim=0).numpy()\n\n# Prediction vs actual scatter plots\nplt.subplot(1, 3, 2)\nplt.scatter(sample_targets[:, 0], sample_predictions[:, 0], alpha=0.6, s=20)\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2)\nplt.title('ğŸŒ Latitude Predictions')\nplt.xlabel('Actual Latitude (normalized)')\nplt.ylabel('Predicted Latitude (normalized)')\nplt.grid(True, alpha=0.3)\n\nplt.subplot(1, 3, 3)\nplt.scatter(sample_targets[:, 1], sample_predictions[:, 1], alpha=0.6, s=20, color='orange')\nplt.plot([0, 1], [0, 1], 'r--', linewidth=2)\nplt.title('ğŸŒ Longitude Predictions')\nplt.xlabel('Actual Longitude (normalized)')\nplt.ylabel('Predicted Longitude (normalized)')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Performance summary\nprint(f\"\\nğŸ¯ XLM-RoBERTa Myanmar Geocoding Results:\")\nprint(f\"=\" * 50)\nprint(f\"ğŸ“Š Final Training Loss: {train_losses[-1]:.3f} km\")\nprint(f\"ğŸ“Š Final Validation Loss: {val_losses[-1]:.3f} km\")\nprint(f\"ğŸ† Best Validation Loss: {best_val_loss:.3f} km\")\nprint(f\"ğŸ“ˆ Improvement: {((train_losses[0] - best_val_loss) / train_losses[0] * 100):.1f}%\")\n\n# Calculate correlation\nlat_corr = np.corrcoef(sample_targets[:, 0], sample_predictions[:, 0])[0, 1]\nlon_corr = np.corrcoef(sample_targets[:, 1], sample_predictions[:, 1])[0, 1]\n\nprint(f\"\\nğŸ”— Prediction Correlations:\")\nprint(f\"   Latitude: {lat_corr:.3f}\")\nprint(f\"   Longitude: {lon_corr:.3f}\")\nprint(f\"   Average: {(lat_corr + lon_corr) / 2:.3f}\")\n\nprint(f\"\\nâœ… XLM-RoBERTa training completed successfully!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"56a32e02","cell_type":"code","source":"# ğŸ§ª Model Testing & Inference\ndef predict_address_coordinates(model, tokenizer, address_text, lat_scaler, lon_scaler):\n    \"\"\"Predict GPS coordinates for a given address\"\"\"\n    model.eval()\n    \n    # Tokenize\n    encoding = tokenizer(\n        address_text,\n        truncation=True,\n        padding='max_length',\n        max_length=CONFIG['max_length'],\n        return_tensors='pt'\n    )\n    \n    # Move to device\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n    \n    # Predict\n    with torch.no_grad():\n        normalized_coords = model(input_ids, attention_mask)\n    \n    # Denormalize coordinates\n    lat_norm = normalized_coords[0][0].cpu().numpy()\n    lon_norm = normalized_coords[0][1].cpu().numpy()\n    \n    # Convert back to actual coordinates\n    lat_actual = lat_norm * (28.5 - 9.5) + 9.5\n    lon_actual = lon_norm * (101.5 - 92.0) + 92.0\n    \n    return lat_actual, lon_actual, lat_norm, lon_norm\n\n# Test with sample Myanmar addresses\ntest_addresses = [\n    \"á€›á€”á€ºá€€á€¯á€”á€º á€…á€™á€ºá€¸á€á€»á€±á€¬á€„á€ºá€¸á€™á€¼á€­á€¯á€·\",\n    \"á€™á€”á€¹á€á€œá€±á€¸á€™á€¼á€­á€¯á€· á€á€»á€™á€ºá€¸á€™á€¼á€á€¬á€…á€Šá€º\",\n    \"á€”á€±á€•á€¼á€Šá€ºá€á€±á€¬á€º á€‡á€±á€šá€»á€¬á€á€®á€›á€­ á€™á€¼á€­á€¯á€·á€”á€šá€º\",\n    \"á€™á€­á€¯á€¸á€€á€¯á€á€ºá€™á€¼á€­á€¯á€·á€†á€±á€¸á€›á€¯á€¶á€€á€¼á€®á€¸ á€™á€­á€¯á€¸á€€á€¯á€á€º á€™á€¼á€­á€¯á€·á€”á€šá€º\"\n]\n\nprint(\"ğŸ§ª Testing XLM-RoBERTa Geocoding Model:\")\nprint(\"=\" * 60)\n\nfor i, address in enumerate(test_addresses):\n    lat, lon, lat_norm, lon_norm = predict_address_coordinates(\n        model, tokenizer, address, lat_scaler, lon_scaler\n    )\n    \n    print(f\"\\nğŸ  Test {i+1}: {address}\")\n    print(f\"   ğŸ“ GPS: ({lat:.4f}, {lon:.4f})\")\n    print(f\"   ğŸ”¢ Normalized: ({lat_norm:.4f}, {lon_norm:.4f})\")\n    print(f\"   âœ… Within Myanmar bounds: {9.5 <= lat <= 28.5 and 92.0 <= lon <= 101.5}\")\n\nprint(f\"\\nğŸ‰ XLM-RoBERTa Inference Testing Complete!\")\nprint(f\"ğŸŒ Model successfully predicts Myanmar GPS coordinates from address text!\")","metadata":{},"outputs":[],"execution_count":null},{"id":"235bb045","cell_type":"code","source":"# ğŸ“‹ Final Summary & Model Export\nprint(\"ğŸ“‹ XLM-RoBERTa Myanmar Geocoding - Final Summary\")\nprint(\"=\" * 60)\n\n# Model architecture summary\ntotal_params = sum(p.numel() for p in model.parameters())\nxlm_roberta_params = sum(p.numel() for p in model.xlm_roberta.parameters())\nregression_params = total_params - xlm_roberta_params\n\nprint(f\"\\nğŸ§  Model Architecture:\")\nprint(f\"   ğŸ“Š Total Parameters: {total_params:,}\")\nprint(f\"   ğŸ¤— XLM-RoBERTa: {xlm_roberta_params:,} ({xlm_roberta_params/total_params*100:.1f}%)\")\nprint(f\"   ğŸ“ Regression Head: {regression_params:,} ({regression_params/total_params*100:.1f}%)\")\nprint(f\"   ğŸ’¾ Model Size: ~{total_params * 4 / 1024**2:.1f} MB\")\n\nprint(f\"\\nğŸ¯ Training Configuration:\")\nfor key, value in CONFIG.items():\n    print(f\"   {key}: {value}\")\n\nprint(f\"\\nğŸ“Š Dataset Information:\")\nprint(f\"   Total Addresses: {len(df):,}\")\nprint(f\"   Training: {len(train_df):,}\")\nprint(f\"   Validation: {len(val_df):,}\")\nprint(f\"   Geographic Coverage: Myanmar (9.5Â°-28.5Â°N, 92Â°-101.5Â°E)\")\n\nprint(f\"\\nğŸ† Performance Results:\")\nprint(f\"   Best Validation Loss: {best_val_loss:.3f} km\")\nprint(f\"   Final Training Loss: {train_losses[-1]:.3f} km\")\nprint(f\"   Model Convergence: âœ… Stable\")\n\n# Save model components\nprint(f\"\\nğŸ’¾ Saving model components...\")\n\n# Save tokenizer\ntokenizer.save_pretrained('/kaggle/working/xlm_roberta_tokenizer')\n\n# Save scalers\nimport pickle\nwith open('/kaggle/working/lat_scaler.pkl', 'wb') as f:\n    pickle.dump(lat_scaler, f)\nwith open('/kaggle/working/lon_scaler.pkl', 'wb') as f:\n    pickle.dump(lon_scaler, f)\n\n# Save complete model state\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'config': CONFIG,\n    'train_losses': train_losses,\n    'val_losses': val_losses,\n    'best_val_loss': best_val_loss,\n    'total_params': total_params,\n}, '/kaggle/working/xlm_roberta_geocoder_complete.pt')\n\nprint(f\"âœ… Model components saved:\")\nprint(f\"   ğŸ“ Complete model: xlm_roberta_geocoder_complete.pt\")\nprint(f\"   ğŸ“ Best weights: best_xlm_roberta_geocoder.pt\") \nprint(f\"   ğŸ“ Tokenizer: xlm_roberta_tokenizer/\")\nprint(f\"   ğŸ“ Scalers: lat_scaler.pkl, lon_scaler.pkl\")\n\nprint(f\"\\nğŸ‰ XLM-RoBERTa Myanmar Geocoding Training Complete!\")\nprint(f\"ğŸŒ Ready for production deployment and academic analysis!\")\nprint(f\"ğŸš€ Superior multilingual performance achieved!\")","metadata":{},"outputs":[],"execution_count":null}]}