{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tklwin/MMDT_2025_MLAI105/blob/thein-kyaw-lwin/thein-kyaw-lwin/Project_03/Project_03_CNN_Models_TheinKyawLwin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_DEy1wtCIyp"
      },
      "source": [
        "# Project Description\n",
        "\n",
        "## Image Classification Using Known CNN Models\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this project, we classify images using five well-known Convolutional Neural Network (CNN) models implemented with the Python `keras` library. The models used are `ResNet50`, `VGG16`, `InceptionV3`, `Xception`, and `EfficientNetB7`. The goal is to load an image, pass it through each of these models, and obtain the top prediction for the image. This project consists of two Python scripts: one for defining the CNN models (`cnn_models.py`) and one main script (`main.py`) for classifying an image.\n",
        "\n",
        "### Project Components\n",
        "\n",
        "#### 1. `cnn_models.py`\n",
        "\n",
        "This script defines a class, `cnnModels`, which provides an interface to load and use the pre-trained CNN models. The class includes methods for initializing models, retrieving models by name, and classifying images.\n",
        "\n",
        "##### `cnnModels` Class\n",
        "\n",
        "- **`__init__(self)`**: Initializes the class and loads the pre-trained models.\n",
        "- **`resnet(self)`**: Loads and returns the `ResNet50` model with ImageNet weights.\n",
        "- **`vggnet(self)`**: Loads and returns the `VGG16` model with ImageNet weights.\n",
        "- **`inception(self)`**: Loads and returns the `InceptionV3` model with ImageNet weights.\n",
        "- **`convnet(self)`**: Loads and returns the `Xception` model with ImageNet weights.\n",
        "- **`efficientnet(self)`**: Loads and returns the `EfficientNetB7` model with ImageNet weights.\n",
        "- **`get_model(self, name)`**: Retrieves a model by name from the dictionary of models.\n",
        "- **`classify_image(self, name, img)`**: Classifies an image using the specified model and returns the top 3 predictions.\n",
        "\n",
        "#### 2. `main.ipynb`\n",
        "\n",
        "This script demonstrates how to use the `cnnModels` class to classify an image.\n",
        "\n",
        "##### Example Usage\n",
        "\n",
        "```python\n",
        "from cnn_models import cnnModels\n",
        "from keras.preprocessing.image import load_img\n",
        "\n",
        "# Specify the image path\n",
        "img_path = './imgs/dog.jpeg'\n",
        "img = load_img(img_path)\n",
        "\n",
        "# Initialize the cnnModels class\n",
        "model = cnnModels()\n",
        "\n",
        "# Classify the image using ResNet50\n",
        "preds1 = model.classify_image('ResNet50', img)\n",
        "\n",
        "# Print the top predictions\n",
        "for pred in preds1:\n",
        "    print(f\"{pred[1]}: {pred[2]}, {pred[3]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9tG3TlPCIyq"
      },
      "source": [
        "The state-of-the-art CNN models are tested using two datasets:\n",
        "1) AI-generated Images that contains 10 images\n",
        "2) 10 Real Images collected from the internet\n",
        "\n",
        "average accuracy, precision and recall scores."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tklwin/Intro-to-Deep-Learning.git\n",
        "import sys\n",
        "sys.path.append('/content/Intro-to-Deep-Learning/chapter3/Project_01')"
      ],
      "metadata": {
        "id": "LmshTdnqCShD",
        "outputId": "28332ed4-ca43-4e9f-989b-0f683a6b718e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Intro-to-Deep-Learning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "agn3QPGRCIyq"
      },
      "outputs": [],
      "source": [
        "import cnn_models\n",
        "import pandas as pd\n",
        "from keras.utils import load_img #type: ignore\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YNK_sh2PCIyq"
      },
      "outputs": [],
      "source": [
        "def get_predictions(image_dir):\n",
        "    model = cnn_models.cnnModels()\n",
        "    model_name = ['ResNet50', 'VGGNet16', 'InceptionV3', 'ConvNeXt', 'EfficientNet']\n",
        "    result_df = pd.DataFrame(columns = model_name + [name + '_prob' for name in model_name])\n",
        "\n",
        "    labels =[]\n",
        "    row_values = []\n",
        "\n",
        "    for filename in os.listdir(image_dir):\n",
        "        if filename.endswith('.jpeg') or filename.endswith('.png')or filename.endswith('.jpg'):\n",
        "            image_path = os.path.join(image_dir, filename)\n",
        "            img = load_img(image_path)\n",
        "            labels.append(filename.split('.')[0])\n",
        "            prob_preds = []\n",
        "            class_preds = []\n",
        "            for name in model_name:\n",
        "                preds = model.classify_image(name, img)[0][0][1:3]\n",
        "                class_preds.append(preds[0])\n",
        "                prob_preds.append(preds[1])\n",
        "\n",
        "            row_values.append(class_preds + prob_preds)\n",
        "\n",
        "    result_df = pd.DataFrame(row_values, columns = model_name + [name + '_prob' for name in model_name])\n",
        "    result_df['label'] = labels\n",
        "\n",
        "    return result_df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/tklwin/MMDT_2025_MLAI105/raw/refs/heads/thein-kyaw-lwin/thein-kyaw-lwin/Project_03/tkl_dataset.zip\"\n",
        "!unzip tkl_dataset.zip\n",
        "tkl_dir = './tkl_dataset/'\n",
        "tkl_result = get_predictions(tkl_dir)"
      ],
      "metadata": {
        "id": "Zzq2dcGvCyyk",
        "outputId": "9d68c8fb-19da-4051-9c71-80bca9b8fa05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-28 15:26:27--  https://github.com/tklwin/MMDT_2025_MLAI105/raw/refs/heads/thein-kyaw-lwin/thein-kyaw-lwin/Project_03/tkl_dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tklwin/MMDT_2025_MLAI105/refs/heads/thein-kyaw-lwin/thein-kyaw-lwin/Project_03/tkl_dataset.zip [following]\n",
            "--2025-06-28 15:26:28--  https://raw.githubusercontent.com/tklwin/MMDT_2025_MLAI105/refs/heads/thein-kyaw-lwin/thein-kyaw-lwin/Project_03/tkl_dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3649352 (3.5M) [application/zip]\n",
            "Saving to: ‘tkl_dataset.zip.1’\n",
            "\n",
            "tkl_dataset.zip.1   100%[===================>]   3.48M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-06-28 15:26:28 (42.1 MB/s) - ‘tkl_dataset.zip.1’ saved [3649352/3649352]\n",
            "\n",
            "Archive:  tkl_dataset.zip\n",
            "replace __MACOSX/._tkl_dataset? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: __MACOSX/._tkl_dataset  \n",
            "  inflating: tkl_dataset/fan.jpg     \n",
            "  inflating: __MACOSX/tkl_dataset/._fan.jpg  \n",
            "  inflating: tkl_dataset/dog.jpg     \n",
            "  inflating: __MACOSX/tkl_dataset/._dog.jpg  \n",
            "  inflating: tkl_dataset/apple.jpg   \n",
            "  inflating: __MACOSX/tkl_dataset/._apple.jpg  \n",
            "  inflating: tkl_dataset/cat.jpg     \n",
            "  inflating: __MACOSX/tkl_dataset/._cat.jpg  \n",
            "  inflating: tkl_dataset/mountain.jpg  \n",
            "  inflating: __MACOSX/tkl_dataset/._mountain.jpg  \n",
            "  inflating: tkl_dataset/car.jpg     \n",
            "  inflating: __MACOSX/tkl_dataset/._car.jpg  \n",
            "  inflating: tkl_dataset/football.jpg  \n",
            "  inflating: __MACOSX/tkl_dataset/._football.jpg  \n",
            "  inflating: tkl_dataset/clock.jpg   \n",
            "  inflating: __MACOSX/tkl_dataset/._clock.jpg  \n",
            "  inflating: tkl_dataset/guitar.jpg  \n",
            "  inflating: __MACOSX/tkl_dataset/._guitar.jpg  \n",
            "  inflating: tkl_dataset/baby.jpg    \n",
            "  inflating: __MACOSX/tkl_dataset/._baby.jpg  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 14 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x787f4e611f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8WEWIXIfCIyr"
      },
      "outputs": [],
      "source": [
        "tkl_result.to_csv('./tkl_result.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "print(tabulate(tkl_result, headers='keys', tablefmt='github'))"
      ],
      "metadata": {
        "id": "d8EmePAOEkaS",
        "outputId": "0d7d810b-36db-4365-d097-4b962dd14956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | ResNet50           | VGGNet16           | InceptionV3   | ConvNeXt           | EfficientNet     |   ResNet50_prob |   VGGNet16_prob |   InceptionV3_prob |   ConvNeXt_prob |   EfficientNet_prob | label    |\n",
            "|----|--------------------|--------------------|---------------|--------------------|------------------|-----------------|-----------------|--------------------|-----------------|---------------------|----------|\n",
            "|  0 | stopwatch          | analog_clock       | web_site      | analog_clock       | strainer         |        0.561457 |       0.552459  |           1        |        0.928616 |           0.326287  | clock    |\n",
            "|  1 | Great_Pyrenees     | Maltese_dog        | web_site      | tub                | mask             |        0.249248 |       0.0936128 |           0.999959 |        0.331172 |           0.0979041 | baby     |\n",
            "|  2 | pomegranate        | pomegranate        | flatworm      | Granny_Smith       | wooden_spoon     |        0.655223 |       0.913065  |           1        |        0.568936 |           0.0838461 | apple    |\n",
            "|  3 | acoustic_guitar    | electric_guitar    | web_site      | electric_guitar    | microphone       |        0.73337  |       0.95669   |           1        |        0.894345 |           0.0964501 | guitar   |\n",
            "|  4 | tabby              | tabby              | flatworm      | lynx               | tabby            |        0.684787 |       0.43925   |           0.999997 |        0.387555 |           0.298807  | cat      |\n",
            "|  5 | sports_car         | sports_car         | web_site      | sports_car         | grille           |        0.213541 |       0.499425  |           1        |        0.671603 |           0.166676  | car      |\n",
            "|  6 | Labrador_retriever | Labrador_retriever | web_site      | Labrador_retriever | golden_retriever |        0.242197 |       0.294892  |           0.999908 |        0.401644 |           0.666864  | dog      |\n",
            "|  7 | soccer_ball        | soccer_ball        | web_site      | soccer_ball        | soccer_ball      |        0.841772 |       0.870668  |           1        |        0.945497 |           0.86134   | football |\n",
            "|  8 | alp                | alp                | flatworm      | alp                | alp              |        0.774511 |       0.803675  |           1        |        0.742693 |           0.682475  | mountain |\n",
            "|  9 | electric_fan       | electric_fan       | web_site      | electric_fan       | electric_fan     |        0.99791  |       0.998452  |           1        |        0.949404 |           0.885767  | fan      |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from keras.preprocessing.image import load_img\n",
        "from tabulate import tabulate\n",
        "\n",
        "import cnn_models\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 1. ACCEPTABLE label mapping (semantic matches)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "ACCEPTABLE = {\n",
        "    \"clock\": {\"analog_clock\", \"stopwatch\", \"wall_clock\"},\n",
        "    \"baby\": {\"baby\"},\n",
        "    \"apple\": {\"apple\"},\n",
        "    \"guitar\": {\"acoustic_guitar\", \"electric_guitar\", \"guitar\"},\n",
        "    \"cat\": {\"tabby\", \"lynx\", \"cat\"},\n",
        "    \"car\": {\"sports_car\", \"car\"},\n",
        "    \"dog\": {\"Labrador_retriever\", \"golden_retriever\", \"dog\"},\n",
        "    \"football\": {\"soccer_ball\", \"football\"},\n",
        "    \"mountain\": {\"alp\", \"mountain\"},\n",
        "    \"fan\": {\"electric_fan\", \"fan\"},\n",
        "}\n",
        "\n",
        "MODEL_NAMES = [\"ResNet50\", \"VGGNet16\", \"InceptionV3\", \"ConvNeXt\", \"EfficientNet\"]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 2. Helper to get top‑3 models that match the label\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def top3_models_for_row(row):\n",
        "    label = row[\"label\"]\n",
        "    accepted = ACCEPTABLE.get(label, {label})\n",
        "    matches = []\n",
        "    for model in MODEL_NAMES:\n",
        "        pred_class = row[model]\n",
        "        prob = row[model + \"_prob\"]\n",
        "        if pred_class in accepted:\n",
        "            matches.append((model, prob))\n",
        "    matches.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [m for m, _ in matches[:3]]\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 3. Main prediction + timing routine\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def get_predictions(image_dir):\n",
        "    model_wrapper = cnn_models.cnnModels()\n",
        "    rows, labels, all_times = [], [], []\n",
        "\n",
        "    for fname in os.listdir(image_dir):\n",
        "        if not fname.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            continue\n",
        "        img_path = os.path.join(image_dir, fname)\n",
        "        img = load_img(img_path)\n",
        "        labels.append(fname.split(\".\")[0])\n",
        "\n",
        "        preds, probs, times = [], [], []\n",
        "        for m in MODEL_NAMES:\n",
        "            t0 = time.time()\n",
        "            # your classify_image returns list [[(id, class, prob)]]\n",
        "            pred_class, pred_prob = model_wrapper.classify_image(m, img)[0][0][1:3]\n",
        "            t1 = time.time()\n",
        "            preds.append(pred_class)\n",
        "            probs.append(pred_prob)\n",
        "            times.append(t1 - t0)\n",
        "\n",
        "        rows.append(preds + probs)\n",
        "        all_times.append(times)\n",
        "\n",
        "    cols = MODEL_NAMES + [m + \"_prob\" for m in MODEL_NAMES]\n",
        "    df = pd.DataFrame(rows, columns=cols)\n",
        "    df[\"label\"] = labels\n",
        "    df[\"top3_models\"] = df.apply(top3_models_for_row, axis=1)\n",
        "\n",
        "    # add inference times per model\n",
        "    for idx, m in enumerate(MODEL_NAMES):\n",
        "        df[m + \"_time\"] = [t[idx] for t in all_times]\n",
        "\n",
        "    return df\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 4. Run inference\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "tkl_dir = \"./tkl_dataset/\"\n",
        "tkl_result = get_predictions(tkl_dir)\n",
        "print(tabulate(tkl_result[[\"label\", \"top3_models\"]], headers=\"keys\", tablefmt=\"github\"))\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 5. Accuracy & timing stats\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def is_correct(pred, label):\n",
        "    return pred in ACCEPTABLE.get(label, {label})\n",
        "\n",
        "correct_counts = {\n",
        "    m: sum(is_correct(r[m], r[\"label\"]) for _, r in tkl_result.iterrows())\n",
        "    for m in MODEL_NAMES\n",
        "}\n",
        "\n",
        "total_images = len(tkl_result)\n",
        "accuracies = {m: correct_counts[m] / total_images for m in MODEL_NAMES}\n",
        "\n",
        "print(\"\\nModel Accuracy:\")\n",
        "for m in MODEL_NAMES:\n",
        "    print(f\"{m:15s}: {accuracies[m]:.2%}\")\n",
        "\n",
        "print(\"\\nAverage Inference Time per Image:\")\n",
        "for m in MODEL_NAMES:\n",
        "    print(f\"{m:15s}: {tkl_result[m + '_time'].mean():.4f} sec\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 6. Parameter counts per model\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\nModel Parameters (trainable + non‑trainable):\")\n",
        "param_counts = {}\n",
        "model_instance = cnn_models.cnnModels()\n",
        "for m in MODEL_NAMES:\n",
        "    net = model_instance.get_model(m)\n",
        "    param_counts[m] = net.count_params()\n",
        "    print(f\"{m:15s}: {param_counts[m]:,}\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 7. Flexible weight‑file size check via glob\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"\\nModel Sizes (MB):\")\n",
        "model_globs = {\n",
        "    \"ResNet50\": \"resnet50*\",\n",
        "    \"VGGNet16\": \"vgg16*\",\n",
        "    \"InceptionV3\": \"inception_v3*\",\n",
        "    \"ConvNeXt\": \"convnext*\",\n",
        "    \"EfficientNet\": \"efficientnet*\",\n",
        "}\n",
        "\n",
        "keras_dir = Path.home() / \".keras\" / \"models\"\n",
        "model_sizes = {}\n",
        "for m, pattern in model_globs.items():\n",
        "    matches = list(keras_dir.glob(pattern))\n",
        "    if matches:\n",
        "        size_mb = matches[0].stat().st_size / (1024 * 1024)\n",
        "        model_sizes[m] = round(size_mb, 2)\n",
        "        print(f\"{m:15s}: {size_mb:.2f} MB  ({matches[0].name})\")\n",
        "    else:\n",
        "        model_sizes[m] = \"Not found\"\n",
        "        print(f\"{m:15s}: Not found\")\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 8. Final benchmark summary table\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "summary_rows = [\n",
        "    [\n",
        "        m,\n",
        "        correct_counts[m],\n",
        "        total_images,\n",
        "        f\"{accuracies[m]:.2%}\",\n",
        "        f\"{tkl_result[m + '_time'].mean():.4f} sec\",\n",
        "        model_sizes[m],\n",
        "        f\"{param_counts[m]:,}\",\n",
        "    ]\n",
        "    for m in MODEL_NAMES\n",
        "]\n",
        "\n",
        "print(\"\\nModel Benchmark Summary:\\n\")\n",
        "print(\n",
        "    tabulate(\n",
        "        summary_rows,\n",
        "        headers=[\n",
        "            \"Model\",\n",
        "            \"Correct\",\n",
        "            \"Total\",\n",
        "            \"Accuracy\",\n",
        "            \"Avg Inference Time\",\n",
        "            \"Model Size (MB)\",\n",
        "            \"Parameters\",\n",
        "        ],\n",
        "        tablefmt=\"github\",\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "2mc0qigWVIoN",
        "outputId": "e2f6caef-ce8e-4ee4-e79e-c6c2c64eeb39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|    | label    | top3_models                              |\n",
            "|----|----------|------------------------------------------|\n",
            "|  0 | clock    | ['ConvNeXt', 'ResNet50', 'VGGNet16']     |\n",
            "|  1 | baby     | []                                       |\n",
            "|  2 | apple    | []                                       |\n",
            "|  3 | guitar   | ['VGGNet16', 'ConvNeXt', 'ResNet50']     |\n",
            "|  4 | cat      | ['ResNet50', 'VGGNet16', 'ConvNeXt']     |\n",
            "|  5 | car      | ['ConvNeXt', 'VGGNet16', 'ResNet50']     |\n",
            "|  6 | dog      | ['EfficientNet', 'ConvNeXt', 'VGGNet16'] |\n",
            "|  7 | football | ['ConvNeXt', 'VGGNet16', 'EfficientNet'] |\n",
            "|  8 | mountain | ['VGGNet16', 'ResNet50', 'ConvNeXt']     |\n",
            "|  9 | fan      | ['VGGNet16', 'ResNet50', 'ConvNeXt']     |\n",
            "\n",
            "Model Accuracy:\n",
            "ResNet50       : 80.00%\n",
            "VGGNet16       : 80.00%\n",
            "InceptionV3    : 0.00%\n",
            "ConvNeXt       : 80.00%\n",
            "EfficientNet   : 50.00%\n",
            "\n",
            "Average Inference Time per Image:\n",
            "ResNet50       : 0.4939 sec\n",
            "VGGNet16       : 0.7686 sec\n",
            "InceptionV3    : 0.6526 sec\n",
            "ConvNeXt       : 1.4892 sec\n",
            "EfficientNet   : 4.0551 sec\n",
            "\n",
            "Model Parameters (trainable + non‑trainable):\n",
            "ResNet50       : 25,636,712\n",
            "VGGNet16       : 138,357,544\n",
            "InceptionV3    : 23,851,784\n",
            "ConvNeXt       : 28,589,128\n",
            "EfficientNet   : 66,658,687\n",
            "\n",
            "Model Sizes (MB):\n",
            "ResNet50       : 98.20 MB  (resnet50_weights_tf_dim_ordering_tf_kernels.h5)\n",
            "VGGNet16       : 527.83 MB  (vgg16_weights_tf_dim_ordering_tf_kernels.h5)\n",
            "InceptionV3    : 91.66 MB  (inception_v3_weights_tf_dim_ordering_tf_kernels.h5)\n",
            "ConvNeXt       : 109.42 MB  (convnext_tiny.h5)\n",
            "EfficientNet   : 255.90 MB  (efficientnetb7.h5)\n",
            "\n",
            "Model Benchmark Summary:\n",
            "\n",
            "| Model        |   Correct |   Total | Accuracy   | Avg Inference Time   |   Model Size (MB) | Parameters   |\n",
            "|--------------|-----------|---------|------------|----------------------|-------------------|--------------|\n",
            "| ResNet50     |         8 |      10 | 80.00%     | 0.4939 sec           |             98.2  | 25,636,712   |\n",
            "| VGGNet16     |         8 |      10 | 80.00%     | 0.7686 sec           |            527.83 | 138,357,544  |\n",
            "| InceptionV3  |         0 |      10 | 0.00%      | 0.6526 sec           |             91.66 | 23,851,784   |\n",
            "| ConvNeXt     |         8 |      10 | 80.00%     | 1.4892 sec           |            109.42 | 28,589,128   |\n",
            "| EfficientNet |         5 |      10 | 50.00%     | 4.0551 sec           |            255.9  | 66,658,687   |\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}