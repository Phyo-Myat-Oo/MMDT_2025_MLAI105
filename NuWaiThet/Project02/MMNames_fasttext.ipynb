{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd473e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!pip uninstall numpy gensim -y\n",
    "# !pip install numpy gensim --force-reinstall --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a239964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nuwai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nuwai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nuwai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "import pandas as pd\n",
    "import data_preprocessing as dp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0009dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "395f4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/MMNames_clean.csv')\n",
    "df = dp.clean_name_column(df, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93e34838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 15087, After deduplication: 15087\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify ambiguous names (appear in multiple regions)\n",
    "dupes = df.groupby(\"name\")[\"SR_Name\"].nunique()\n",
    "ambiguous_names = dupes[dupes > 1].index\n",
    "\n",
    "# Step 2: For ambiguous names, keep only the first occurrence\n",
    "ambiguous_df = df[df[\"name\"].isin(ambiguous_names)]\n",
    "ambiguous_deduped = ambiguous_df.drop_duplicates(subset=\"name\", keep=\"first\")\n",
    "\n",
    "# Step 3: For non-ambiguous names, just keep them as-is\n",
    "non_ambiguous_df = df[~df[\"name\"].isin(ambiguous_names)]\n",
    "\n",
    "# Step 4: Combine them back together\n",
    "df = pd.concat([non_ambiguous_df, ambiguous_deduped], ignore_index=True)\n",
    "\n",
    "# Optional: Check final size\n",
    "print(f\"Original: {len(df)}, After deduplication: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20465bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['SR_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume df['name'] contains romanized text\n",
    "df['name'].dropna().astype(str).to_csv(\"names.txt\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each name into a list of characters or subwords (or words, depending on data)\n",
    "df['tokens'] = df['name'].astype(str).apply(lambda x: x.split())  # word-level\n",
    "# OR for character-level: list(x)\n",
    "tokenized_data = df['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d0937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_burmese_phonics(text):\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Mapping of Romanized Burmese phonics to standard forms\n",
    "    phonics_map = {\n",
    "        # Aspirated consonants → base form\n",
    "        'ph': 'p',\n",
    "        'hp': 'p',\n",
    "        'hpy': 'py',\n",
    "        'hs': 's',\n",
    "        'th': 't',\n",
    "        'ht': 't',\n",
    "        'kh': 'k',\n",
    "        'hk': 'k',\n",
    "        'ng': 'n',\n",
    "        'ny': 'n',\n",
    "        'my': 'm',\n",
    "\n",
    "        # Diphthongs and vowels\n",
    "        'oo': 'u',\n",
    "        'ou': 'u',\n",
    "        'au': 'o',\n",
    "        'aw': 'o',\n",
    "        'ae': 'e',\n",
    "        'ay': 'e',\n",
    "        'ei': 'e',\n",
    "        'ia': 'ya',   # ex: \"Pyi A\" or \"Pya\"\n",
    "        'ua': 'wa',\n",
    "\n",
    "        # Word endings or tones\n",
    "        'aung': 'ong',\n",
    "        'auk': 'ok',\n",
    "        'ein': 'en',\n",
    "        'yin': 'in',\n",
    "        'yan': 'an',\n",
    "\n",
    "        # Optional tone reduction\n",
    "        'ya': 'a',\n",
    "        'wa': 'a',\n",
    "        'ra': 'a',\n",
    "\n",
    "        # Silent or redundant\n",
    "        'rr': 'r',\n",
    "        'll': 'l',\n",
    "        'pp': 'p',\n",
    "        'tt': 't',\n",
    "        'kk': 'k',\n",
    "        'mm': 'm',\n",
    "        'nn': 'n',\n",
    "        'gg': 'g',\n",
    "        'ss': 's',\n",
    "    }\n",
    "\n",
    "    # Apply rules based on length, avoiding overlap\n",
    "    for k in sorted(phonics_map, key=lambda x: -len(x)):\n",
    "        text = re.sub(k, phonics_map[k], text)\n",
    "\n",
    "    # Remove unwanted characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Normalize whitespace and repeated letters\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629800f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name'] = df['name'].apply(normalize_burmese_phonics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b358b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              bogale\n",
       "1             danubyu\n",
       "2                dede\n",
       "3                enme\n",
       "4             hintada\n",
       "             ...     \n",
       "15082          nar ku\n",
       "15083     tone bo gyi\n",
       "15084    pan kar kone\n",
       "15085         par kar\n",
       "15086          an mai\n",
       "Name: name, Length: 15087, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27a4e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Bidirectional, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# === PARAMETERS ===\n",
    "MAX_LEN = 30\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# === 1. Tokenize names into char lists for FastText ===\n",
    "df['tokens'] = df['name'].astype(str).apply(lambda x: list(x.strip().lower()))\n",
    "tokenized_data = df['tokens'].tolist()\n",
    "\n",
    "# === 2. Train FastText model on all data (unsupervised embeddings) ===\n",
    "fasttext_model = FastText(\n",
    "    sentences=tokenized_data,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    window=3,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1  # skip-gram\n",
    ")\n",
    "fasttext_model.save(\"fasttext_gensim.model\")\n",
    "\n",
    "# === 3. Prepare texts for Keras Tokenizer (space-separated chars) ===\n",
    "texts_for_keras = [' '.join(tokens) for tokens in tokenized_data]\n",
    "\n",
    "# === 4. Split data into train and test BEFORE fitting tokenizer and label encoder to avoid leakage ===\n",
    "X_train_texts, X_test_texts, y_train_raw, y_test_raw = train_test_split(\n",
    "    texts_for_keras, df['SR_Name'], test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df['SR_Name']\n",
    ")\n",
    "\n",
    "# === 5. Fit Keras Tokenizer ONLY on training data ===\n",
    "tokenizer = Tokenizer(char_level=False, lower=True)  # words = chars separated by space\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "# === 6. Encode labels with LabelEncoder fitted on train labels ONLY ===\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train_raw)\n",
    "\n",
    "y_train = label_encoder.transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "# === 7. Create embedding matrix from FastText for tokenizer vocabulary ===\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in fasttext_model.wv:\n",
    "        embedding_matrix[i] = fasttext_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(size=(EMBEDDING_DIM,))\n",
    "\n",
    "# === 8. Convert texts to padded sequences ===\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad0dcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, SpatialDropout1D\n",
    "\n",
    "\n",
    "def create_conv_lstm_model(vocab_size, max_len, num_classes,embedding_matrix):\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=False  # freeze embeddings, set True if you want to fine-tune\n",
    "        ),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Conv1D(64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(2),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84d411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.1266 - loss: 2.7341 - val_accuracy: 0.1734 - val_loss: 2.6045\n",
      "Epoch 2/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.1768 - loss: 2.6006 - val_accuracy: 0.1933 - val_loss: 2.5328\n",
      "Epoch 3/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.1893 - loss: 2.5391 - val_accuracy: 0.2026 - val_loss: 2.4888\n",
      "Epoch 4/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - accuracy: 0.2085 - loss: 2.4875 - val_accuracy: 0.2101 - val_loss: 2.4614\n",
      "Epoch 5/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2191 - loss: 2.4650 - val_accuracy: 0.2107 - val_loss: 2.4579\n",
      "Epoch 6/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2375 - loss: 2.4134 - val_accuracy: 0.2187 - val_loss: 2.4307\n",
      "Epoch 7/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2421 - loss: 2.3956 - val_accuracy: 0.2176 - val_loss: 2.4215\n",
      "Epoch 8/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2499 - loss: 2.3631 - val_accuracy: 0.2189 - val_loss: 2.4071\n",
      "Epoch 9/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2543 - loss: 2.3303 - val_accuracy: 0.2304 - val_loss: 2.3789\n",
      "Epoch 10/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2558 - loss: 2.3244 - val_accuracy: 0.2300 - val_loss: 2.3782\n",
      "Epoch 11/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2780 - loss: 2.2802 - val_accuracy: 0.2355 - val_loss: 2.3698\n",
      "Epoch 12/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.2723 - loss: 2.2723 - val_accuracy: 0.2375 - val_loss: 2.3723\n",
      "Epoch 13/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.2874 - loss: 2.2405 - val_accuracy: 0.2408 - val_loss: 2.3510\n",
      "Epoch 14/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2968 - loss: 2.2171 - val_accuracy: 0.2459 - val_loss: 2.3496\n",
      "Epoch 15/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2983 - loss: 2.1955 - val_accuracy: 0.2450 - val_loss: 2.3725\n",
      "Epoch 16/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.2871 - loss: 2.1889 - val_accuracy: 0.2496 - val_loss: 2.3546\n",
      "Epoch 17/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3156 - loss: 2.1224 - val_accuracy: 0.2529 - val_loss: 2.3436\n",
      "Epoch 18/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3282 - loss: 2.0920 - val_accuracy: 0.2545 - val_loss: 2.3483\n",
      "Epoch 19/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3277 - loss: 2.0539 - val_accuracy: 0.2543 - val_loss: 2.3463\n",
      "Epoch 20/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3510 - loss: 2.0094 - val_accuracy: 0.2571 - val_loss: 2.3382\n",
      "Epoch 21/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3494 - loss: 1.9955 - val_accuracy: 0.2593 - val_loss: 2.3527\n",
      "Epoch 22/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.3583 - loss: 1.9672 - val_accuracy: 0.2635 - val_loss: 2.3677\n",
      "Epoch 23/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3759 - loss: 1.9225 - val_accuracy: 0.2679 - val_loss: 2.3612\n",
      "Epoch 24/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3862 - loss: 1.8867 - val_accuracy: 0.2655 - val_loss: 2.3760\n",
      "Epoch 25/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.3841 - loss: 1.8546 - val_accuracy: 0.2786 - val_loss: 2.3573\n",
      "Epoch 26/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.4128 - loss: 1.7987 - val_accuracy: 0.2719 - val_loss: 2.4223\n",
      "Epoch 27/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.4169 - loss: 1.7774 - val_accuracy: 0.2739 - val_loss: 2.4193\n",
      "Epoch 28/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.4282 - loss: 1.7333 - val_accuracy: 0.2757 - val_loss: 2.3982\n",
      "Epoch 29/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.4369 - loss: 1.6986 - val_accuracy: 0.2781 - val_loss: 2.4140\n",
      "Epoch 30/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.4524 - loss: 1.6749 - val_accuracy: 0.2797 - val_loss: 2.4223\n",
      "Epoch 31/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.4712 - loss: 1.6342 - val_accuracy: 0.2898 - val_loss: 2.4523\n",
      "Epoch 32/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.4953 - loss: 1.5909 - val_accuracy: 0.2852 - val_loss: 2.4391\n",
      "Epoch 33/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.4975 - loss: 1.5659 - val_accuracy: 0.2962 - val_loss: 2.4510\n",
      "Epoch 34/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5025 - loss: 1.5132 - val_accuracy: 0.2907 - val_loss: 2.4821\n",
      "Epoch 35/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5118 - loss: 1.4976 - val_accuracy: 0.2962 - val_loss: 2.5004\n",
      "Epoch 36/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5238 - loss: 1.4557 - val_accuracy: 0.3097 - val_loss: 2.5137\n",
      "Epoch 37/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5298 - loss: 1.4403 - val_accuracy: 0.3053 - val_loss: 2.4969\n",
      "Epoch 38/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5500 - loss: 1.3765 - val_accuracy: 0.2991 - val_loss: 2.5517\n",
      "Epoch 39/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.5565 - loss: 1.3628 - val_accuracy: 0.3090 - val_loss: 2.5302\n",
      "Epoch 40/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5530 - loss: 1.3465 - val_accuracy: 0.3128 - val_loss: 2.5607\n",
      "Epoch 41/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5795 - loss: 1.2888 - val_accuracy: 0.3126 - val_loss: 2.5182\n",
      "Epoch 42/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5787 - loss: 1.2849 - val_accuracy: 0.3172 - val_loss: 2.5182\n",
      "Epoch 43/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.5886 - loss: 1.2643 - val_accuracy: 0.3285 - val_loss: 2.6141\n",
      "Epoch 44/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.5934 - loss: 1.2478 - val_accuracy: 0.3172 - val_loss: 2.6096\n",
      "Epoch 45/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6084 - loss: 1.2039 - val_accuracy: 0.3274 - val_loss: 2.6080\n",
      "Epoch 46/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6134 - loss: 1.1682 - val_accuracy: 0.3236 - val_loss: 2.6616\n",
      "Epoch 47/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6258 - loss: 1.1414 - val_accuracy: 0.3302 - val_loss: 2.6645\n",
      "Epoch 48/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6282 - loss: 1.1259 - val_accuracy: 0.3271 - val_loss: 2.6633\n",
      "Epoch 49/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6373 - loss: 1.1122 - val_accuracy: 0.3322 - val_loss: 2.6565\n",
      "Epoch 50/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6492 - loss: 1.0697 - val_accuracy: 0.3342 - val_loss: 2.6965\n",
      "Epoch 51/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6534 - loss: 1.0619 - val_accuracy: 0.3366 - val_loss: 2.7273\n",
      "Epoch 52/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6630 - loss: 1.0443 - val_accuracy: 0.3466 - val_loss: 2.7324\n",
      "Epoch 53/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6570 - loss: 1.0517 - val_accuracy: 0.3415 - val_loss: 2.7889\n",
      "Epoch 54/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6582 - loss: 1.0291 - val_accuracy: 0.3459 - val_loss: 2.7538\n",
      "Epoch 55/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6695 - loss: 1.0223 - val_accuracy: 0.3457 - val_loss: 2.8335\n",
      "Epoch 56/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6811 - loss: 0.9899 - val_accuracy: 0.3468 - val_loss: 2.7287\n",
      "Epoch 57/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6836 - loss: 0.9754 - val_accuracy: 0.3492 - val_loss: 2.8347\n",
      "Epoch 58/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 14ms/step - accuracy: 0.6896 - loss: 0.9534 - val_accuracy: 0.3400 - val_loss: 2.8014\n",
      "Epoch 59/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.6931 - loss: 0.9344 - val_accuracy: 0.3431 - val_loss: 2.8435\n",
      "Epoch 60/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.6854 - loss: 0.9606 - val_accuracy: 0.3411 - val_loss: 2.9037\n",
      "Epoch 61/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7048 - loss: 0.9211 - val_accuracy: 0.3442 - val_loss: 2.8899\n",
      "Epoch 62/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7018 - loss: 0.8967 - val_accuracy: 0.3453 - val_loss: 2.8797\n",
      "Epoch 63/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7062 - loss: 0.9066 - val_accuracy: 0.3461 - val_loss: 2.8792\n",
      "Epoch 64/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7107 - loss: 0.8780 - val_accuracy: 0.3466 - val_loss: 2.9512\n",
      "Epoch 65/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7350 - loss: 0.8264 - val_accuracy: 0.3499 - val_loss: 2.9920\n",
      "Epoch 66/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7165 - loss: 0.8547 - val_accuracy: 0.3490 - val_loss: 2.9907\n",
      "Epoch 67/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7277 - loss: 0.8227 - val_accuracy: 0.3468 - val_loss: 2.9412\n",
      "Epoch 68/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7334 - loss: 0.8210 - val_accuracy: 0.3510 - val_loss: 2.9887\n",
      "Epoch 69/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7289 - loss: 0.8325 - val_accuracy: 0.3534 - val_loss: 3.0197\n",
      "Epoch 70/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7402 - loss: 0.7956 - val_accuracy: 0.3495 - val_loss: 3.0414\n",
      "Epoch 71/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.7386 - loss: 0.7928 - val_accuracy: 0.3499 - val_loss: 3.0554\n",
      "Epoch 72/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7529 - loss: 0.7546 - val_accuracy: 0.3475 - val_loss: 3.0944\n",
      "Epoch 73/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7378 - loss: 0.7814 - val_accuracy: 0.3499 - val_loss: 3.0402\n",
      "Epoch 74/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7457 - loss: 0.7796 - val_accuracy: 0.3446 - val_loss: 3.0907\n",
      "Epoch 75/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7512 - loss: 0.7736 - val_accuracy: 0.3552 - val_loss: 2.9917\n",
      "Epoch 76/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7404 - loss: 0.7857 - val_accuracy: 0.3521 - val_loss: 3.0475\n",
      "Epoch 77/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7425 - loss: 0.7690 - val_accuracy: 0.3503 - val_loss: 3.1691\n",
      "Epoch 78/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7608 - loss: 0.7283 - val_accuracy: 0.3514 - val_loss: 3.1368\n",
      "Epoch 79/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7626 - loss: 0.7157 - val_accuracy: 0.3510 - val_loss: 3.1636\n",
      "Epoch 80/80\n",
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.7527 - loss: 0.7410 - val_accuracy: 0.3565 - val_loss: 3.1694\n",
      "\u001b[1m142/142\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.3758 - loss: 3.0897\n",
      "Test loss: 3.1694, Test accuracy: 0.3565\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "def create_wide_conv_model(vocab_size, max_len, num_classes, embedding_matrix): \n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_matrix.shape[1],\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=max_len,\n",
    "            trainable=False  # freeze embeddings, set True if you want to fine-tune\n",
    "        ),\n",
    "        Conv1D(128, kernel_size=3, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(1024, activation='relu'),\n",
    "        Dropout(0.3 ),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model  \n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "#model = create_wide_conv_model(vocab_size, MAX_LEN, num_classes, embedding_matrix)\n",
    "model = create_wide_conv_model(vocab_size, MAX_LEN, num_classes, embedding_matrix)\n",
    "\n",
    "# === 10. Setup callbacks ===\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# === 11. Train model with validation on test set ===\n",
    "history = model.fit(\n",
    "    X_train_padded, y_train,\n",
    "    epochs=80,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_test_padded, y_test),\n",
    "    #callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === 12. Evaluate on test set ===\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f8e7742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.2559\n",
      "Test loss: 0.2578, Train accuracy: 0.9255\n"
     ]
    }
   ],
   "source": [
    "# === 12. Evaluate on test set ===\n",
    "loss, accuracy = model.evaluate(X_train_padded, y_train, verbose=1)\n",
    "print(f\"Test loss: {loss:.4f}, Train accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49bc9736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    top_k_accuracy_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_classification_model(model, X, y_true, output_path, prefix=\"test\", batch_size=32, top_k=3, label_encoder=None):\n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Top-k Accuracy (optional)\n",
    "    top_k_acc = top_k_accuracy_score(y_true, y_probs, k=top_k)\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True,target_names=le.classes_)\n",
    "    report_df = pd.DataFrame(report).round(2).transpose()\n",
    "    report_df.loc[\"accuracy\"] = acc\n",
    "    report_df.loc[f\"top_{top_k}_accuracy\"] = top_k_acc\n",
    "\n",
    "    # Save Report\n",
    "    report_df.to_csv(f\"{output_path}/cls_report_{prefix}.csv\", index=True)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if label_encoder is not None:\n",
    "        xticks = yticks = label_encoder.classes_\n",
    "        label_map = dict(enumerate(label_encoder.classes_))\n",
    "    else:\n",
    "        xticks = yticks = np.arange(len(np.unique(y_true)))\n",
    "        label_map = None\n",
    "    print(f\"label map: {label_map}\")\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=xticks, yticklabels=yticks)\n",
    "    plt.title(f\"Confusion Matrix - {prefix}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path}/confusion_matrix_{prefix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        f\"top_{top_k}_accuracy\": top_k_acc,\n",
    "        \"classification_report\": report_df,\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58a0f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map: None\n",
      "label map: None\n",
      "0.3565275016567263\n",
      "0.9204545454545454\n"
     ]
    }
   ],
   "source": [
    "test_results = evaluate_classification_model(model, X_test_padded, y_test, './data', prefix=\"_norm_test_wideconv_fastext_epoch80\")\n",
    "train_results = evaluate_classification_model(model, X_train_padded, y_train, './data', prefix=\"norm_train_convlstm_fasttext_epoch80\")\n",
    "print(test_results['accuracy'])\n",
    "print(train_results['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
