{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1077b4e4",
   "metadata": {},
   "source": [
    "###this is to run under myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523ea836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n",
      "1.24.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cae806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myTokenize import SyllableTokenizer\n",
    "from myTokenize import WordTokenizer\n",
    "def word_tokenize(text):\n",
    "    tokenizer = SyllableTokenizer(engine=\"myWord\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6fcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from myTokenize import WordTokenizer\n",
    "\n",
    "def tokenize_burmese_file(input_file, text_column='text', output_file='tokenized_output.xlsx', join_tokens=True):\n",
    "    \"\"\"\n",
    "    Tokenizes Burmese text from an input CSV or Excel file using SyllableTokenizer.\n",
    "    \n",
    "    Parameters:\n",
    "        input_file (str): Path to the input file (.csv or .xlsx).\n",
    "        text_column (str): Name of the column containing Burmese text.\n",
    "        output_file (str): Path to save the output file.\n",
    "        join_tokens (bool): If True, join tokens with space. If False, keep tokens as list.\n",
    "    \"\"\"\n",
    "    # Define tokenizer\n",
    "    tokenizer = WordTokenizer(engine=\"CRF\")\n",
    "\n",
    "    # Read input file\n",
    "    if input_file.endswith('.xlsx'):\n",
    "        df = pd.read_excel(input_file)\n",
    "    elif input_file.endswith('.csv'):\n",
    "        df = pd.read_csv(input_file)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use .csv or .xlsx\")\n",
    "\n",
    "    # Ensure the text column exists\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{text_column}' not found in the input file.\")\n",
    "    print(f\"Shape of input data: {df.shape}\")\n",
    "    # Remove trailing white spaces from the text column (say column is 'text')\n",
    "    df[text_column] = df[text_column].astype(str).str.strip()\n",
    "\n",
    "    # Remove duplicate rows based on the text column (or entire row if you want)\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "    # Tokenize row by row\n",
    "    if join_tokens:\n",
    "        df['tokens'] = df[text_column].astype(str).apply(lambda x: ' '.join(tokenizer.tokenize(x)))\n",
    "    else:\n",
    "        df['tokens'] = df[text_column].astype(str).apply(lambda x: tokenizer.tokenize(x))\n",
    "    \n",
    "    print(df['tokens'].head())  # Display first few tokenized rows for verification\n",
    "\n",
    "    # Save to output file\n",
    "    if output_file.endswith('.xlsx'):\n",
    "        df.to_excel(output_file, index=False)\n",
    "    elif output_file.endswith('.csv'):\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    else:\n",
    "        raise ValueError(\"Output file must end with .csv or .xlsx\")\n",
    "\n",
    "    print(f\"Tokenized file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4503f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data: (949, 1)\n",
      "Shape after removing duplicates: (903, 1)\n",
      "0    ယနေ့ ည မန္တလေး - လား ရှိုးလမ်း တွင် တွဲ ကား ချ...\n",
      "1    ပြင် ဦးလွင် နယ်မြေ (၃ ) ၊မန္တလေး - လား ရှိုးလမ...\n",
      "2    အသင်း ထံ သို့ ဆက်သွယ် အ ကူအညီ တောင်းခံ ချက် အ ...\n",
      "3    အမျိုးသား (၂ ) ဦး ၏De a d Body တို့ ကို ပြည်သူ...\n",
      "4    UCLA တက္ကသိုလ် က သိပ္ ပံ ပညာရှင် Willia m Low ...\n",
      "Name: tokens, dtype: object\n",
      "Tokenized file saved to: tokenized_rawnews.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_file = os.path.abspath(os.path.join('datasets','raw_news.csv'))\n",
    "tokenize_burmese_file(input_file,text_column='Sentence', output_file='tokenized_rawnews.csv', join_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c94cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myTokenize import WordTokenizer\n",
    "\n",
    "text = \"ကျန်းမာရေးဝန်ကြီးဌာန၏ သစ်လွင်သားဖွား ခန့်အပ်တာဝန်ပေးအပ်ခြင်းအခမ်းအနားကို ယနေ့ နံနက် (၉:၀၀)နာရီတွင် ရန်ကုန်မြို့ရှိ သူနာပြုတက္ကသိုလ်ခန်းမ၌ကျင်းပရာ ကျန်းမာရေးဝန်ကြီးဌာန၊ ပြည်ထောင်စုဝန်ကြီး ပါမောက္ခဒေါက်တာသက်ခိုင်ဝင်းနှင့် ရန်ကုန်တိုင်းဒေသကြီးအစိုးရအဖွဲ့၊ ဝန်ကြီးချုပ်ဦးစိုးသိန်းတို့မှ တက်ရောက် အမှာစကား ပြောကြားခဲ့ပါသည်။\"\n",
    "tokenizer = WordTokenizer(engine=\"myWord\")\n",
    "words = tokenizer.tokenize(text)\n",
    "print(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662cfcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['မြန်မာ', 'နိုင်ငံ', '။']\n"
     ]
    }
   ],
   "source": [
    "from myTokenize import WordTokenizer\n",
    "\n",
    "tokenizer = WordTokenizer(engine=\"myWord\")\n",
    "words = tokenizer.tokenize(\"မြန်မာနိုင်ငံ။\")\n",
    "print(words)  # ['မြန်မာ', 'နိုင်ငံ', '။']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775963cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Text:\n",
      "ထို့ နောက် ပြည်ထောင်စု အမျိုးသား လွှတ်တော် ဒုတိယ ဥက္ကဋ္ဌ ဦးအေးသာ အောင် က ဂုဏ်ပြု အ မှာ စကား ပြော ကြား ပေး ခဲ့ ပြီး တိုင်းရင်းသား လူမျိုး များ ရေး ရာဝန်ကြီးဌာန ပြည်ထောင်စု ဝန်ကြီး နိုင်သက်လွင် က \" ကျွန်တော် တို့ တိုင်းရင်းသား အခွင့် အရေး များ ကို ကာကွယ် စောင့် ရှောက် ခြင်း အပြင် တိုင်းရင်းသား တို့ ရဲ့ စကား ၊ စာပေ ၊ယဉ်ကျေး မှု ဓလေ့ထုံးစံ များ ၊ အမျိုးသား ရေး လက္ခဏာ များ မ ပျောက် မ ပျက်အောင် ထိန်း သိမ်းဆောင်ရွက် လျက်ရှိ ပါ ကြောင်း၊ တစ်ဖက် က လည်း ဒေသ ဖွံ့ဖြိုးရေး အ တွက် ဘယ်လို ဆောင်ရွက် ရ မယ် ဆို တဲ့ အစီအ အ မံ တွေ ၊ အတွေး အ ခေါ် အမြှော် အမြင် တွေ ကို လည်း ပံပိုးကူညီ ဆောင်ရွက် ပေး သွား မည် ဖြစ် ပါ ကြောင်း၊ တိုင်းရင်းသား လူမျိုး များ ရဲ့ နေ့ထူး နေ့မြတ် အခမ်းအနား များ ကို လည်း လွတ်လွတ်လပ်လပ်ခမ်းခမ်းနားနားကျင်းပ ခွင့် ရရှိ စေ ရေး နှင့် သက်ဆိုင်ရာ ပြည်နယ်/ တိုင်း ဒေသ ကြီး အစိုးရ အဖွဲ့ က လည်း လို အပ် တဲ့ ရံပုံငွေ ထောက်ပံ့ ကူညီ ပေး နိုင် ဖို့ လည်းညှို နှိုင်းဆောင်ရွက် ပေး လျက်ရှိ ပါ ကြောင်း၊ တိုင်းရင်းသား များ ရဲ့ လူမှု ဘဝ တိုးတက် ရေး နှင့် ယဉ်ကျေး မှု အ မွေအ နှစ် များ ထိန်း သိမ်း စောင့် ရှောက် နိုင် ရေး အ တွက် တစ် ဦး တစ် ယောက် ဒေသ တစ် ခု ချင်း အ လိုက်တင်ပြ ဆောင်ရွက် ခြင်း မျိုး မ ဟုတ် ဘဲ တိုင်းရင်းသား အ လိုက် ဘက်စုံဖွံ့ဖြိုး တိုး တတ် ရေးကော်မတီ များ ဖွဲ့စည်းဆောင်ရွက် ပြီး အချက်အလက်မှတ်တမ်း ဓာတ်ပုံ များ နှင့် အတူ တင်ပြ မှ သာ သက်ဆိုင်ရာဝန်ကြီးဌာန များ နှင့် မိမိ တို့ က ညှိနှိုင်းတင်ပြ ဆောင်ရွက် ပေး နိုင် မှာ ဖြစ် ပါ ကြောင်း ၊ နိုင်ငံ မှာ ရှိ သော တိုင်းရင်းသား များ ဖွံ့ဖြိုး တိုးတက် စေ ရေး အ တွက် စီးပွား ရေး တောင့်တင်း ပြီး လူမှု အ သိုက်အဝန်း တည်ငြိမ် အေးချမ်း ရေး မှာ အရေးကြီး သည့်အတွက် ကြောင့် နိုင်ငံတော် အစိုးရ ကတိုင်းရင်းသား လက်နက် ကိုင် အဖွဲ့အစည်း တွေ နှင့် တစ် မျိုးသားလုံး အ ပစ် အခတ် ရပ်စဲ ရေး သဘောတူ စာချုပ် ( NCA ) ကို လက်မှတ်ရေး ထိုးပြီး ပြည်ထောင်စု ငြိမ်းချမ်း ရေး ( ၂၁ ) ရာစု ပင် လုံညီလာခံ ကျင်းပ နေ ခြင်း ဟာ တိုင်းရင်းသား များ အားလုံး လို ချင် နေ တဲ့ ဒီမိုကရေစီ ဖက်ဒရယ် ပြည်ထောင်စု ကို ထူထောင် နိုင် ရန် ပြည်သူ များ ၏ ဆန္ဒ နဲ့ အ ညီ အခြေခံ ဥပဒေ ကို ရေးဆွဲ နိုင် ဖို့ ဖြစ် ပါ ကြောင်း နှင့်ဖြစ် ပေါ် လာ မည့် အခြေခံ ဥပဒေ နှင့် နိုင်ငံတော် ကို တည်ဆောက် နိုင် မှ သာ ထာဝရ ငြိမ်းချမ်း ရေး ကို ရရှိ မည် ဖြစ် ပြီး နိုင်ငံတော်ဖွံ့ဖြိုး တိုးတက် အောင်ဆောင်ရွက် နိုင် မည် ဖြစ် ပါ ကြောင်း နှင့် ယခု ကာလ မှာ လူငယ်ထု အားခြိမ်း ခြောက် နေ သော မူးယစ်ဆေးဝါးအန္တရာယ် ကို အမျိုးသား ရေး အ မြင် ဖြစ် သတိထား ပြီး တိုင်းရင်းသား အားလုံး က အ တူ တကွ ပူးပေါင်း တိုက်ဖျက်သွား ကြ ရန် လို အပ် လျက်ရှိ ပါ သည် \" ဟု ပြော ကြား ခဲ့ သည် ။\n",
      "Length: 2323\n",
      "\n",
      "5 Shortest Texts:\n",
      "- (5 chars) ဂျပန်\n",
      "- (7 chars) နိဒါန်း\n",
      "- (7 chars) ဟယ် လို\n",
      "- (8 chars) ဘီဘီစီ ။\n",
      "- (8 chars) အမေရိကန်\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_file = os.path.abspath(os.path.join('datasets','tokenized_ner.csv'))\n",
    "# Load your file (adjust file name and format as needed)\n",
    "df = pd.read_csv(input_file)  # or pd.read_excel('your_file.xlsx')\n",
    "\n",
    "# Make sure your text column is in string format\n",
    "df['tokens'] = df['tokens'].astype(str)\n",
    "\n",
    "# Add a new column with text length\n",
    "df['text_length'] = df['tokens'].str.len()\n",
    "\n",
    "# Find the longest text\n",
    "longest_text = df.loc[df['text_length'].idxmax()]\n",
    "print(\"Longest Text:\")\n",
    "print(longest_text['tokens'])\n",
    "print(\"Length:\", longest_text['text_length'])\n",
    "\n",
    "# Find the 5 shortest texts\n",
    "shortest_texts = df.nsmallest(5, 'text_length')\n",
    "print(\"\\n5 Shortest Texts:\")\n",
    "for i, row in shortest_texts.iterrows():\n",
    "    print(f\"- ({row['text_length']} chars) {row['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Step 1: Load the CSV\n",
    "# df = pd.read_csv(\"datasets/ner_raw_final.csv\", header=None, names=[\"Sentence\"])\n",
    "\n",
    "# # Step 2: Clean only rows that start with SNT.xxx.xxx\n",
    "# df[\"Sentence\"] = df[\"Sentence\"].apply(\n",
    "#     lambda x: re.sub(r'^SNT\\.\\d+\\.\\d+\\s*', '', str(x)) if str(x).startswith(\"SNT.\") else x\n",
    "# )\n",
    "\n",
    "# # Step 3: Save to a new cleaned file\n",
    "# df.to_csv(\"datasets/ner_raw_final_cleanedSNT.csv\", index=False, header=False, encoding='utf-8-sig')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
