{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a588d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_burmese_sentences(input_file, output_file=\"output_sentences.xlsx\"):\n",
    "    # Read the Excel file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Replace NaN with empty string to avoid issues\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    # Flatten all columns into one single column as lines\n",
    "    all_text = df.astype(str).apply(lambda x: '\\n'.join(x), axis=1).str.cat(sep='\\n')\n",
    "\n",
    "    # Split by newline, clean whitespace, and skip empty lines\n",
    "    lines = [line.strip() for line in all_text.split('\\n') if line.strip()]\n",
    "\n",
    "    # Split sentences\n",
    "    sentences = []\n",
    "    for line in lines:\n",
    "        if \"။\" in line:\n",
    "            parts = line.split(\"။\")\n",
    "            for part in parts:\n",
    "                clean_part = part.strip()\n",
    "                if clean_part:\n",
    "                    sentences.append(clean_part + \"။\")\n",
    "        else:\n",
    "            # Keep as-is if no \"။\" and not empty\n",
    "            if line.strip():\n",
    "                sentences.append(line)\n",
    "\n",
    "    # Final cleanup: drop empty or NaN-like strings\n",
    "    sentences = [s for s in sentences if s and s.lower() != 'nan']\n",
    "\n",
    "    # Create DataFrame and export\n",
    "    result_df = pd.DataFrame(sentences, columns=[\"Sentence\"])\n",
    "    result_df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ Cleaned sentences saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5265e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.13 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:03:15) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nuwai\\Anaconda3\\envs\\tf-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56928ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned sentences saved to: ner_final_cleaned_1.csv\n"
     ]
    }
   ],
   "source": [
    "extract_burmese_sentences(\"ner_final_cleaned.csv\", output_file=\"ner_final_cleaned_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c52e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_unwanted(line):\n",
    "    # Step 1: Normalize line by removing all invisible characters\n",
    "    line = str(line)\n",
    "    line = re.sub(r\"[\\r\\n\\t\\u200b\\u2028\\u00a0]\", \"\", line)  # remove invisible unicode\n",
    "    line = line.strip()\n",
    "    line = re.sub(r\"\\s+\", \"\", line)  # remove all spaces\n",
    "\n",
    "    # Step 2: Remove formatting characters like comma, slash, dash\n",
    "    digits_only = re.sub(r\"[,\\-/]\", \"\", line)\n",
    "\n",
    "    # Step 3: Matching rules\n",
    "    return (\n",
    "        line.startswith(\"https://www.bbc.com/burmese/\")\n",
    "        or re.fullmatch(r\"\\d+\", digits_only)                # 5386538 or 5487018\n",
    "        or re.fullmatch(r\"\\d+\\.\\d+\", digits_only)           # decimal number\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_lines(input_file, output_file=\"cleaned_file.xlsx\"):\n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(input_file)\n",
    "\n",
    "    # Replace NaNs with empty strings\n",
    "    df = df.fillna(\"\")\n",
    "\n",
    "    # Extract lines from all columns if needed\n",
    "    if df.shape[1] > 1:\n",
    "        all_text = df.astype(str).apply(lambda x: '\\n'.join(x), axis=1).str.cat(sep='\\n')\n",
    "        lines = [line.strip() for line in all_text.split('\\n') if line.strip()]\n",
    "    else:\n",
    "        lines = df.iloc[:, 0].astype(str).str.strip().tolist()\n",
    "        lines = [line for line in lines if line]\n",
    "\n",
    "\n",
    "    # Filter out unwanted lines\n",
    "    cleaned_lines = [line for line in lines if not is_unwanted(line)]\n",
    "\n",
    "    # Save to Excel\n",
    "    cleaned_df = pd.DataFrame(cleaned_lines, columns=[\"Line\"])\n",
    "    cleaned_df.to_excel(output_file, index=False)\n",
    "    print(f\"✅ Cleaned file saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e59d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved to: bbc_trading_cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "clean_lines(\"mdn_cele_details.xlsx\", \"mdn_cele_details_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391d9cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.13 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:03:15) [MSC v.1929 64 bit (AMD64)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\nuwai\\Anaconda3\\envs\\tf-env\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "# from myTokenize import SyllableTokenizer\n",
    "# from myTokenize import WordTokenizer\n",
    "import sys\n",
    "print(sys.version)\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "\n",
    "# from myTokenize as mt\n",
    "# print(mt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85597412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def add_spaces_around_numbers(input_csv, output_csv, text_column='text'):\n",
    "    burmese_digits = \"၀၁၂၃၄၅၆၇၈၉\"\n",
    "    western_digits = \"0123456789\"\n",
    "\n",
    "    # Regex for digit sequences (Burmese and/or Western), surrounded by non-digit characters\n",
    "    digit_pattern = re.compile(\n",
    "        rf'(?<![\\s{burmese_digits}{western_digits}])([{burmese_digits}{western_digits}]+)(?![\\s{burmese_digits}{western_digits}])'\n",
    "    )\n",
    "\n",
    "    def process_text(text):\n",
    "        # Only add spaces around digit sequences not already spaced and not part of a word\n",
    "        text = digit_pattern.sub(r' \\1 ', text)\n",
    "        # Normalize multiple spaces\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    df = pd.read_csv(input_csv, encoding='utf-8-sig')\n",
    "    df[text_column] = df[text_column].astype(str).apply(process_text)\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"✅ Saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1600615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to cleaned_raw_news.csv\n"
     ]
    }
   ],
   "source": [
    "add_spaces_around_numbers(\n",
    "    input_csv='tokenized_rawnews.csv',\n",
    "    output_csv='cleaned_raw_news.csv',\n",
    "    text_column='tokens'  # <-- Change if your column name is different\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0bacdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Text:\n",
      "မေမြို့ကအဆောက်အဦ ၃၁ လုံး National Heritage စာရင်းဝင်တယ်ဆိုတာကို Maymyo GuideBook fb page မှာ မြင်မိပါတယ်၊ ကျမတို့မိသားစုနေခဲ့တဲ့ အဖေ့ရုံးကဝန်ထမ်းအိမ်ယာ \"ငွေသော်တာ\"ကလည်း အမှတ်စဉ် ၂၇ မှာ ပါတယ်ဆိုတော့ ပိုပြီးဝမ်းသာရ လွမ်းရပါတယ်၊ National Heritage စာရင်းဝင်တဲ့ အဆောက်အဦများနဲ့ပါတ်သက်ပြီး မှတ်မိသမျှ ပြောပါရစေ၊ ကျမတို့ငယ်ငယ်က \"နန်းမြိုင်\"က ဆောက်လုပ်ရေးဧည့်ရိပ်သာပါ၊ ဟိုတယ်မဟုတ်ပါဘူး၊ \"ယုဇနမြိုင် သဇင်မြိုင် ချယ်ရီမြိုင်\" ဆိုတာထက် အစိုးရဧည့်ဂေဟာအမှတ် ၁/၂/၃ လို့ပဲ စိတ်ထဲစွဲနေတာက အိမ်တော်လမ်းကအဆောက်အဦတွေလို့ ထင်ပါတယ်၊ နန်းမြိုင်နဲ့ ဧည့်ဂေဟာ ၁/၂/၃ ကို အဖေနဲ့ ပါသွားဖူးပါတယ်၊ \"ဇီဝက\"ကတော့ ခေတ်အဆက်ဆက် ဆေးရုံအုပ်ကြီးအိမ်ပါ၊ ကျမတို့မေမြို့မှာရှိစဉ်က ဆေးရုံအုပ်ကြီး(ကလေးအထူးကုဆရာဝန်ကြီး)အိမ်မှာ ကျမတို့မောင်နှမ အဖျားအနာရှိရင် သွားပြရပါတယ်၊ အမေ မန္တလေးပြောင်းသွားပြီး အဖေက ကလေး၂ယောက်နဲ့ ကျန်ခဲ့ချိန်မှာ ကလေးတွေနေမကောင်းလို့ အမေ့ဆီကိုဖုန်းဆက်ပြောရင် အမေက ဆရာကြီးအိမ်သွားပြပါလို့ပဲ ပြောပါတယ်၊ \"ဇီဝက\" ကို ကျော်သွားပြီး ကလပ်လမ်းထဲချိုးဝင်လိုက်ရင် \"ငွေသော်တာ\" ရှိပါတယ်၊ အဲဒီကလပ်လမ်းပေါ်မှာပဲ ငွေသော်တာကိုနည်းနည်းကျော်သွားရင် လင်းပင်မင်းသားကြီးအိမ် ရှိပါတယ်၊ ၁၉၇၃ /၁၉၇၄ လောက်က လင်းပင်မင်းသားကြီးအိမ်မှာ နာရေးအခမ်းအနားတခုကို နန်းဆန်ဆန် လုပ်တာ တွေ့ဖူးပါတယ် (နာရေးပို့တဲ့အချိန်မှာ ပြသာဒ်လို အချွန်အတက်တွေနဲ့ ပြင်ဆင်ထားတာပါ) ပြီးခဲ့တဲ့ရက်အနည်းငယ်က Colin McPhedran ရေးတဲ့ White Butterflies ဆိုတဲ့စာအုပ်ဖတ်ဖြစ်ပါတယ်၊ ၁၉၄၀ ဝန်းကျင်က သူတို့မိသားစုမေမြို့မှာနေခဲ့တဲ့အိမ်က national heritage စာရင်းမှာ ပါလာတဲ့ Jamshed villa ပါ၊ အိန္ဒိယနိုငံသား စက်မှုလုပ်ငန်းရှင်တဦးရဲ့ နွေရာသီစံအိမ်ကို စာရေးသူ Colin ရဲ့အဖေက ဝယ်ခဲ့ပါတယ်၊ မူလပိုင်ရှင်အိန္ဒိယလူမျိုး Tata ဆိုသူက သူ့စက်မှုလုပ်ငန်းများတည်ရှိရာမြို့ဖြစ်တဲ့ Jamshedpur ကိုအစွဲပြုပြီး Jamshed villa လို့ အမည်ပေးခဲ့ပါတယ်၊ စာရေးသူ Colin က Jamshed villa က Circular road မှာ ရှိတယ်လို့ဆိုပါတယ်၊ National Heritage စာရင်းအရဆိုရင် လမ်းသာယာလမ်းပါ၊ ၁၉၄၀ ခုနှစ်ခန့်က Jamshed villa က တောစပ်မှာရှိပြီး ရံဖန်ရံခါ ဥယျာဉ်အတွင်း ကျားများပင် ဝင်ရောက်လာတတ်တယ်လို့ စာရေးသူက ဆိုပါတယ်၊ နောက်တခါ မေမြို့ရောက်ခဲ့ရင် ကျမသွားကြည့်ရမယ့်နေရာပါပဲ၊ သန့်မြင့်ဦးရေးတဲ့စာအုပ်တခုမှာတော့ မေမြို့ကအဆောက်အဦများကို တူရကီအကျဉ်းသားများက ဆောက်ခဲ့ကြတယ်လို့ ဖတ်ဖူးပါတယ်၊ ဘယ်သူပဲဆောက်ခဲ့ ဆောက်ခဲ့ ဒီအဆောက်အဦတွေကြောင့် မေမြို့က ပိုပြီးကျက်သရေရှိနေတာ၊ ခန့်ငြားထယ်ဝါနေတာ အမှန်ပါပဲ၊\n",
      "Length: 2149\n",
      "\n",
      "5 Shortest Texts:\n",
      "- (5 chars) ဂျပန်\n",
      "- (6 chars) မရဘူး။\n",
      "- (6 chars) အလာပြ။\n",
      "- (6 chars) ဟယ်လို\n",
      "- (7 chars) ဘီဘီစီ။\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#input_file = os.path.abspath(os.path.join('datasets','tokenized_ner.csv'))\n",
    "input_file = 'ner_final_cleaned_3.csv'  # Adjust as needed\n",
    "text_column = 'Sentence'  # Change if your column name is different\n",
    "# Load your file (adjust file name and format as needed)\n",
    "df = pd.read_csv(input_file)  # or pd.read_excel('your_file.xlsx')\n",
    "\n",
    "# Make sure your text column is in string format\n",
    "df[text_column] = df[text_column].astype(str)\n",
    "\n",
    "# Add a new column with text length\n",
    "df['text_length'] = df[text_column].str.len()\n",
    "\n",
    "# Find the longest text\n",
    "longest_text = df.loc[df['text_length'].idxmax()]\n",
    "print(\"Longest Text:\")\n",
    "print(longest_text[text_column])\n",
    "print(\"Length:\", longest_text['text_length'])\n",
    "\n",
    "# Find the 5 shortest texts\n",
    "shortest_texts = df.nsmallest(5, 'text_length')\n",
    "print(\"\\n5 Shortest Texts:\")\n",
    "for i, row in shortest_texts.iterrows():\n",
    "    print(f\"- ({row['text_length']} chars) {row[text_column]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef66583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_text_column(input_file, output_file, column_name='text'):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, cleans the specified text column by stripping spaces\n",
    "    and removing duplicates, prints shape before and after, and saves to output.\n",
    "\n",
    "    Parameters:\n",
    "        input_file (str): Path to input CSV file.\n",
    "        output_file (str): Path to save the cleaned CSV file.\n",
    "        column_name (str): Column name to clean. Default is 'text'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "\n",
    "    # Strip leading/trailing spaces from the specified column\n",
    "    df[column_name] = df[column_name].astype(str).str.strip()\n",
    "\n",
    "    # Drop duplicate rows based on the specified column\n",
    "    df = df.drop_duplicates(subset=[column_name])\n",
    "    print(f\"Cleaned shape: {df.shape}\")\n",
    "\n",
    "    # Save the cleaned DataFrame to CSV\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Cleaned CSV saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab6d1b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (72656, 1)\n",
      "Cleaned shape: (71756, 1)\n",
      "Cleaned CSV saved to: ner_final_cleaned_2.csv\n"
     ]
    }
   ],
   "source": [
    "input_file ='ner_final_cleaned_1.csv'\n",
    "clean_text_column(input_file, output_file='ner_final_cleaned_2.csv', column_name='Sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f9c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_text_noise(input_file, output_file, text_column='text'):\n",
    "    \"\"\"\n",
    "    Reads a CSV, removes noisy HTML entities and patterns from the text column, and saves the cleaned version.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_file (str): Path to the input CSV file\n",
    "    - output_file (str): Path to save the cleaned CSV file\n",
    "    - text_column (str): Name of the text column to clean (default = 'text')\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        if pd.isnull(text):\n",
    "            return text\n",
    "        # Remove patterns like &#1234; or &#1234 or &#;\n",
    "        text = re.sub(r'&#\\d{1,5};?', '', text)\n",
    "        # Remove standalone HTML entities like &#8221, &quot;, etc.\n",
    "        text = re.sub(r'&[#a-zA-Z0-9]+;?', '', text)\n",
    "        # Remove things like […] or anything inside [ ]\n",
    "        text = re.sub(r'\\[.*?\\]', '', text)\n",
    "        # Remove weird unicode quotes and extra spaces\n",
    "        text = text.replace('”', '').replace('“', '').replace('…', '')\n",
    "        return text.strip()\n",
    "\n",
    "    # Read file\n",
    "    df = pd.read_csv(input_file, encoding='utf-8-sig')\n",
    "\n",
    "    # Show shape before\n",
    "    print(\"Before cleaning:\", df.shape)\n",
    "\n",
    "    # Clean text column\n",
    "    if text_column in df.columns:\n",
    "        df[text_column] = df[text_column].astype(str).apply(clean_text)\n",
    "    else:\n",
    "        raise ValueError(f\"'{text_column}' column not found in the CSV.\")\n",
    "\n",
    "    # Drop duplicates and strip trailing spaces\n",
    "    df[text_column] = df[text_column].str.strip()\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "\n",
    "    # Show shape after\n",
    "    print(\"After cleaning:\", df.shape)\n",
    "\n",
    "    # Save cleaned file\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Cleaned CSV saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81de269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning: (71740, 1)\n",
      "After cleaning: (71734, 1)\n",
      "Cleaned CSV saved to ner_final_cleaned_3.csv\n"
     ]
    }
   ],
   "source": [
    "clean_text_noise('ner_final_cleaned_2.csv', 'ner_final_cleaned_3.csv', text_column='Sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45cdf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def jsonl_to_bio(jsonl_path: str, output_path: str):\n",
    "    def label_tokens(text, entities):\n",
    "        tags = ['O'] * len(text)\n",
    "        for ent in entities:\n",
    "            start, end, label = ent['start_offset'], ent['end_offset'], ent['label']\n",
    "            tags[start] = f\"B-{label}\"\n",
    "            for i in range(start + 1, end):\n",
    "                tags[i] = f\"I-{label}\"\n",
    "        return tags\n",
    "\n",
    "    with open(jsonl_path, encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            example = json.loads(line)\n",
    "            text = example['text']\n",
    "            entities = example['entities']\n",
    "            if not text.strip():\n",
    "                continue\n",
    "\n",
    "            char_tags = label_tokens(text, entities)\n",
    "\n",
    "            for char, tag in zip(text, char_tags):\n",
    "                if char.strip() == \"\":\n",
    "                    continue  # skip whitespace\n",
    "                outfile.write(f\"{char} {tag}\\n\")\n",
    "            outfile.write(\"\\n\")  # sentence boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e4629da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: text, Label: ORG\n",
      "Entity: ဒေ, Label: Title\n",
      "Entity: ါ်အောင်ဆန, Label: PER\n",
      "Entity: ဒေ, Label: Title\n",
      "Entity: ် အောင်ဆန, Label: PER\n",
      "Entity: ဒေ, Label: Title\n",
      "Entity: ါ်အောင်ဆန်, Label: PER\n",
      "Entity: ကျောင်းသား လက်န, Label: ORG\n",
      "Entity:  တပ်မမှူး ဗို, Label: ORG\n",
      "Entity: ျောင်, Label: ORG\n",
      "Entity: ိမ, Label: ORG\n",
      "Entity: ှိ သူ, Label: ORG\n",
      "Entity: ့ လူ ချင်း , Label: Title\n",
      "Entity: က် ချိန, Label: ORG\n",
      "Entity: ြို့ , Label: ORG\n",
      "Entity: အမျိုးသား ညီညွတ် ရ, Label: ORG\n",
      "Entity: အစိ, Label: ORG\n",
      "Entity: ေသ ရဲ့ အင်, Label: Title\n",
      "Entity: ာင်း, Label: ORG\n",
      "Entity:  ပေ ့ါ, Label: ORG\n",
      "Entity:  အတွင်း, Label: ORG\n",
      "Entity: ှာ တ, Label: ORG\n",
      "Entity: ABS, Label: ORG\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def jsonl_to_bioes_token_spans(jsonl_path, output_path):\n",
    "    def get_spans(text, entities):\n",
    "        spans = []\n",
    "        last = 0\n",
    "        entities = sorted(entities, key=lambda x: x['start_offset'])\n",
    "        for ent in entities:\n",
    "            start, end = ent['start_offset'], ent['end_offset']\n",
    "            label = ent.get('label') or ent.get('labels')\n",
    "            # Non-entity span before this entity\n",
    "            if start > last:\n",
    "                spans.append((text[last:start], 'O'))\n",
    "            # Entity span\n",
    "            ent_text = text[start:end]\n",
    "            print(f\"Entity: {ent_text}, Label: {label}\")\n",
    "            ent_len = end - start\n",
    "            if ent_len == 1:\n",
    "                spans.append((ent_text, f'S-{label}'))\n",
    "            else:\n",
    "                # BIOES for multi-token entity\n",
    "                # If you want to split entity span into sub-tokens, do it here\n",
    "                spans.append((ent_text, f'B-{label}'))\n",
    "                # No further splitting, so E- for last token\n",
    "                # If you want to split into syllables/words, you can add that logic\n",
    "                # For now, treat the whole span as one token\n",
    "                # If more than 2 chars, add I- tags for middle chars\n",
    "                if ent_len > 2:\n",
    "                    # Middle part\n",
    "                    middle = ent_text[1:-1]\n",
    "                    if middle:\n",
    "                        spans.append((middle, f'I-{label}'))\n",
    "                # End part\n",
    "                if ent_len > 1:\n",
    "                    spans.append((ent_text[-1], f'E-{label}'))\n",
    "            last = end\n",
    "        # Remaining non-entity span\n",
    "        if last < len(text):\n",
    "            spans.append((text[last:], 'O'))\n",
    "        return spans\n",
    "\n",
    "    with open(jsonl_path, encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            example = json.loads(line)\n",
    "            text = example['text']\n",
    "            entities = example.get('entities', [])\n",
    "            spans = get_spans(text, entities)\n",
    "            for token, tag in spans:\n",
    "                token = token.strip()\n",
    "                if token:\n",
    "                    outfile.write(f\"{token} {tag}\\n\")\n",
    "            outfile.write(\"\\n\")  # sentence boundary\n",
    "\n",
    "# Usage:\n",
    "jsonl_to_bioes_token_spans(\"all.jsonl\", \"output_bioes.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "822183cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_to_bioes(\"all.jsonl\", \"output_bioes.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18df114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Apple, Label: ORG, Start: 0, End: 5\n",
      "Entity: U.K., Label: GPE, Start: 27, End: 31\n",
      "Entity: $1 billion, Label: MONEY, Start: 44, End: 54\n",
      "Entity: 2025, Label: DATE, Start: 58, End: 62\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "def check_ner_entities(text, model_name=\"en_core_web_sm\"):\n",
    "    # Load the spaCy model (download if necessary: python -m spacy download en_core_web_sm)\n",
    "    nlp = spacy.load(model_name)\n",
    "\n",
    "    # Process the input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            \"text\": ent.text,\n",
    "            \"label\": ent.label_,\n",
    "            \"start_char\": ent.start_char,\n",
    "            \"end_char\": ent.end_char\n",
    "        })\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Example usage\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion in 2025.\"\n",
    "result = check_ner_entities(text)\n",
    "\n",
    "for ent in result:\n",
    "    print(f\"Entity: {ent['text']}, Label: {ent['label']}, Start: {ent['start_char']}, End: {ent['end_char']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed1eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ဒေါ်\n",
      "်အော\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "text= 'ဒေါ်အောင်ဆန်းစုကြည်သည် မြန်မာနိုင်ငံ၏ ပထမဆုံး အမျိုးသမီး ဥက္ကဌ ဖြစ်သည်။' \n",
    "print(text[0:4])\n",
    "print(text[3:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a335ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
