{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 88034 token-tag pairs to datasets/annotate_with_ai/clean_annotated_ner_1.conll\n"
     ]
    }
   ],
   "source": [
    "# to extract NER tags only from label-studio output\n",
    "def extract_tokens_and_ner_tags(lines):\n",
    "    token_tag_pairs = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            token_tag_pairs.append((\"\", \"\"))  # Sentence boundary\n",
    "            continue\n",
    "\n",
    "        # Handle format: token -X- _ TAG\n",
    "        if '-X-' in line and '_' in line:\n",
    "            try:\n",
    "                parts = line.split()\n",
    "                token = parts[0]\n",
    "                tag = parts[-1]\n",
    "                token_tag_pairs.append((token, tag))\n",
    "            except:\n",
    "                continue\n",
    "        # Handle: token \\t TAG\n",
    "        elif '\\t' in line:\n",
    "            token, tag = line.split('\\t')\n",
    "            token_tag_pairs.append((token.strip(), tag.strip()))\n",
    "        # Handle: token TAG\n",
    "        elif len(line.split()) == 2:\n",
    "            token, tag = line.split()\n",
    "            token_tag_pairs.append((token.strip(), tag.strip()))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return token_tag_pairs\n",
    "\n",
    "def save_to_conll(pairs, output_file=\"output.conll\"):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for token, tag in pairs:\n",
    "            if token == \"\" and tag == \"\":\n",
    "                f.write(\"\\n\")  # Sentence break\n",
    "            else:\n",
    "                f.write(f\"{token} {tag}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"datasets/annotated_ner_1.conll\"     # Update this to your input file\n",
    "    output_path = \"datasets/annotate_with_ai/clean_annotated_ner_1.conll\"       # Output path\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    pairs = extract_tokens_and_ner_tags(lines)\n",
    "    save_to_conll(pairs, output_path)\n",
    "\n",
    "    print(f\"✅ Saved {len(pairs)} token-tag pairs to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e9fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_txt_file(input_txt_path, output_dir, lines_per_chunk=7000):\n",
    "    # Read all lines from the .txt file\n",
    "    with open(input_txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.rstrip() for line in f if line.strip()]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Split and save chunks\n",
    "    start_line = 27001  # Starting from line 27002\n",
    "    file_index = 1\n",
    "    while start_line < len(lines):\n",
    "        end_line = min(start_line + lines_per_chunk, len(lines))\n",
    "        chunk_lines = lines[start_line:end_line]\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"split_part_{file_index}.txt\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(chunk_lines))\n",
    "\n",
    "        print(f\"Saved: {output_path} ({len(chunk_lines)} lines)\")\n",
    "\n",
    "        start_line = end_line\n",
    "        file_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: datasets/annotate_with_ai\\split_part_1.txt (7000 lines)\n",
      "Saved: datasets/annotate_with_ai\\split_part_2.txt (7000 lines)\n",
      "Saved: datasets/annotate_with_ai\\split_part_3.txt (7000 lines)\n",
      "Saved: datasets/annotate_with_ai\\split_part_4.txt (7000 lines)\n",
      "Saved: datasets/annotate_with_ai\\split_part_5.txt (7000 lines)\n",
      "Saved: datasets/annotate_with_ai\\split_part_6.txt (7000 lines)\n",
      "Saved: datasets/annotate_with_ai\\split_part_7.txt (2713 lines)\n"
     ]
    }
   ],
   "source": [
    "# input_path = 'datasets/ner_text.txt'\n",
    "# output_path = \"datasets/annotate_with_ai\"\n",
    "# split_txt_file(input_path, output_path, lines_per_chunk=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73823284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pos_to_conll(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert POS-tagged text (word/POS format) to CoNLL format.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the input .txt file.\n",
    "        output_path (str): Path to the output .conll file.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue  # skip empty lines\n",
    "\n",
    "            tokens = line.split()\n",
    "            for token in tokens:\n",
    "                if '/' in token:\n",
    "                    word, pos = token.rsplit('/', 1)\n",
    "                    outfile.write(f\"{word} {pos}\\n\")\n",
    "                else:\n",
    "                    # in case there's a token without POS\n",
    "                    outfile.write(f\"{token} O\\n\")\n",
    "            outfile.write(\"\\n\")  # Sentence separator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad6e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_pos_to_conll(\"ner_myword_tokenize_pos_tagged.txt\", \"pos_tagged_conll.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40071d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_pos_keep_ner(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Remove POS tag from a CoNLL-style file (token POS NER) and keep only token and NER tag.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input file.\n",
    "        output_path (str): Path to save the output file.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                outfile.write(\"\\n\")\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) == 3:\n",
    "                token, _, ner = parts\n",
    "                outfile.write(f\"{token} {ner}\\n\")\n",
    "            else:\n",
    "                # Handle cases with unexpected format\n",
    "                outfile.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3a76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def process_folder(input_folder, output_folder, prefix=\"bio_without_pos\"):\n",
    "    \"\"\"\n",
    "    Process all .txt files in the folder to strip POS and keep only NER tags.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): Folder containing input .txt files.\n",
    "        output_folder (str): Folder to write processed output files.\n",
    "        prefix (str): Prefix to add to output filenames.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        if filename.endswith(\".conll\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_filename = prefix + filename\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            strip_pos_keep_ner(input_path, output_path)\n",
    "            print(f\"Processed: {filename} → {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc3541f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: tagged_no_tag_ner_1.conll\n",
      "Processed: tagged_no_tag_ner_1.conll → bio_without_postagged_no_tag_ner_1.conll\n",
      "Processing file: tagged_no_tag_ner_10.conll\n",
      "Processed: tagged_no_tag_ner_10.conll → bio_without_postagged_no_tag_ner_10.conll\n",
      "Processing file: tagged_no_tag_ner_11.conll\n",
      "Processed: tagged_no_tag_ner_11.conll → bio_without_postagged_no_tag_ner_11.conll\n",
      "Processing file: tagged_no_tag_ner_12.conll\n",
      "Processed: tagged_no_tag_ner_12.conll → bio_without_postagged_no_tag_ner_12.conll\n",
      "Processing file: tagged_no_tag_ner_2.conll\n",
      "Processed: tagged_no_tag_ner_2.conll → bio_without_postagged_no_tag_ner_2.conll\n",
      "Processing file: tagged_no_tag_ner_3.conll\n",
      "Processed: tagged_no_tag_ner_3.conll → bio_without_postagged_no_tag_ner_3.conll\n",
      "Processing file: tagged_no_tag_ner_4.conll\n",
      "Processed: tagged_no_tag_ner_4.conll → bio_without_postagged_no_tag_ner_4.conll\n",
      "Processing file: tagged_no_tag_ner_5.conll\n",
      "Processed: tagged_no_tag_ner_5.conll → bio_without_postagged_no_tag_ner_5.conll\n",
      "Processing file: tagged_no_tag_ner_6.conll\n",
      "Processed: tagged_no_tag_ner_6.conll → bio_without_postagged_no_tag_ner_6.conll\n",
      "Processing file: tagged_no_tag_ner_7.conll\n",
      "Processed: tagged_no_tag_ner_7.conll → bio_without_postagged_no_tag_ner_7.conll\n",
      "Processing file: tagged_no_tag_ner_8.conll\n",
      "Processed: tagged_no_tag_ner_8.conll → bio_without_postagged_no_tag_ner_8.conll\n",
      "Processing file: tagged_no_tag_ner_9.conll\n",
      "Processed: tagged_no_tag_ner_9.conll → bio_without_postagged_no_tag_ner_9.conll\n",
      "Processing file: test.conll\n",
      "Processed: test.conll → bio_without_postest.conll\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"datasets/annotate_with_ai/bio_tagging/corrected\"\n",
    "output_dir = \"datasets/annotate_with_ai/bio_tagging/bio_without_pos\"\n",
    "\n",
    "process_folder(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efeaf925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bioes_to_bio(ner_tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a BIOES tag to BIO format.\n",
    "    \"\"\"\n",
    "    if ner_tag.startswith(\"S-\"):\n",
    "        return \"B-\" + ner_tag[2:]\n",
    "    elif ner_tag.startswith(\"E-\"):\n",
    "        return \"I-\" + ner_tag[2:]\n",
    "    elif ner_tag.startswith(\"B-\") or ner_tag.startswith(\"I-\") or ner_tag == \"O\":\n",
    "        return ner_tag\n",
    "    elif ner_tag.startswith(\"O\"):\n",
    "        return \"O\"\n",
    "    else:\n",
    "        # Unknown tag format, return as-is (or raise warning if needed)\n",
    "        return ner_tag\n",
    "\n",
    "\n",
    "def convert_bioes_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert a BIOES-tagged file to BIO format.\n",
    "    Handles both 2-column (token NER) and 3-column (token POS NER) formats.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input file.\n",
    "        output_path (str): Path to save the BIO-converted output.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                outfile.write('\\n')\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) == 2:\n",
    "                token, ner = parts\n",
    "                bio_ner = bioes_to_bio(ner)\n",
    "                outfile.write(f\"{token} {bio_ner}\\n\")\n",
    "            elif len(parts) == 3:\n",
    "                token, pos, ner = parts\n",
    "                bio_ner = bioes_to_bio(ner)\n",
    "                outfile.write(f\"{token} {pos} {bio_ner}\\n\")\n",
    "            else:\n",
    "                # Malformed line, write it back unchanged (or skip if preferred)\n",
    "                outfile.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "670cb8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: tagged_no_tag_ner_1.conll\n",
      "Converted: tagged_no_tag_ner_1.conll\n",
      "Processing file: tagged_no_tag_ner_10.conll\n",
      "Converted: tagged_no_tag_ner_10.conll\n",
      "Processing file: tagged_no_tag_ner_11.conll\n",
      "Converted: tagged_no_tag_ner_11.conll\n",
      "Processing file: tagged_no_tag_ner_12.conll\n",
      "Converted: tagged_no_tag_ner_12.conll\n",
      "Processing file: tagged_no_tag_ner_2.conll\n",
      "Converted: tagged_no_tag_ner_2.conll\n",
      "Processing file: tagged_no_tag_ner_3.conll\n",
      "Converted: tagged_no_tag_ner_3.conll\n",
      "Processing file: tagged_no_tag_ner_4.conll\n",
      "Converted: tagged_no_tag_ner_4.conll\n",
      "Processing file: tagged_no_tag_ner_5.conll\n",
      "Converted: tagged_no_tag_ner_5.conll\n",
      "Processing file: tagged_no_tag_ner_6.conll\n",
      "Converted: tagged_no_tag_ner_6.conll\n",
      "Processing file: tagged_no_tag_ner_7.conll\n",
      "Converted: tagged_no_tag_ner_7.conll\n",
      "Processing file: tagged_no_tag_ner_8.conll\n",
      "Converted: tagged_no_tag_ner_8.conll\n",
      "Processing file: tagged_no_tag_ner_9.conll\n",
      "Converted: tagged_no_tag_ner_9.conll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def process_folder_bioes_to_bio(input_folder, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for filename in os.listdir(input_folder):\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        if filename.endswith((\".txt\", \".conll\")):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            convert_bioes_file(input_path, output_path)\n",
    "            print(f\"Converted: {filename}\")\n",
    "input_dir = \"datasets/annotate_with_ai/bioes tagging/bioes with pos\"\n",
    "output_dir = \"datasets/annotate_with_ai/bio with pos\"\n",
    "process_folder_bioes_to_bio(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fba5c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def correct_token_tags_in_conll(input_path, output_path, corrections):\n",
    "    \"\"\"\n",
    "    Corrects token-level POS or entity tag based on a corrections dictionary.\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the input .conll file.\n",
    "        output_path (str): Path to write the corrected .conll file.\n",
    "        corrections (dict): Dictionary of token to (correct_pos, correct_entity) tuples.\n",
    "                           Use None for values you don't want to change.\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                outfile.write(\"\\n\")\n",
    "                continue\n",
    "\n",
    "            #parts = line.split()\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 3:\n",
    "                token, pos, ent = parts\n",
    "                if token in corrections:\n",
    "                    correct_pos, correct_ent = corrections[token]\n",
    "                    pos = correct_pos if correct_pos else pos\n",
    "                    ent = correct_ent if correct_ent else ent\n",
    "                #outfile.write(f\"{token} {pos} {ent}\\n\")\n",
    "                outfile.write(f\"{token}\\t{pos}\\t{ent}\\n\")\n",
    "\n",
    "            else:\n",
    "                outfile.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "def apply_corrections_to_directory(input_dir, output_dir, corrections):\n",
    "    \"\"\"\n",
    "    Applies token corrections to all .conll files in a directory (recursively).\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Root input directory containing .conll files.\n",
    "        output_dir (str): Root output directory to save corrected files.\n",
    "        corrections (dict): Dictionary of token to (correct_pos, correct_entity).\n",
    "                            Example: {\"ဘီဘီစီ\": (\"abb\", \"B-ORG\")}\n",
    "    \"\"\"\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.conll'):\n",
    "                input_path = os.path.join(root, filename)\n",
    "                relative_path = os.path.relpath(input_path, input_dir)\n",
    "                output_path = os.path.join(output_dir, relative_path)\n",
    "                os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "                correct_token_tags_in_conll(input_path, output_path, corrections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106f61b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def apply_corrections_to_file(input_file, output_file, corrections):\n",
    "    corrected_lines = []\n",
    "\n",
    "    # Step 1: Read the file first\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                corrected_lines.append(\"\")\n",
    "                continue\n",
    "\n",
    "            #parts = line.split()\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 3:\n",
    "                token, pos, entity = parts\n",
    "                if token in corrections:\n",
    "                    correct_pos, correct_entity = corrections[token]\n",
    "\n",
    "                    # Only replace if needed\n",
    "                    pos = correct_pos if correct_pos is not None else pos\n",
    "                    entity = correct_entity if correct_entity is not None else entity\n",
    "\n",
    "                #corrected_lines.append(f\"{token} {pos} {entity}\")\n",
    "                corrected_lines.append(f\"{token}\\t{pos}\\t{entity}\")\n",
    "            else:\n",
    "                # If the format is wrong, just keep the line as-is\n",
    "                corrected_lines.append(line)\n",
    "\n",
    "    # Step 2: Write to file *after* reading and correcting\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(\"\\n\".join(corrected_lines) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f45f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_token_sequences_in_file(file_path, sequence_corrections):\n",
    "    \"\"\"\n",
    "    Correct sequences of consecutive tokens in a CoNLL file with 3 columns (token, pos, entity).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the input .conll file. The same file will be overwritten.\n",
    "        sequence_corrections (list of dict): Each dict should have:\n",
    "            - 'tokens': List of token strings to match (e.g., [\"စစ်\", \"ကောင်စီ\"])\n",
    "            - 'pos':    List of POS tags to correct to (or None to keep original)\n",
    "            - 'ner':    List of NER tags to correct to\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    corrected_lines = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        matched = False\n",
    "        for correction in sequence_corrections:\n",
    "            tokens = correction['tokens']\n",
    "            pos_tags = correction.get('pos', [None] * len(tokens))\n",
    "            ner_tags = correction['ner']\n",
    "\n",
    "            if i + len(tokens) <= len(lines):\n",
    "                window = lines[i:i+len(tokens)]\n",
    "\n",
    "                if all(\n",
    "                    window[j].strip() and window[j].strip().split()[0] == tokens[j]\n",
    "                    for j in range(len(tokens))\n",
    "                ):\n",
    "                    # Apply corrections\n",
    "                    for j in range(len(tokens)):\n",
    "                        parts = window[j].strip().split()\n",
    "                        #parts = window[j].strip().split(\"\\t\")\n",
    "                        if len(parts) == 3:\n",
    "                            token, old_pos, _ = parts\n",
    "                            new_pos = pos_tags[j] if pos_tags[j] else old_pos\n",
    "                            try:\n",
    "                                new_ner = ner_tags[j]\n",
    "                            except IndexError:\n",
    "                                print(f\"IndexError: ner_tags index out of range at file line {i + j + 1}\")\n",
    "                                print(f\"ner_tags length: {len(ner_tags)}, j: {j}\")\n",
    "                                print(f\"tokens: {tokens}\")\n",
    "                                raise\n",
    "                            #corrected_lines.append(f\"{token} {new_pos} {new_ner}\")\n",
    "                            corrected_lines.append(f\"{token}\\t{new_pos}\\t{new_ner}\")\n",
    "                    # for j in range(len(tokens)):\n",
    "                    #     parts = window[j].strip().split()\n",
    "                    #     #parts = window[j].strip().split(\"\\t\")\n",
    "                    #     if len(parts) == 3:\n",
    "                    #         token, old_pos, _ = parts\n",
    "                    #         new_pos = pos_tags[j] if pos_tags[j] else old_pos\n",
    "                    #         new_ner = ner_tags[j]\n",
    "                    #         #corrected_lines.append(f\"{token} {new_pos} {new_ner}\")\n",
    "                    #         corrected_lines.append(f\"{token}\\t{new_pos}\\t{new_ner}\")\n",
    "\n",
    "                    i += len(tokens)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "        if not matched:\n",
    "            corrected_lines.append(lines[i].strip())\n",
    "            i += 1\n",
    "\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(corrected_lines) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5998d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections4 = {\n",
    "    \"မနက်ဖြန်\" : (\"n\", \"B-DATE\"),\n",
    "\n",
    "   \n",
    " }\n",
    "\n",
    "\n",
    "input_dir = \"datasets/annotate_with_ai/bio_tagging/corrected\"\n",
    "output_dir = \"datasets/annotate_with_ai/bio_tagging/corrected1\"\n",
    "\n",
    "apply_corrections_to_directory(input_dir, output_dir, corrections4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "000ea898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\temp\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\temp\\3entity_annotated_ner_cleaned.conll\n"
     ]
    }
   ],
   "source": [
    "sequence_corrections4 =[  \n",
    "              {\n",
    "        \"tokens\": [\"မယ်\", \"စုံ\",\"မြို့နယ်\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"ကပ်\", \"သပြေ\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"နမ့်\", \"ခုတ်\",\"မြစ်\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "        {\n",
    "        \"tokens\": [\"ကျောက်\", \"တာ\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "        {\n",
    "        \"tokens\": [\"သစ်ဆိမ့်\", \"ကြီး\",\"ရွာ\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "        {\n",
    "        \"tokens\": [\"မြန်မာ့\", \"ငလျင်\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"O\"],\n",
    "      },\n",
    "          {\n",
    "        \"tokens\": [\"မြန်မာ\", \"ငလျင်\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"O\"],\n",
    "      },\n",
    "           {\n",
    "        \"tokens\": [\"ပန်း\", \"တောင်း\",\"မြို့\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "            {\n",
    "        \"tokens\": [\"ညောင်\", \"ကျိုး\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "        {\n",
    "        \"tokens\": [\"ညောင်\", \"ခြေ\",\"ထောက်\",\"ရွာ\"],\n",
    "        \"pos\":    [None,None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "              {\n",
    "        \"tokens\": [\"ဥသျှစ်ပင်\", \"လမ်းဆုံ\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "             {\n",
    "        \"tokens\": [\"ဥသျှစ်ပင်\", \"မြို့\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"မြန်မာ\", \"ြည်တွင်းစစ်\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"O\"],\n",
    "      },\n",
    "          {\n",
    "        \"tokens\": [\"၇\", \"မိုင်\",\"ရွာ\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "       {\n",
    "        \"tokens\": [\"ဓညဝတီ\", \"ရေ\",\"တပ်\",\"ဌာနချုပ်\"],\n",
    "        \"pos\":    [None,None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"ရှောက်\", \"ချောင်း\",\"ကျေးရွာ\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"ခန်း\", \"ခေါင်\",\"ကျွန်း\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"ဖီလာဒဲလ်ဖီယာ\", \"မြို့\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"ဂုတ်\", \"စီး\",\"ရိုး\",\"ကျေးရွာ\"],\n",
    "        \"pos\":    [None,None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "          {\n",
    "        \"tokens\": [\"မယ်\", \"ထော်\",\"သ\",\"လေး\",\"ကျေးရွာ\"],\n",
    "        \"pos\":    [None,None,None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "         {\n",
    "        \"tokens\": [\"ကိုးကန့်\", \"မြို့\"],\n",
    "        \"pos\":    [None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "\n",
    "      {\n",
    "        \"tokens\": [\"ဒေါင်\", \"မေ\", \"ဥယျာဉ်\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "        \"tokens\": [\"ရွှေသောင်ယံ\"],\n",
    "        \"pos\":    [None],\n",
    "        \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "        \"tokens\": [\"ဆိပ်\", \"မူ\", \"ကျေးရွာ\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "       {\n",
    "        \"tokens\": [\"ဆိပ်\", \"မူ\", \"ရွာ\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "       {\n",
    "        \"tokens\": [\"စ\", \"ပေါ့\", \"ကျေးရွာ\"],\n",
    "        \"pos\":    [None,None,None],  \n",
    "        \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကံ\", \"ဆီး\", \"ရွာ\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အ\", \"နန့်\", \"ပါ\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မ\", \"လွန်\", \"မြို့\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ပဲ\", \"ကုန်း\", \"ကြီး\",\"ကျေးရွာ\"],\n",
    "          \"pos\":    [None,None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ပဲ\", \"ကုန်း\", \"ကြီး\",\"ရွာ\"],\n",
    "          \"pos\":    [None,None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မျှား\", \"ပိုင်း\", \"ရွာ\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အနီး\", \"စခန်း\"],\n",
    "          \"pos\":    [None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အပေါက်ဝ\", \"ကျေးရွာ\"],\n",
    "          \"pos\":    [None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"သံခဲ\", \"ချိုင်\", \"ရွာ\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"သံခဲ\", \"ချိုင်\"],\n",
    "          \"pos\":    [None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အုန်း\", \"ဖန်\", \"မြို့နယ်\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ပ\", \"ဒါန်း\", \"ရွာ\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကျောက်\", \"စ\", \"ကွဲ\"],\n",
    "          \"pos\":    [None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "       {\n",
    "          \"tokens\": [\"ကျောက်\", \"စ\", \"ကွဲ\",\"ကျေးရွာ\"],\n",
    "          \"pos\":    [None,None,None,None],  \n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဇာ\", \"ဒက်\", \"ကြီး\", \"ကျွန်း\"],\n",
    "          \"pos\":    [None, None, None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"သံ\", \"ကျွန်း\"],\n",
    "          \"pos\":    [None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"နောင်\", \"လောင်း\", \"ရွာ\"],\n",
    "          \"pos\":    [None, None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"တောင်\", \"ပြို\", \"လက်ဝဲ\", \"မြို့\"],\n",
    "          \"pos\":    [None, None, None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"], \n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကြွေ\", \"ချိုင်\", \"ကျေးရွာ\"],\n",
    "          \"pos\":    [None, None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"နောင်\", \"ဘူ\", \"ရီ\"],\n",
    "          \"pos\":    [None, None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကိုးကန့်\", \"ဒေသ\"],\n",
    "          \"pos\":    [None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကူ\", \"တော်\", \"ရွာ\"],\n",
    "          \"pos\":    [None, None, None],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"စီ\", \"ပါ\", \"ရွာ\"],\n",
    "          \"pos\":    [\"part\", \"part\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မော့\", \"က\", \"နင်\", \"တံတား\"],\n",
    "          \"pos\":    [\"n\", \"ppm\", \"pron\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဖော်\", \"တော်\", \"ဦး\", \"ဘုရား\"],\n",
    "          \"pos\":    [\"v\", \"part\", \"part\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကေ\", \"လာ\", \"ကျေးရွာ\"],\n",
    "          \"pos\":    [\"v\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အင်း\", \"ခြံ\", \"ရွာ\"],\n",
    "          \"pos\":    [\"part\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "       {\n",
    "          \"tokens\": [\"က\", \" ျိန္တလီ\", \"မြို့\"],\n",
    "          \"pos\":    [\"part\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "        {\n",
    "          \"tokens\": [\"လေး\", \"မျက်နှာ\", \"နတ်ကွန်း\"],\n",
    "          \"pos\":    [\"tn\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"O\", \"O\", \"O\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မြ\", \"တောင်\"],\n",
    "          \"pos\":    [\"n\", \"part\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မ\", \"ဟာ\", \"ချိုင်\"],\n",
    "          \"pos\":    [\"part\", \"ppm\", \"n\"],\n",
    "          \"ner\":    [\"O\", \"O\", \"O\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဖင်လန်\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"တွံတေး\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"လှိုင်သာယာ\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အလွမ်း\", \"တောင်\"],\n",
    "          \"pos\":    [\"n\", \"part\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဆူ\", \"ရ\", \"်\", \"ခွား\", \"မြို့နယ်\"],\n",
    "          \"pos\":    [\"part\", \"part\", \"n\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဟိုက်\", \"ပါ\", \"ကျေးရွာ\"],\n",
    "          \"pos\":    [\"v\", \"part\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ပ\", \"ခန်း\", \"ကြီး\", \"မြို့\"],\n",
    "          \"pos\":    [\"part\", \"n\", \"adj\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ခိုင်\", \"ကမ်း\", \"မြို့\"],\n",
    "          \"pos\":    [\"v\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"စကား\", \"ကင်း\", \"ကျေးရွာ\"],\n",
    "          \"pos\":    [\"n\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"သုဝဏ္ဏ\", \"ဝတီ\", \"မြို့\"],\n",
    "          \"pos\":    [\"n\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"လင်း\", \"ခ\", \"ေး\"],\n",
    "          \"pos\":    [\"n\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ကို\", \"ထ\", \"ရီ\", \"မြို့\"],\n",
    "          \"pos\":    [\"ppm\", \"v\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဘာ\", \"တွန်\", \"ပင်လယ်အော်\"],\n",
    "          \"pos\":    [\"pron\", \"v\", \"part\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဘဂ္ဂဒက်\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"အိန္ဒိယ\", \"သံရုံး\"],\n",
    "          \"pos\":    [\"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဆာဘီးယား\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မွန်တီနီဂရိုး\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"မက်ဆီ\", \"ဒို\", \"နာ\"],\n",
    "          \"pos\":    [\"n\", \"n\", \"v\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဆူ\", \"လာ\", \"ဝေ\", \"ဆီ\"],\n",
    "          \"pos\":    [\"part\", \"part\", \"n\", \"ppm\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"ဒိုက်ဦး\"],\n",
    "          \"pos\":    [\"n\"],\n",
    "          \"ner\":    [\"B-LOC\"],\n",
    "      },\n",
    "      {\n",
    "          \"tokens\": [\"စီ\", \"မီး\", \"အူး\", \"လူ\", \"ကျွန်း\"],\n",
    "          \"pos\":    [\"part\", \"n\", \"n\", \"n\", \"n\"],\n",
    "          \"ner\":    [\"B-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\", \"I-LOC\"],\n",
    "      }\n",
    "   \n",
    "]\n",
    "\n",
    "import os\n",
    "\n",
    "input_dir = os.path.abspath(r\"datasets/temp/\")\n",
    "print(input_dir)\n",
    "# if not os.path.exists(input_dir):\n",
    "#     raise FileNotFoundError(f\"Directory does not exist: {input_dir}\")\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".conll\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        correct_token_sequences_in_file(file_path, sequence_corrections4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "731c24aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_1.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_10.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_11.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_12.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_2.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_3.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_4.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_5.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_6.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_7.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_8.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postagged_no_tag_ner_9.conll\n",
      "Processing: c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\bio_without_pos\\bio_without_postest.conll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def convert_space_to_tab_conll(input_dir):\n",
    "    \"\"\"\n",
    "    Converts space-separated CoNLL files (2 or 3 columns) to tab-separated format.\n",
    "    Overwrites each file in-place.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to the directory containing .conll files.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".conll\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                stripped = line.strip()\n",
    "                if not stripped:\n",
    "                    new_lines.append(\"\")  # keep sentence boundary\n",
    "                else:\n",
    "                    parts = stripped.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        new_lines.append(\"\\t\".join(parts))\n",
    "                    else:\n",
    "                        # Keep malformed lines untouched\n",
    "                        new_lines.append(stripped)\n",
    "\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"\\n\".join(new_lines) + \"\\n\")\n",
    "\n",
    "input_dir = os.path.abspath(r\"datasets/annotate_with_ai/bio_tagging/bio_without_pos\")\n",
    "convert_space_to_tab_conll(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1436a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def correct_conll_sequences(file_path, output_path, patterns, debug=False):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    corrected_lines = lines.copy()\n",
    "    n = len(lines)\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        if lines[i].strip():\n",
    "            window = []\n",
    "            for j in range(i, min(i + 10, n)):  # lookahead max 10 tokens\n",
    "                if not lines[j].strip():\n",
    "                    break\n",
    "                parts = lines[j].strip().split(\"\\t\")\n",
    "                if len(parts) != 3:\n",
    "                    # If line format unexpected, break window\n",
    "                    break\n",
    "                token, pos, ner = parts\n",
    "                window.append((token, pos, ner))\n",
    "\n",
    "            if not window:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            tokens_only = [t[0] for t in window]\n",
    "            joined_tokens = \" \".join(tokens_only)\n",
    "\n",
    "            matched = False\n",
    "            for rule in patterns:\n",
    "                # Use match() so pattern can match prefix of a longer token sequence\n",
    "                m = rule[\"regex\"].match(joined_tokens)\n",
    "                if not m:\n",
    "                    continue\n",
    "\n",
    "                matched_text = m.group(0).strip()\n",
    "                if matched_text == \"\":\n",
    "                    continue\n",
    "\n",
    "                # Count how many tokens the matched prefix contains\n",
    "                matched_tokens = matched_text.split()\n",
    "                L = len(matched_tokens)\n",
    "\n",
    "                # Safety: ensure matched length doesn't exceed window length\n",
    "                if L > len(window):\n",
    "                    if debug:\n",
    "                        print(f\"Skipping match because matched length {L} > window {len(window)}\")\n",
    "                    continue\n",
    "\n",
    "                matched = True\n",
    "\n",
    "                # decide ner tags (dynamic if None)\n",
    "                if rule.get(\"ner_tags\") is None:\n",
    "                    ner_tags = [\"B-DATE\"] + [\"I-DATE\"] * (L - 1)\n",
    "                else:\n",
    "                    ner_tags = rule[\"ner_tags\"]\n",
    "\n",
    "                # Apply tags only for the matched token span\n",
    "                for k in range(L):\n",
    "                    token, pos, _ = window[k]\n",
    "                    tag = ner_tags[k] if k < len(ner_tags) else \"I-DATE\"\n",
    "                    corrected_lines[i + k] = f\"{token}\\t{pos}\\t{tag}\\n\"\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"Rule {rule['regex'].pattern} matched prefix '{matched_text}' -> tagging {L} tokens at index {i}\")\n",
    "\n",
    "                # advance by number of tokens matched (not entire window)\n",
    "                i += L\n",
    "                break  # stop scanning rules for this position\n",
    "\n",
    "            if not matched:\n",
    "                i += 1\n",
    "\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(corrected_lines)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"✅ Corrections applied and saved to {output_path}\")\n",
    "\n",
    "\n",
    "# Month names and pattern\n",
    "month_names = [\n",
    "    \"ဇန်နဝါရီ\", \"ဖေဖော်ဝါရီ\", \"မတ်\", \"ဧပြီ\", \"မေ\", \"ဇွန်\",\n",
    "    \"ဇူလိုင်\", \"ဩဂုတ်\", \"စက်တင်ဘာ\", \"အောက်တိုဘာ\", \"နိုဝင်ဘာ\", \"ဒီဇင်ဘာ\"\n",
    "]\n",
    "month_pattern = \"(\" + \"|\".join(month_names) + \")\"\n",
    "\n",
    "patterns = [\n",
    "    # 1) Year + \"ခု\" + \"နှစ်\" + Month + optional 'လ' + optional days + optional 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^[\\d၀-၉]{{4}}\\sခု\\sနှစ်\\s{month_pattern}(?:\\sလ)?(?:\\s[\\d၀-၉]+)*(?:\\s(?:ရက်|ရက်နေ့))?$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 2) Year + Month + day + 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^[\\d၀-၉]{{4}}\\s{month_pattern}(?:\\sလ)?\\s[\\d၀-၉]+(?:\\s(?:ရက်|ရက်နေ့))$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # # 3) Month + day + 'ရက်' or 'ရက်နေ့' with one token for day\n",
    "    # {\n",
    "    #     \"regex\": re.compile(\n",
    "    #         rf\"^{month_pattern}(?:\\sလ)?\\s[\\d၀-၉]+(?:\\s(?:ရက်|ရက်နေ့))$\"\n",
    "    #     ),\n",
    "    #     \"ner_tags\": None\n",
    "    # },\n",
    "    # 3) Month + (optional လ) + day number (multi-token) + 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^{month_pattern}(?:\\s+လ)?(?:\\s+[\\d၀-၉]+)+(?:\\s+(?:ရက်|ရက်နေ့))$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 4) Year + \"ခု\" + \"နှစ်\"\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\sခု\\sနှစ်$\"),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 5) Year + \"နှစ်\"\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\sနှစ်$\"),\n",
    "        \"ner_tags\": [\"B-DATE\", \"I-DATE\"]\n",
    "    },\n",
    "\n",
    "    # 6) Year range with hyphen\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\s-\\s[\\d၀-၉]{4}$\"),\n",
    "        \"ner_tags\": [\"B-DATE\", \"O\", \"B-DATE\"]\n",
    "    },\n",
    "\n",
    "    # 7) Year only\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}$\"),\n",
    "        \"ner_tags\": [\"B-DATE\"]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# patterns = [\n",
    "\n",
    "#        # 1) Year + \"ခု\" + \"နှစ်\"\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\sခု\\sနှစ်$\"),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # 2) Year + Month + day + 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\s{month_pattern}(?:\\sလ)?\\s[\\d၀-၉]+(?:\\s(?:ရက်|ရက်နေ့))$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # 3) Month + day + 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^{month_pattern}(?:\\sလ)?\\s[\\d၀-၉]+(?:\\s(?:ရက်|ရက်နေ့))$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # 4) Year only (strict: exactly 4 digits)\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4}$\"),\n",
    "#         \"ner_tags\": [\"B-DATE\"]\n",
    "#     },\n",
    "\n",
    "#     # =====================\n",
    "#     # Your existing patterns\n",
    "#     # =====================\n",
    "\n",
    "#     # Year + \"ခု\" + \"နှစ်\" + Month + optional 'လ' + optional days + optional 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\sခု\\sနှစ်\\s{month_pattern}(?:\\sလ)?(?:\\s[\\d၀-၉]+)*(?:\\s(?:ရက်|ရက်နေ့))?$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Year + \"ခု နှစ်\"\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4} ခု နှစ်$\"),\n",
    "#         \"ner_tags\": [\"B-DATE\", \"I-DATE\", \"I-DATE\"]\n",
    "#     },\n",
    "\n",
    "#     # Year + \"ခု\" + \"နှစ်\"\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\sခု\\sနှစ်$\"),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Year + \"နှစ်\"\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4} နှစ်$\"),\n",
    "#         \"ner_tags\": [\"B-DATE\", \"I-DATE\"]\n",
    "#     },\n",
    "\n",
    "#     # Year + Month + optional 'လ' + one or more day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\s{month_pattern}(?:\\sလ)?(?:\\s[\\d၀-၉]+)+(?:\\s(?:ရက်|ရက်နေ့))?$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Year + Month + optional 'လ' + one or two day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}} {month_pattern} လ (?:[\\d၀-၉]+ ?)+ (?:ရက်|ရက်နေ့)?$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Year + Month + optional 'လ' only\n",
    "#     {\n",
    "#         \"regex\": re.compile(rf\"^[\\d၀-၉]{{4}} {month_pattern}(?: လ)?$\"),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Month only + optional 'လ' + one or more day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^{month_pattern}(?:\\sလ)?(?:\\s[\\d၀-၉]+)+(?:\\s(?:ရက်|ရက်နေ့))?$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Month only + optional 'လ' + optional day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^{month_pattern}(?: လ)?(?: [\\d၀-၉]+)*(?: (?:ရက်|ရက်နေ့))?$\"\n",
    "#         ),\n",
    "#         \"ner_tags\": None\n",
    "#     },\n",
    "\n",
    "#     # Year range with hyphen\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4} - [\\d၀-၉]{4}$\"),\n",
    "#         \"ner_tags\": [\"B-DATE\", \"O\", \"B-DATE\"]\n",
    "#     },\n",
    "\n",
    "#     # Year only\n",
    "#     {\n",
    "#         \"regex\": re.compile(r\"^[\\d၀-၉]{4}$\"),\n",
    "#         \"ner_tags\": [\"B-DATE\"]\n",
    "#     },\n",
    "# ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd39c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_1.conll\n",
      "Processing tagged_no_tag_ner_1.conll -> tagged_no_tag_ner_1.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_1.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_10.conll\n",
      "Processing tagged_no_tag_ner_10.conll -> tagged_no_tag_ner_10.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_10.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_11.conll\n",
      "Processing tagged_no_tag_ner_11.conll -> tagged_no_tag_ner_11.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_11.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_12.conll\n",
      "Processing tagged_no_tag_ner_12.conll -> tagged_no_tag_ner_12.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_12.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_2.conll\n",
      "Processing tagged_no_tag_ner_2.conll -> tagged_no_tag_ner_2.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_2.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_3.conll\n",
      "Processing tagged_no_tag_ner_3.conll -> tagged_no_tag_ner_3.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_3.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_4.conll\n",
      "Processing tagged_no_tag_ner_4.conll -> tagged_no_tag_ner_4.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_4.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_5.conll\n",
      "Processing tagged_no_tag_ner_5.conll -> tagged_no_tag_ner_5.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_5.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_6.conll\n",
      "Processing tagged_no_tag_ner_6.conll -> tagged_no_tag_ner_6.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_6.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_7.conll\n",
      "Processing tagged_no_tag_ner_7.conll -> tagged_no_tag_ner_7.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_7.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_8.conll\n",
      "Processing tagged_no_tag_ner_8.conll -> tagged_no_tag_ner_8.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_8.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_9.conll\n",
      "Processing tagged_no_tag_ner_9.conll -> tagged_no_tag_ner_9.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\tagged_no_tag_ner_9.conll\n",
      "Processing and overwriting c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\test.conll\n",
      "Processing test.conll -> test.conll\n",
      "✅ Corrections applied and saved to c:\\Users\\nuwai\\Documents\\Sophia_Skill_Development\\Sophia_projects\\burmese_NER\\datasets\\annotate_with_ai\\bio_tagging\\corrected\\test.conll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# (Include your correct_conll_sequences and helper functions here)\n",
    "\n",
    "input_folder= os.path.abspath(r\"datasets/annotate_with_ai/bio_tagging/corrected\")\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".conll\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = input_path  # overwrite original file\n",
    "        print(f\"Processing and overwriting {input_path}\")\n",
    "        correct_conll_sequences(input_path, output_path, patterns)\n",
    "        print(f\"Processing {filename} -> {os.path.basename(output_path)}\")\n",
    "        correct_conll_sequences(input_path, output_path, patterns, debug=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "958afc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_name_tags_with_pos_filter(sent_lines):\n",
    "#     \"\"\"\n",
    "#     Replace NER tags to B-PER and I-PER for name sequences matching patterns:\n",
    "#     Prefix tokens: ကို, မ, ဦး, ဒေါ်\n",
    "#     Followed by one or more name tokens (based on POS)\n",
    "#     Ending before tokens like 'က' plus fixed phrases.\n",
    "\n",
    "#     Only continuous tokens with allowed POS tags are included in name.\n",
    "\n",
    "#     Args:\n",
    "#         sent_lines: list of lines (str), each line like \"token\\tpos\\ttag\"\n",
    "#     Returns:\n",
    "#         modified sent_lines with updated tags for matched name sequences\n",
    "#     \"\"\"\n",
    "#     prefix_tokens = {\"ကို\", \"မ\", \"ဦး\", \"ဒေါ်\"}\n",
    "#     allowed_name_pos = {\"n\", \"nr\", \"nz\", \"np\"}  # adjust based on your POS tags for names; 'n' = noun here\n",
    "#     tokens = [line.split(\"\\t\")[0] for line in sent_lines]\n",
    "#     poses = [line.split(\"\\t\")[1] for line in sent_lines]\n",
    "\n",
    "#     n = len(tokens)\n",
    "#     i = 0\n",
    "#     while i < n:\n",
    "#         if tokens[i] in prefix_tokens:\n",
    "#             # Find next 'က'\n",
    "#             try:\n",
    "#                 k_idx = tokens.index(\"က\", i + 1)\n",
    "#             except ValueError:\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "\n",
    "#             # The name tokens must be continuous tokens after prefix (i), up to k_idx (exclusive)\n",
    "#             # But only tokens with allowed POS should be included\n",
    "#             name_end = i\n",
    "#             for idx in range(i + 1, k_idx):\n",
    "#                 if poses[idx] in allowed_name_pos:\n",
    "#                     name_end = idx\n",
    "#                 else:\n",
    "#                     # Stop at first token not allowed\n",
    "#                     break\n",
    "\n",
    "#             if name_end == i:\n",
    "#                 # No name tokens found\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "\n",
    "#             # Check phrase after 'က' to match allowed phrase tails\n",
    "#             phrase_tails = [\n",
    "#                 [\"ပြော\", \"ပါ\", \"တယ်\"],\n",
    "#                 [\"ပြော\", \"ပြ\", \"ပေး\", \"မှာ\", \"ပါ\"],\n",
    "#                 [\"ပြော\", \"ပြ\", \"ပေး\", \"သွား\", \"မှာ\", \"ပါ\"],\n",
    "#                 [\"ဘီဘီစီ\", \"ကို\", \"ပြော\", \"ပါ\", \"တယ်\"]\n",
    "#             ]\n",
    "\n",
    "#             matched_phrase = False\n",
    "#             for phrase in phrase_tails:\n",
    "#                 phrase_len = len(phrase)\n",
    "#                 if k_idx + phrase_len < n + 1:\n",
    "#                     after_k_tokens = tokens[k_idx + 1: k_idx + 1 + phrase_len]\n",
    "#                     if after_k_tokens == phrase:\n",
    "#                         matched_phrase = True\n",
    "#                         break\n",
    "\n",
    "#             if not matched_phrase:\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "\n",
    "#             # Tag prefix token as B-PER\n",
    "#             sent_lines[i] = f\"{tokens[i]}\\t{poses[i]}\\tB-PER\"\n",
    "#             # Tag name tokens (from i+1 to name_end) as I-PER\n",
    "#             for idx in range(i + 1, name_end + 1):\n",
    "#                 sent_lines[idx] = f\"{tokens[idx]}\\t{poses[idx]}\\tI-PER\"\n",
    "\n",
    "#             # Do NOT tag tokens between name_end+1 and k_idx (they are excluded)\n",
    "#             # Advance i past the phrase\n",
    "#             i = k_idx + phrase_len + 1\n",
    "#         else:\n",
    "#             i += 1\n",
    "\n",
    "#     return sent_lines\n",
    "\n",
    "\n",
    "# input_folder= os.path.abspath(r\"datasets/annotate_with_ai/bio_tagging/corrected/test.conll\")\n",
    "# output_dir = os.path.abspath(r\"datasets/annotate_with_ai/bio_tagging/corrected/testoutput.conll\")\n",
    "# #Example usage:\n",
    "# #Load your file lines\n",
    "# with open(input_folder, \"r\", encoding=\"utf-8\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# new_lines = replace_name_tags_with_pos_filter(lines)\n",
    "\n",
    "\n",
    "# #Save fixed lines\n",
    "# with open(output_dir, \"w\", encoding=\"utf-8\") as f:\n",
    "#     for line in new_lines:\n",
    "#         f.write(line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8e34a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Number token sequences fixed safely in all files in 'corrected/' folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "input_folder= os.path.abspath(r\"datasets/annotate_with_ai/bio_tagging/corrected\")\n",
    "\n",
    "# Regex to identify tokens that can be part of number sequences\n",
    "number_token_pattern = re.compile(r\"^[\\d၀-၉]+$|^,$|^\\.$\")\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.endswith(\".conll\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(input_folder, filename)\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    corrected_lines = []\n",
    "    i = 0\n",
    "    n = len(lines)\n",
    "\n",
    "    while i < n:\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # If blank line, just keep and move on\n",
    "        if not line:\n",
    "            corrected_lines.append(\"\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        parts = line.split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            corrected_lines.append(line)\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        token, pos, tag = parts\n",
    "\n",
    "        # Only fix sequences where tokens look like number parts and tagged wrongly or partially wrong (excluding DATE tags)\n",
    "        if number_token_pattern.match(token) and tag not in (\"B-DATE\", \"I-DATE\"):\n",
    "\n",
    "            # Start collecting consecutive number tokens\n",
    "            seq_start = i\n",
    "            seq_tokens = []\n",
    "            seq_tags = []\n",
    "\n",
    "            while i < n:\n",
    "                cur_line = lines[i].strip()\n",
    "                if not cur_line:\n",
    "                    break\n",
    "                cur_parts = cur_line.split(\"\\t\")\n",
    "                if len(cur_parts) != 3:\n",
    "                    break\n",
    "\n",
    "                cur_token, cur_pos, cur_tag = cur_parts\n",
    "\n",
    "                if number_token_pattern.match(cur_token) and cur_tag not in (\"B-DATE\", \"I-DATE\"):\n",
    "                    seq_tokens.append(cur_token)\n",
    "                    seq_tags.append(cur_tag)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Re-tag this sequence: first token B-NUM, rest I-NUM\n",
    "            for idx, tok in enumerate(seq_tokens):\n",
    "                new_tag = \"B-NUM\" if idx == 0 else \"I-NUM\"\n",
    "                corrected_lines.append(f\"{tok}\\t{pos}\\t{new_tag}\")\n",
    "\n",
    "        else:\n",
    "            # No fix needed, just keep line as-is\n",
    "            corrected_lines.append(line)\n",
    "            i += 1\n",
    "\n",
    "    # Overwrite file with fixed tags\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(corrected_lines))\n",
    "\n",
    "print(\"✅ Number token sequences fixed safely in all files in 'corrected/' folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c23dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-empty lines have exactly 3 columns.\n",
      "Safe to run correction.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def check_conll_file_format(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                continue  # skip empty lines\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 3:\n",
    "                print(f\"Line {line_num} does not have exactly 3 columns: {line}\")\n",
    "                return False\n",
    "    print(\"All non-empty lines have exactly 3 columns.\")\n",
    "    return True\n",
    "\n",
    "# Usage example:\n",
    "#input_folder= os.path.abspath(r\"datasets/3entity_annotated_ner_cleaned.conll\")\n",
    "input_folder= os.path.abspath(r\"models/ner_train.conll\")\n",
    "if check_conll_file_format(input_folder):\n",
    "    print(\"Safe to run correction.\")\n",
    "else:\n",
    "    print(\"Fix file format before running correction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d1282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.abspath(r\"models/ner_val.conll\")\n",
    "\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):  # i is line number starting from 1\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip empty lines\n",
    "        parts = line.split()\n",
    "        if len(parts) < 2:\n",
    "            print(f\"Line {i}: too few columns -> {line}\")\n",
    "        elif len(parts) > 3:\n",
    "            print(f\"Line {i}: too many columns -> {line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c9e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def validate_and_fix_date_spans(file_path, output_path, patterns, debug=False):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    corrected_lines = lines.copy()\n",
    "    n = len(lines)\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        line = lines[i]\n",
    "        if not line.strip():\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        token, pos, ner = parts\n",
    "\n",
    "        if ner == \"B-DATE\":\n",
    "            # Start collecting full span\n",
    "            span_tokens = [token]\n",
    "            span_indices = [i]\n",
    "\n",
    "            j = i + 1\n",
    "            while j < n:\n",
    "                next_line = lines[j]\n",
    "                if not next_line.strip():\n",
    "                    break\n",
    "                next_parts = next_line.strip().split(\"\\t\")\n",
    "                if len(next_parts) != 3:\n",
    "                    break\n",
    "                _, _, next_ner = next_parts\n",
    "                if next_ner == \"I-DATE\":\n",
    "                    span_tokens.append(next_parts[0])\n",
    "                    span_indices.append(j)\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            span_text = \" \".join(span_tokens)\n",
    "\n",
    "            # Check against all patterns using fullmatch()\n",
    "            matched = False\n",
    "            for rule in patterns:\n",
    "                if rule[\"regex\"].fullmatch(span_text):\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if not matched:\n",
    "                # Fix all span tokens to O\n",
    "                if debug:\n",
    "                    print(f\"Fixing incorrect DATE span at lines {span_indices}: '{span_text}'\")\n",
    "                for idx in span_indices:\n",
    "                    t, p, _ = lines[idx].strip().split(\"\\t\")\n",
    "                    corrected_lines[idx] = f\"{t}\\t{p}\\tO\\n\"\n",
    "\n",
    "            i = span_indices[-1] + 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(corrected_lines)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"✅ Date validation fix applied and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f915c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tag_dates_strict(input_path, output_path, patterns, debug=False):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    corrected_lines = lines.copy()\n",
    "    n = len(lines)\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        line = lines[i]\n",
    "        if not line.strip():\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        token, pos, ner = parts\n",
    "\n",
    "        # Skip tokens already tagged correctly\n",
    "        if ner in [\"B-DATE\", \"I-DATE\"]:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Lookahead window max 10 tokens or until empty line\n",
    "        window = []\n",
    "        j = i\n",
    "        while j < n and len(window) < 10:\n",
    "            if not lines[j].strip():\n",
    "                break\n",
    "            parts_j = lines[j].strip().split(\"\\t\")\n",
    "            if len(parts_j) != 3:\n",
    "                break\n",
    "            # If already tagged as DATE, stop expanding to avoid re-tagging existing correct spans\n",
    "            if parts_j[2] in [\"B-DATE\", \"I-DATE\"]:\n",
    "                break\n",
    "            window.append(parts_j)\n",
    "            j += 1\n",
    "\n",
    "        tokens_only = [w[0] for w in window]\n",
    "        joined_tokens = \" \".join(tokens_only)\n",
    "\n",
    "        matched = False\n",
    "        for rule in patterns:\n",
    "            if rule[\"regex\"].fullmatch(joined_tokens):\n",
    "                matched = True\n",
    "                length = len(tokens_only)\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"Pattern matched at line {i}: '{joined_tokens}'\")\n",
    "\n",
    "                # Apply tags: B-DATE first token, then I-DATE for rest\n",
    "                for k in range(length):\n",
    "                    t, p, old_ner = window[k]\n",
    "                    # Only tag tokens not already tagged as DATE\n",
    "                    if old_ner not in [\"B-DATE\", \"I-DATE\"]:\n",
    "                        tag = \"B-DATE\" if k == 0 else \"I-DATE\"\n",
    "                        corrected_lines[i + k] = f\"{t}\\t{p}\\t{tag}\\n\"\n",
    "\n",
    "                i += length\n",
    "                break\n",
    "\n",
    "        if not matched:\n",
    "            i += 1\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(corrected_lines)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"✅ Date tagging completed and saved to {output_path}\")\n",
    "\n",
    "\n",
    "month_names = [\n",
    "    \"ဇန်နဝါရီ\", \"ဖေဖော်ဝါရီ\", \"မတ်\", \"ဧပြီ\", \"မေ\", \"ဇွန်\",\n",
    "    \"ဇူလိုင်\", \"ဂျူလိုင်\",\"ဩဂုတ်\", \"စက်တင်ဘာ\", \"အောက်တိုဘာ\", \"နိုဝင်ဘာ\", \"ဒီဇင်ဘာ\"\n",
    "]\n",
    "month_pattern = \"(\" + \"|\".join(month_names) + \")\"\n",
    "\n",
    "patterns = [\n",
    "    # 1) Year + \"ခု နှစ်\"\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\s+ခု\\s+နှစ်$\"),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 2) Year + \"ခု\" + \"နှစ်\" + Month + optional 'လ' + optional days + optional 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^[\\d၀-၉]{{4}}\\s+ခု\\s+နှစ်\\s+{month_pattern}(?:\\s+လ)?(?:\\s+[\\d၀-၉]+)*(?:\\s+(?:ရက်|ရက်နေ့))?$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 3) Year + Month + optional 'လ' + one or two day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^[\\d၀-၉]{{4}}\\s+{month_pattern}\\s+လ(?:\\s+[\\d၀-၉]+)+\\s+(?:ရက်|ရက်နေ့)?$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 4) Year + Month + optional 'လ' + one or more day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^[\\d၀-၉]{{4}}\\s+{month_pattern}(?:\\s+လ)?(?:\\s+[\\d၀-၉]+)+(?:\\s+(?:ရက်|ရက်နေ့))?$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 5) Year + Month + day + 'ရက်' or 'ရက်နေ့' (single day token)\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^[\\d၀-၉]{{4}}\\s+{month_pattern}(?:\\s+လ)?\\s+[\\d၀-၉]+(?:\\s+(?:ရက်|ရက်နေ့))$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 6) Year + \"နှစ်\"\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\s+နှစ်$\"),\n",
    "        \"ner_tags\": [\"B-DATE\", \"I-DATE\"]\n",
    "    },\n",
    "\n",
    "    # 7) Year range with hyphen (e.g., 2019 - 2021)\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}\\s*-\\s*[\\d၀-၉]{4}$\"),\n",
    "        \"ner_tags\": [\"B-DATE\", \"O\", \"B-DATE\"]\n",
    "    },\n",
    "\n",
    "    # 8) Year only (exactly 4 digits)\n",
    "    {\n",
    "        \"regex\": re.compile(r\"^[\\d၀-၉]{4}$\"),\n",
    "        \"ner_tags\": [\"B-DATE\"]\n",
    "    },\n",
    "\n",
    "    # 9) Month only + optional 'လ' + one or more day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^{month_pattern}(?:\\s+လ)?(?:\\s+[\\d၀-၉]+)+(?:\\s+(?:ရက်|ရက်နေ့))?$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 10) Month only + optional 'လ' + optional day tokens + optional 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^{month_pattern}(?:\\s+လ)?(?:\\s+[\\d၀-၉]+)*(?:\\s+(?:ရက်|ရက်နေ့))?$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 11) Month + day + 'ရက်' or 'ရက်နေ့' (single day token)\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^{month_pattern}(?:\\s+လ)?\\s+[\\d၀-၉]+(?:\\s+(?:ရက်|ရက်နေ့))$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "\n",
    "    # 12) Month + (optional လ) + day number (multi-token) + 'ရက်' or 'ရက်နေ့'\n",
    "    {\n",
    "        \"regex\": re.compile(\n",
    "            rf\"^{month_pattern}(?:\\s+လ)?(?:\\s+[\\d၀-၉]+)+(?:\\s+(?:ရက်|ရက်နေ့))$\"\n",
    "        ),\n",
    "        \"ner_tags\": None\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91238500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_overflow_date_tags(input_path, output_path, patterns, debug=False):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    corrected_lines = lines.copy()\n",
    "    n = len(lines)\n",
    "    i = 0\n",
    "\n",
    "    while i < n:\n",
    "        line = lines[i]\n",
    "        if not line.strip():\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) != 3:\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        token, pos, ner = parts\n",
    "\n",
    "        # Find B-DATE spans\n",
    "        if ner == \"B-DATE\":\n",
    "            span_tokens = [token]\n",
    "            span_indices = [i]\n",
    "\n",
    "            j = i + 1\n",
    "            while j < n:\n",
    "                next_line = lines[j]\n",
    "                if not next_line.strip():\n",
    "                    break\n",
    "                next_parts = next_line.strip().split(\"\\t\")\n",
    "                if len(next_parts) != 3:\n",
    "                    break\n",
    "                next_token, next_pos, next_ner = next_parts\n",
    "                if next_ner == \"I-DATE\":\n",
    "                    span_tokens.append(next_token)\n",
    "                    span_indices.append(j)\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            span_text = \" \".join(span_tokens)\n",
    "\n",
    "            # Check fullmatch against patterns\n",
    "            matched_rule = None\n",
    "            for rule in patterns:\n",
    "                if rule[\"regex\"].fullmatch(span_text):\n",
    "                    matched_rule = rule\n",
    "                    break\n",
    "\n",
    "            if matched_rule:\n",
    "                # Perfect match - keep as is\n",
    "                if debug:\n",
    "                    print(f\"Span matched fully: '{span_text}' at lines {span_indices}\")\n",
    "            else:\n",
    "                # No full match - try to find longest matching substring spans inside this span\n",
    "\n",
    "                # We'll check partial spans starting from first token and shrinking from right\n",
    "                # to find a longest prefix that matches a pattern, and similarly longest suffix.\n",
    "                # Then set unmatched tokens outside the longest matching substring as O\n",
    "\n",
    "                # 1) Find longest matching prefix span\n",
    "                longest_prefix_len = 0\n",
    "                for length in range(len(span_tokens), 0, -1):\n",
    "                    candidate_text = \" \".join(span_tokens[:length])\n",
    "                    if any(rule[\"regex\"].fullmatch(candidate_text) for rule in patterns):\n",
    "                        longest_prefix_len = length\n",
    "                        break\n",
    "\n",
    "                # 2) Find longest matching suffix span\n",
    "                longest_suffix_len = 0\n",
    "                for length in range(len(span_tokens), 0, -1):\n",
    "                    candidate_text = \" \".join(span_tokens[-length:])\n",
    "                    if any(rule[\"regex\"].fullmatch(candidate_text) for rule in patterns):\n",
    "                        longest_suffix_len = length\n",
    "                        break\n",
    "\n",
    "                # Mark tokens outside longest prefix and suffix as O\n",
    "                # Tokens inside prefix and suffix remain tagged\n",
    "                # If prefix + suffix overlap, keep overlapping middle tokens tagged\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"No full match for span: '{span_text}' at lines {span_indices}\")\n",
    "                    print(f\"Longest prefix match length: {longest_prefix_len}\")\n",
    "                    print(f\"Longest suffix match length: {longest_suffix_len}\")\n",
    "\n",
    "                prefix_end = longest_prefix_len - 1\n",
    "                suffix_start = len(span_tokens) - longest_suffix_len\n",
    "\n",
    "                for idx_pos, token_idx in enumerate(span_indices):\n",
    "                    if idx_pos <= prefix_end or idx_pos >= suffix_start:\n",
    "                        # keep B-DATE/I-DATE tags as is\n",
    "                        # But for the first token of the span, ensure B-DATE, others I-DATE\n",
    "                        if idx_pos == 0:\n",
    "                            tag = \"B-DATE\"\n",
    "                        else:\n",
    "                            tag = \"I-DATE\"\n",
    "                        t, p, old_ner = lines[token_idx].strip().split(\"\\t\")\n",
    "                        corrected_lines[token_idx] = f\"{t}\\t{p}\\t{tag}\\n\"\n",
    "                    else:\n",
    "                        # Outside matching prefix/suffix: convert to O\n",
    "                        t, p, old_ner = lines[token_idx].strip().split(\"\\t\")\n",
    "                        corrected_lines[token_idx] = f\"{t}\\t{p}\\tO\\n\"\n",
    "\n",
    "            i = span_indices[-1] + 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(corrected_lines)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"✅ Overflow date tags corrected and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43570f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# month_names = [\n",
    "#     \"ဇန်နဝါရီ\", \"ဖေဖော်ဝါရီ\", \"မတ်\", \"ဧပြီ\", \"မေ\", \"ဇွန်\",\n",
    "#     \"ဇူလိုင်\", \"ဩဂုတ်\", \"စက်တင်ဘာ\", \"အောက်တိုဘာ\", \"နိုဝင်ဘာ\", \"ဒီဇင်ဘာ\"\n",
    "# ]\n",
    "# month_pattern = \"(\" + \"|\".join(month_names) + \")\"\n",
    "\n",
    "# patterns_fix = [\n",
    "\n",
    "#     # 1) year + \"ခု\" + \"နှစ်\" + punctuation (optional) + month + optional \"လ\" + day tokens + optional \"ရက်\" or \"ရက်နေ့\"\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\sခု\\sနှစ်\\s[၊,]?\\s{month_pattern}(?:\\sလ)?(?:\\s[\\d၀-၉]+)+(?:\\s(?:ရက်|ရက်နေ့))?$\"\n",
    "#         ),\n",
    "#         \"description\": \"year + ခု + နှစ် + punctuation + month + လ? + day(s) + ရက်/ရက်နေ့\"\n",
    "#     },\n",
    "\n",
    "#     # 2) year + \"ခု\" + \"နှစ်\" + punctuation (optional) + month + optional \"လ\"\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\sခု\\sနှစ်\\s[၊,]?\\s{month_pattern}(?:\\sလ)?$\"\n",
    "#         ),\n",
    "#         \"description\": \"year + ခု + နှစ် + punctuation + month + optional လ\"\n",
    "#     },\n",
    "\n",
    "#     # 3) year + \"ခု\" + \"နှစ်\" + month (no punctuation, no လ)\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\sခု\\sနှစ်\\s{month_pattern}$\"\n",
    "#         ),\n",
    "#         \"description\": \"year + ခု + နှစ် + month (no punctuation, no လ)\"\n",
    "#     },\n",
    "\n",
    "#     # 4) year + month + optional လ (no ခု + နှစ်)\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             rf\"^[\\d၀-၉]{{4}}\\s{month_pattern}(?:\\sလ)?$\"\n",
    "#         ),\n",
    "#         \"description\": \"year + month + optional လ (no ခု + နှစ်)\"\n",
    "#     },\n",
    "\n",
    "#     # 5) year + \"ခု\" + \"နှစ်\" only (you said you want to keep these as is, so let's keep pattern here for safety)\n",
    "#     {\n",
    "#         \"regex\": re.compile(\n",
    "#             r\"^[\\d၀-၉]{4}\\sခု\\sနှစ်$\"\n",
    "#         ),\n",
    "#         \"description\": \"year + ခု + နှစ် only\"\n",
    "#     },\n",
    "\n",
    "# ]\n",
    "# def fix_partial_date_tagging(input_path, output_path, fix_patterns, debug=False):\n",
    "#     with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     corrected_lines = lines.copy()\n",
    "#     n = len(lines)\n",
    "#     i = 0\n",
    "\n",
    "#     while i < n:\n",
    "#         if not lines[i].strip():\n",
    "#             i += 1\n",
    "#             continue\n",
    "        \n",
    "#         parts = lines[i].strip().split(\"\\t\")\n",
    "#         if len(parts) != 3:\n",
    "#             i += 1\n",
    "#             continue\n",
    "\n",
    "#         token, pos, ner = parts\n",
    "\n",
    "#         if ner == \"B-DATE\":\n",
    "#             # Collect full DATE span (B-DATE + I-DATE)\n",
    "#             span_tokens = [token]\n",
    "#             span_indices = [i]\n",
    "#             j = i + 1\n",
    "#             while j < n:\n",
    "#                 next_line = lines[j]\n",
    "#                 if not next_line.strip():\n",
    "#                     break\n",
    "#                 next_parts = next_line.strip().split(\"\\t\")\n",
    "#                 if len(next_parts) != 3:\n",
    "#                     break\n",
    "#                 next_token, next_pos, next_ner = next_parts\n",
    "#                 if next_ner == \"I-DATE\":\n",
    "#                     span_tokens.append(next_token)\n",
    "#                     span_indices.append(j)\n",
    "#                     j += 1\n",
    "#                 else:\n",
    "#                     break\n",
    "            \n",
    "#             # Try to extend span forward to cover fix_patterns if possible\n",
    "#             # Lookahead window max 10 tokens after span end\n",
    "#             lookahead_limit = 10\n",
    "#             extend_tokens = []\n",
    "#             extend_indices = []\n",
    "\n",
    "#             for k in range(j, min(n, j + lookahead_limit)):\n",
    "#                 if not lines[k].strip():\n",
    "#                     break\n",
    "#                 parts_k = lines[k].strip().split(\"\\t\")\n",
    "#                 if len(parts_k) != 3:\n",
    "#                     break\n",
    "#                 t, p, tag = parts_k\n",
    "#                 # We want to extend only tokens currently tagged O (to avoid overwriting)\n",
    "#                 if tag == \"O\":\n",
    "#                     extend_tokens.append(t)\n",
    "#                     extend_indices.append(k)\n",
    "#                 else:\n",
    "#                     break  # stop extension on encountering any tag other than O\n",
    "            \n",
    "#             # Try to find the longest extension that forms a pattern when joined with current span_tokens\n",
    "#             # We'll attempt from longest possible extension to none\n",
    "\n",
    "#             found_extension = False\n",
    "#             for length in range(len(extend_tokens), -1, -1):\n",
    "#                 candidate_tokens = span_tokens + extend_tokens[:length]\n",
    "#                 candidate_text = \" \".join(candidate_tokens)\n",
    "#                 # Check all fix patterns for fullmatch\n",
    "#                 matched = False\n",
    "#                 for rule in fix_patterns:\n",
    "#                     if rule[\"regex\"].fullmatch(candidate_text):\n",
    "#                         matched = True\n",
    "#                         if debug:\n",
    "#                             print(f\"Extending span at line {i} with {length} tokens matched pattern: {rule['description']}\")\n",
    "#                         # Tag extension tokens as I-DATE, existing span tokens remain same (B-DATE + I-DATE)\n",
    "#                         for ext_idx in range(length):\n",
    "#                             idx_to_tag = extend_indices[ext_idx]\n",
    "#                             t, p, old_tag = lines[idx_to_tag].strip().split(\"\\t\")\n",
    "#                             corrected_lines[idx_to_tag] = f\"{t}\\t{p}\\tI-DATE\\n\"\n",
    "#                         found_extension = True\n",
    "#                         break\n",
    "#                 if matched:\n",
    "#                     break\n",
    "            \n",
    "#             i = span_indices[-1] + 1 + (length if found_extension else 0)\n",
    "#         else:\n",
    "#             i += 1\n",
    "\n",
    "#     with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.writelines(corrected_lines)\n",
    "\n",
    "#     if debug:\n",
    "#         print(f\"✅ Partial date tagging fixed and saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99de6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: tagged_no_tag_ner_1.conll\n",
      "Processed: tagged_no_tag_ner_10.conll\n",
      "Processed: tagged_no_tag_ner_11.conll\n",
      "Processed: tagged_no_tag_ner_12.conll\n",
      "Processed: tagged_no_tag_ner_2.conll\n",
      "Processed: tagged_no_tag_ner_3.conll\n",
      "Processed: tagged_no_tag_ner_4.conll\n",
      "Processed: tagged_no_tag_ner_5.conll\n",
      "Processed: tagged_no_tag_ner_6.conll\n",
      "Processed: tagged_no_tag_ner_7.conll\n",
      "Processed: tagged_no_tag_ner_8.conll\n",
      "Processed: tagged_no_tag_ner_9.conll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def fix_specific_date_formats(file_path):\n",
    "    \"\"\"\n",
    "    Fix specific date formats in a .conll file without touching already correct ones.\n",
    "    Patterns:\n",
    "    1) month + date (1 or more tokens) + ရက်\n",
    "    2) date(4 digits) - date(4 digits)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "    tokens = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            token, pos, tag = line.split(\"\\t\")\n",
    "            tokens.append([token, pos, tag])\n",
    "        else:\n",
    "            tokens.append([\"\", \"\", \"\"])  # sentence break\n",
    "\n",
    "    # Burmese month names\n",
    "    burmese_months = r\"(ဇန်နဝါရီ|ဖေဖော်ဝါရီ|မတ်|ဧပြီ|မေ|ဇွန်|ဇူလိုင်|ဩဂုတ်|စက်တင်ဘာ|အောက်တိုဘာ|နိုဝင်ဘာ|ဒီဇင်ဘာ)\"\n",
    "    \n",
    "    # Pattern 1: month + number(s) + ရက်\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if re.fullmatch(burmese_months, tokens[i][0]):\n",
    "            j = i + 1\n",
    "            # allow 1+ numeric tokens\n",
    "            while j < len(tokens) and re.fullmatch(r\"[0-9၀-၉]+\", tokens[j][0]):\n",
    "                j += 1\n",
    "            if j < len(tokens) and re.fullmatch(r\"(ရက်|ရက်နေ့)\", tokens[j][0]):\n",
    "                span_indices = list(range(i, j + 1))\n",
    "                # Tag only if not already correct\n",
    "                if not (tokens[i][2] == \"B-DATE\" and all(tokens[k][2] in (\"B-DATE\", \"I-DATE\") for k in span_indices[1:])):\n",
    "                    for k, idx in enumerate(span_indices):\n",
    "                        tokens[idx][2] = \"B-DATE\" if k == 0 else \"I-DATE\"\n",
    "                i = j\n",
    "        i += 1\n",
    "\n",
    "    # Pattern 2: YYYY - YYYY\n",
    "    year_pattern = r\"[0-9၀-၉]{4}\"\n",
    "    i = 0\n",
    "    while i < len(tokens) - 2:\n",
    "        if re.fullmatch(year_pattern, tokens[i][0]) and tokens[i + 1][0] == \"-\" and re.fullmatch(year_pattern, tokens[i + 2][0]):\n",
    "            span_indices = [i, i + 1, i + 2]\n",
    "            if not (tokens[i][2] == \"B-DATE\" and tokens[i + 2][2] == \"I-DATE\"):\n",
    "                tokens[i][2] = \"B-DATE\"\n",
    "                tokens[i + 1][2] = \"I-DATE\"\n",
    "                tokens[i + 2][2] = \"I-DATE\"\n",
    "        i += 1\n",
    "\n",
    "    # Save back\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for token, pos, tag in tokens:\n",
    "            if token:\n",
    "                f.write(f\"{token}\\t{pos}\\t{tag}\\n\")\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "# === Run for all files ===\n",
    "input_dir = \"datasets/annotate_with_ai/bio_tagging/corrected\"\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".conll\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        fix_specific_date_formats(file_path)\n",
    "        print(f\"Processed: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b64f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed: tagged_no_tag_ner_1.conll\n",
      "Fixed: tagged_no_tag_ner_10.conll\n",
      "Fixed: tagged_no_tag_ner_11.conll\n",
      "Fixed: tagged_no_tag_ner_12.conll\n",
      "Fixed: tagged_no_tag_ner_2.conll\n",
      "Fixed: tagged_no_tag_ner_3.conll\n",
      "Fixed: tagged_no_tag_ner_4.conll\n",
      "Fixed: tagged_no_tag_ner_5.conll\n",
      "Fixed: tagged_no_tag_ner_6.conll\n",
      "Fixed: tagged_no_tag_ner_7.conll\n",
      "Fixed: tagged_no_tag_ner_8.conll\n",
      "Fixed: tagged_no_tag_ner_9.conll\n",
      "Fixed: test.conll\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# input_dir = \"datasets/annotate_with_ai/bio_tagging/corrected\"\n",
    "# output_dir = \"datasets/annotate_with_ai/bio_tagging/corrected1\"\n",
    "\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# for filename in os.listdir(input_dir):\n",
    "#     if filename.endswith(\".conll\"):\n",
    "#         input_path = os.path.join(input_dir, filename)\n",
    "#         output_path = os.path.join(output_dir, filename)\n",
    "#         fix_partial_date_tagging(input_path, output_path, patterns, debug=False)\n",
    "#         print(f\"Fixed: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74971941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed TIME tagging: tagged_no_tag_ner_1.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_10.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_11.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_12.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_2.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_3.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_4.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_5.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_6.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_7.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_8.conll\n",
      "Processed TIME tagging: tagged_no_tag_ner_9.conll\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def build_time_patterns():\n",
    "    \"\"\"\n",
    "    Returns a list of regex patterns (strings) for TIME spans.\n",
    "    These work over token-joined-with-space sentences.\n",
    "    \"\"\"\n",
    "    # period = r\"(?:ည|မနက်|ညနေ|နံနက်|နေ့လယ်)\"\n",
    "    # num_tok = r\"[0-9၀-၉]+\"\n",
    "    # units = r\"(?:နာရီ|မိနစ်|စက္ကန့်)\"\n",
    "\n",
    "    period = r\"(?:ည|မနက်|ညနေ|နံနက်|နေ့လယ်|မွန်းလွဲ)\"\n",
    "    num_tok = r\"(?:[0-9၀-၉]|တစ်|နှစ်|သုံး|လေး|ငါး|ခြောက်|ခုနှစ်|ရှစ်|ကိုး|တစ်ဆယ်|ဆယ်|ဆယ့်တစ်|ဆယ့်နှစ်)\"  # include Burmese number words\n",
    "    units = r\"(?:နာရီ|မိနစ်|စက္ကန့်)\"\n",
    "\n",
    "    time_patterns = [\n",
    "    # Period + number + unit (+ repeated number+unit)\n",
    "    rf\"{period}\\s+{num_tok}\\s+{units}(?:\\s+{num_tok}\\s+{units})*\",\n",
    "    \n",
    "    # Number + unit (+ repeated)\n",
    "    rf\"{num_tok}\\s+{units}(?:\\s+{num_tok}\\s+{units})*\",\n",
    "    \n",
    "    # Unit-first + number (+ repeated)\n",
    "    rf\"{units}\\s+{num_tok}(?:\\s+{units}\\s+{num_tok})*\",\n",
    "    \n",
    "    # Period + unit (rare, no number)\n",
    "    rf\"{period}\\s+{units}\",\n",
    "    \n",
    "    # Do NOT include “unit alone” as a separate pattern anymore\n",
    "    ]\n",
    "    return time_patterns\n",
    "\n",
    "\n",
    "def tag_time_spans_in_conll(file_path, compiled_patterns, debug=False):\n",
    "    \"\"\"\n",
    "    Fix/complete TIME tagging using provided regex patterns.\n",
    "    - Only modifies spans that match a TIME pattern.\n",
    "    - If already correctly tagged (B-TIME then I-TIME...), leaves them unchanged.\n",
    "    - If partially/wrongly tagged, corrects only the matched span.\n",
    "    - Does NOT touch tokens outside matched spans.\n",
    "\n",
    "    Overwrites the file in place.\n",
    "    \"\"\"\n",
    "    # Read & split into sentences\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_lines = [ln.rstrip(\"\\n\") for ln in f]\n",
    "\n",
    "    sentences = []  # list of list[[tok,pos,tag], ...]\n",
    "    cur = []\n",
    "    for ln in raw_lines:\n",
    "        if ln.strip():\n",
    "            parts = ln.split(\"\\t\")\n",
    "            if len(parts) == 3:\n",
    "                cur.append(parts)\n",
    "            else:\n",
    "                # keep malformed lines as-is in current sentence\n",
    "                cur.append([ln, \"\", \"\"])\n",
    "        else:\n",
    "            if cur:\n",
    "                sentences.append(cur)\n",
    "                cur = []\n",
    "    if cur:\n",
    "        sentences.append(cur)\n",
    "\n",
    "    total_fixes = 0\n",
    "\n",
    "    for sent_idx, sent in enumerate(sentences):\n",
    "        # Build sentence string and fast token mapping\n",
    "        tokens = [t[0] for t in sent]\n",
    "        if not tokens:\n",
    "            continue\n",
    "        sent_str = \" \".join(tokens)\n",
    "\n",
    "        # Collect all matches across all patterns\n",
    "        matches = []  # (start_token_idx, end_token_idx, priority, match_text)\n",
    "        for prio, pat in enumerate(compiled_patterns):\n",
    "            for m in pat.finditer(sent_str):\n",
    "                matched_text = m.group(0).strip()\n",
    "                if not matched_text:\n",
    "                    continue\n",
    "\n",
    "                # Map char position to token indices via word counting\n",
    "                start_token = len(sent_str[:m.start()].split())\n",
    "                span_len = len(matched_text.split())\n",
    "                end_token = start_token + span_len - 1\n",
    "\n",
    "                # Safety bounds\n",
    "                if start_token < 0 or end_token >= len(tokens):\n",
    "                    continue\n",
    "\n",
    "                matches.append((start_token, end_token, prio, matched_text))\n",
    "\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        # Resolve overlaps: prefer longer spans first, then earlier priority, then earlier start\n",
    "        matches.sort(key=lambda x: (-(x[1]-x[0]+1), x[2], x[0]))\n",
    "        used = [False] * len(tokens)\n",
    "        selected = []\n",
    "        for s, e, prio, text in matches:\n",
    "            if any(used[i] for i in range(s, e+1)):\n",
    "                continue\n",
    "            for i in range(s, e+1):\n",
    "                used[i] = True\n",
    "            selected.append((s, e, prio, text))\n",
    "\n",
    "        # Apply tagging corrections span-by-span\n",
    "        for s, e, _, text in selected:\n",
    "            # Desired tags\n",
    "            desired = [\"B-TIME\"] + [\"I-TIME\"] * (e - s)\n",
    "\n",
    "            # Check if already correct\n",
    "            current = [sent[i][2] for i in range(s, e+1)]\n",
    "            if current == desired:\n",
    "                continue  # perfect; no change\n",
    "\n",
    "            # Fix only this span\n",
    "            for k, idx in enumerate(range(s, e+1)):\n",
    "                sent[idx][2] = desired[k]\n",
    "            total_fixes += 1\n",
    "            if debug:\n",
    "                print(f\"[TIME FIX] Sent#{sent_idx} tokens[{s}:{e}] -> '{text}'\")\n",
    "\n",
    "    # Write back\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sent in sentences:\n",
    "            for tok, pos, tag in sent:\n",
    "                f.write(f\"{tok}\\t{pos}\\t{tag}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"✅ Done. Spans fixed: {total_fixes} in {file_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Example: run over a folder\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"datasets/annotate_with_ai/bio_tagging/corrected\"\n",
    "    pattern_strs = build_time_patterns()\n",
    "    # Compile each regex\n",
    "    compiled_patterns = [re.compile(p) for p in pattern_strs]\n",
    "\n",
    "    # Then run\n",
    "    for fname in os.listdir(input_dir):\n",
    "        if fname.endswith(\".conll\"):\n",
    "            path = os.path.join(input_dir, fname)\n",
    "            tag_time_spans_in_conll(path, compiled_patterns, debug=False)\n",
    "            print(f\"Processed TIME tagging: {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6c0d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_ner_tags(file_path, output_path, tags_to_replace=None):\n",
    "    \"\"\"\n",
    "    Read a .conll file and replace specified NER tags with 'O'.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): path to input .conll file\n",
    "        output_path (str): path to save modified .conll file\n",
    "        tags_to_replace (list or set): list of NER tags to replace with 'O', e.g., ['B-ORG','I-ORG','B-PER','I-PER']\n",
    "    \"\"\"\n",
    "    if tags_to_replace is None:\n",
    "        tags_to_replace = []\n",
    "\n",
    "    tags_to_replace = set(tags_to_replace)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    modified_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_strip = line.strip()\n",
    "        if line_strip == \"\":\n",
    "            # Keep empty lines (sentence separators)\n",
    "            modified_lines.append(\"\\n\")\n",
    "            continue\n",
    "\n",
    "        parts = line_strip.split(\"\\t\")\n",
    "        if len(parts) < 3:\n",
    "            # Keep malformed lines as-is\n",
    "            modified_lines.append(line)\n",
    "            continue\n",
    "\n",
    "        token, pos, ner = parts[0], parts[1], parts[2]\n",
    "        if ner in tags_to_replace:\n",
    "            ner = \"O\"  # Replace specified tags with O\n",
    "\n",
    "        modified_lines.append(f\"{token}\\t{pos}\\t{ner}\\n\")\n",
    "\n",
    "    # Write to output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(modified_lines)\n",
    "\n",
    "    print(f\"Processed file saved to {output_path}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ebc343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved to datasets/annotate_with_ai/bio_tagging/corrected/3entity_annotated_ner.conll\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"datasets/annotate_with_ai/bio_tagging/corrected/full_annotated_ner.conll\"\n",
    "output_dir = \"datasets/annotate_with_ai/bio_tagging/corrected/3entity_annotated_ner.conll\"\n",
    "tags_to_replace = ['B-ORG','I-ORG','B-PER','I-PER','B-NUM','I-NUM']\n",
    "\n",
    "replace_ner_tags(input_dir, output_dir, tags_to_replace=tags_to_replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eb123f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label classes: {'I-LOC', 'I-TIME', 'B-LOC', 'B-DATE', 'B-TIME', 'O', 'I-DATE'}\n",
      "\n",
      "Label distribution:\n",
      "O: 2483849\n",
      "B-LOC: 54918\n",
      "I-LOC: 40131\n",
      "I-DATE: 21456\n",
      "B-DATE: 13293\n",
      "I-TIME: 3681\n",
      "B-TIME: 2898\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- Load your .conll file ----------\n",
    "def load_conll(file_path):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 3:\n",
    "                    token, pos, ner = parts[0], parts[1], parts[2]\n",
    "                else:\n",
    "                    token = parts[0]\n",
    "                    pos = parts[1] if len(parts) > 1 else 'X'\n",
    "                    ner = parts[2] if len(parts) > 2 else 'O'\n",
    "                sentence.append((token, pos, ner))\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# ---------- Explore dataset ----------\n",
    "input_dir = \"datasets/3entity_annotated_ner_cleaned.conll\"\n",
    "sentences = load_conll(input_dir)\n",
    "\n",
    "# Flatten all NER labels\n",
    "all_labels = [ner for sentence in sentences for (_, _, ner) in sentence]\n",
    "\n",
    "# Unique label classes\n",
    "label_classes = set(all_labels)\n",
    "print(\"Label classes:\", label_classes)\n",
    "\n",
    "# Distribution\n",
    "label_counts = Counter(all_labels)\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label, count in label_counts.most_common():\n",
    "    print(f\"{label}: {count}\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Mapping BIO tags to broader entity names ----------\n",
    "merge_map = {\n",
    "    \"B-LOC\": \"LOCATION\",\n",
    "    \"I-LOC\": \"LOCATION\",\n",
    "    \"B-DATE\": \"DATE\",\n",
    "    \"I-DATE\": \"DATE\",\n",
    "    \"B-TIME\": \"TIME\",\n",
    "    \"I-TIME\": \"TIME\"\n",
    "}\n",
    "\n",
    "# Apply the mapping, skip 'O'\n",
    "merged_labels = [\n",
    "    merge_map.get(ner, ner)  # map if in merge_map else keep original\n",
    "    for ner in all_labels\n",
    "    if ner != \"O\"\n",
    "]\n",
    "\n",
    "# Count merged labels\n",
    "label_counts = Counter(merged_labels)\n",
    "\n",
    "# ---------- Plot ----------\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(label_counts.keys(), label_counts.values())\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"NER Label Distribution (Merged)\")\n",
    "plt.xlabel(\"Entity Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bfbe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 112540: ပ\tpart\tB_PER\n",
      "Line 125874: နာ\tv\t-PER\n",
      "Line 141797: အီ\tn\tB_PER\n",
      "Line 610299: ခဲ့\tpart\tOj\n",
      "Line 1382069: မြို့\tn\tရပ်ကွက်\n",
      "Line 1444409: ဂွ\tn\ti-LOC\n",
      "Line 1765845: နာရီ\tn\tအတွင်း\n",
      "Line 1861915: ၁\tnum\tII-TIME\n",
      "Line 2027548: လမ်း\tn\tလ်\n",
      "Line 2380892: စက်တင်ဘာ\tn\tB-\n",
      "Line 2402369: ကျွန်းဆွယ်\tn\tကျွန်းဆွယ်\n",
      "Line 2485795: ဂျီ\tn\to\n",
      "Line 2624557: ခန့်\tpart\tTIME\n"
     ]
    }
   ],
   "source": [
    "# Define the set of valid/expected NER tags\n",
    "expected_tags = {\n",
    "    \"O\", \n",
    "    \"B-LOC\", \"I-LOC\",\n",
    "    \"B-DATE\", \"I-DATE\",\n",
    "    \"B-TIME\", \"I-TIME\",\n",
    "}\n",
    "\n",
    "# Open the file and check for invalid tags\n",
    "file_path = \"datasets/3entity_annotated_ner_cleaned_1.conll\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    for lineno, line in enumerate(f, 1):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue  # skip blank lines\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 3:\n",
    "            token, pos, ner = parts[0], parts[1], parts[2]\n",
    "            if ner not in expected_tags:\n",
    "                print(f\"Line {lineno}: {line}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65c8a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned file saved to datasets/3entity_annotated_ner_cleaned_1.conll\n"
     ]
    }
   ],
   "source": [
    "def clean_ner_tags(file_path, output_path, replacement_map):\n",
    "    \"\"\"\n",
    "    Replace messy NER tags in the 3rd column according to replacement_map.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): input .conll file\n",
    "        output_path (str): output cleaned .conll file\n",
    "        replacement_map (dict): {old_tag: new_tag}\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        line_strip = line.strip()\n",
    "        if line_strip == \"\":\n",
    "            cleaned_lines.append(\"\\n\")\n",
    "            continue\n",
    "        parts = line_strip.split(\"\\t\")\n",
    "        if len(parts) < 3:\n",
    "            cleaned_lines.append(line)\n",
    "            continue\n",
    "        token, pos, ner = parts[0], parts[1], parts[2]\n",
    "        if ner in replacement_map:\n",
    "            ner = replacement_map[ner]\n",
    "        cleaned_lines.append(f\"{token}\\t{pos}\\t{ner}\\n\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "    \n",
    "    print(f\"Cleaned file saved to {output_path}\")\n",
    "\n",
    "\n",
    "# ---------- Define your replacements ----------\n",
    "replacement_map = {\n",
    "    \"IO\": \"O\",\n",
    "    \"B_LOC\": \"B-LOC\",\n",
    "    \"I_LOC\": \"I-LOC\",\n",
    "    \"OE\": \"O\",\n",
    "    \"-ORG\": \"O\",\n",
    "    \"I-Land\": \"I-LOC\",\n",
    "    \"BO\": \"O\",\n",
    "    \"I_PER\": \"O\",\n",
    "    \"I-LOCI-LOC\": \"I-LOC\",\n",
    "    \"OI-PER\": \"O\",\n",
    "    \"I-oRG\": \"O\",\n",
    "    \"I-PEr\": \"O\",\n",
    "    \"I_ORG\" : \"O\",\n",
    "    \"B-LIC\" : \"B-LOC\",\n",
    "    \"i-DATE\" : \"I-DATE\"\n",
    "}\n",
    "\n",
    "# ---------- Usage ----------\n",
    "input_file = \"datasets/3entity_annotated_ner_cleaned.conll\"\n",
    "output_file = \"datasets/3entity_annotated_ner_cleaned_1.conll\"\n",
    "\n",
    "clean_ner_tags(input_file, output_file, replacement_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
