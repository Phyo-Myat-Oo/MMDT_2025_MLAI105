{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:42:16.101068Z",
     "iopub.status.busy": "2025-08-18T13:42:16.100770Z",
     "iopub.status.idle": "2025-08-18T13:42:19.380517Z",
     "shell.execute_reply": "2025-08-18T13:42:19.379555Z",
     "shell.execute_reply.started": "2025-08-18T13:42:16.101045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-crf\n",
      "  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: pytorch-crf\n",
      "Successfully installed pytorch-crf-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-18T13:42:22.192895Z",
     "iopub.status.busy": "2025-08-18T13:42:22.192611Z",
     "iopub.status.idle": "2025-08-18T13:42:22.304497Z",
     "shell.execute_reply": "2025-08-18T13:42:22.303663Z",
     "shell.execute_reply.started": "2025-08-18T13:42:22.192872Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\tf-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from seqeval.metrics import classification_report\n",
    "#from torchcrf import CRF \n",
    "from TorchCRF import CRF\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:44:02.787951Z",
     "iopub.status.busy": "2025-08-18T13:44:02.787436Z",
     "iopub.status.idle": "2025-08-18T13:44:05.185838Z",
     "shell.execute_reply": "2025-08-18T13:44:05.185044Z",
     "shell.execute_reply.started": "2025-08-18T13:44:02.787917Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45894 11474 14343\n"
     ]
    }
   ],
   "source": [
    "def parse_conll(path):\n",
    "    tokens, labels = [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        s_tok, s_lab = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if s_tok:\n",
    "                    tokens.append(s_tok); labels.append(s_lab)\n",
    "                    s_tok, s_lab = [], []\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) >= 3:\n",
    "                tok, _, ner = parts[0], parts[1], parts[2]\n",
    "                s_tok.append(tok); s_lab.append(ner)\n",
    "        if s_tok:\n",
    "            tokens.append(s_tok); labels.append(s_lab)\n",
    "    return tokens, labels\n",
    "\n",
    "dataset_path = \"/kaggle/input/myner-mmdt/\"\n",
    "train_tokens, train_labels = parse_conll(dataset_path + \"ner_train.conll\")\n",
    "val_tokens,   val_labels = parse_conll(dataset_path + \"ner_val.conll\")\n",
    "test_tokens,  test_labels  = parse_conll(dataset_path + \"ner_test.conll\")\n",
    "\n",
    "\n",
    "print(len(train_tokens), len(val_tokens), len(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:44:13.308024Z",
     "iopub.status.busy": "2025-08-18T13:44:13.307466Z",
     "iopub.status.idle": "2025-08-18T13:44:13.365815Z",
     "shell.execute_reply": "2025-08-18T13:44:13.365093Z",
     "shell.execute_reply.started": "2025-08-18T13:44:13.308001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'B-DATE': 0, 'B-LOC': 1, 'B-TIME': 2, 'I-DATE': 3, 'I-LOC': 4, 'I-TIME': 5, 'O': 6}\n"
     ]
    }
   ],
   "source": [
    "# === 2) Build label vocab ===\n",
    "all_labels = train_labels + val_labels  # build from train+val (test unseen)\n",
    "uniq = sorted({t for seq in all_labels for t in seq})\n",
    "if \"O\" not in uniq: uniq = [\"O\"] + [l for l in uniq if l != \"O\"]\n",
    "\n",
    "label2id = {l:i for i,l in enumerate(uniq)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "print(\"Labels:\", label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-DATE', 1: 'B-LOC', 2: 'B-TIME', 3: 'I-DATE', 4: 'I-LOC', 5: 'I-TIME', 6: 'O'}\n"
     ]
    }
   ],
   "source": [
    "label2id = {'B-DATE': 0, 'B-LOC': 1, 'B-TIME': 2, 'I-DATE': 3, 'I-LOC': 4, 'I-TIME': 5, 'O': 6}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize pre-segmented text (already split into word-level tokens) and align the corresponding NER labels to the resulting subword tokens.\n",
    "        \n",
    "Input:\n",
    "\n",
    "        tokens_list: List of sentences (each sentence = list of word-level tokens, e.g., from Burmese word segmenter)\n",
    "\n",
    "        labels_list: List of sentences (each sentence = list of BIO/NER labels aligned to tokens)\n",
    "\n",
    "Output:\n",
    "\n",
    "        Dictionary containing tensors, each of shape [num_sentences, MAX_LEN]:\n",
    "\n",
    "        \"input_ids\" → token IDs (subword tokenized, padded/truncated)\n",
    "\n",
    "        \"attention_mask\" → 1 for real tokens, 0 for padding\n",
    "\n",
    "        \"labels\" → aligned label IDs, with -100 for ignored tokens (CLS, SEP, padding, etc.)\n",
    "    This function is used in sequence labeling tasks such as Named Entity Recognition (NER),\n",
    "    where each word has a label (BIO/BIOES). Since BERT-based tokenizers (like DistilBERT)\n",
    "    may split a word into multiple subword tokens, we must ensure that labels are properly\n",
    "    aligned with the subword sequence.\n",
    "\n",
    "    Steps performed:\n",
    "    ----------------\n",
    "    1. Use a Hugging Face tokenizer to encode each list of word tokens into\n",
    "       input_ids, attention_mask, etc. (`is_split_into_words=True` ensures\n",
    "       the tokenizer treats each element of tokens_list as a pre-tokenized word).\n",
    "    \n",
    "    2. For each word:\n",
    "        - The first subword inherits the word’s original label.\n",
    "        - Any additional subwords are assigned the corresponding I- label\n",
    "          (if the word’s label was B-XXX) or the same label if it was already I-/O.\n",
    "    \n",
    "    3. Special tokens ([CLS], [SEP], padding) are assigned a label of -100,\n",
    "       which tells the model to ignore them during loss calculation.\n",
    "    \n",
    "    4. Return a dictionary with:\n",
    "        - \"input_ids\": tensor of token IDs padded/truncated to `MAX_LEN`\n",
    "        - \"attention_mask\": tensor indicating valid tokens vs. padding\n",
    "        - \"labels\": tensor of aligned label IDs with -100 for ignored tokens\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokens_list : List[List[str]]\n",
    "        List of sentences, where each sentence is a list of word-level tokens\n",
    "        (already segmented, e.g., with a Myanmar word segmenter).\n",
    "    \n",
    "    labels_list : List[List[str]]\n",
    "        List of sentences, where each sentence is a list of labels (BIO tags)\n",
    "        aligned to the `tokens_list`.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    encodings : Dict[str, torch.Tensor]\n",
    "        Dictionary containing batched tensors for:\n",
    "        - \"input_ids\"      : shape [num_sentences, max_len]\n",
    "        - \"attention_mask\" : shape [num_sentences, max_len]\n",
    "        - \"labels\"         : shape [num_sentences, max_len]\n",
    "        Ready to be wrapped into a PyTorch Dataset for training.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    tokens_list = [[\"ရန်ကုန်\", \"မြို့\"], [\"မြန်မာနိုင်ငံ\", \"စာ\"]]\n",
    "    labels_list = [[\"B-LOC\", \"I-LOC\"], [\"B-LOC\", \"O\"]]\n",
    "\n",
    "    encoded = tokenize_and_align(tokens_list, labels_list)\n",
    "\n",
    "    encoded[\"input_ids\"].shape      # torch.Size([2, MAX_LEN])\n",
    "    encoded[\"attention_mask\"].shape # torch.Size([2, MAX_LEN])\n",
    "    encoded[\"labels\"].shape         # torch.Size([2, MAX_LEN])\n",
    "    \n",
    "        {\n",
    "        \"input_ids\": tensor([[   101, 34567, 45678, 78910, 56789,   102,     0,     0, ...]]),\n",
    "        \"attention_mask\": tensor([[1, 1, 1, 1, 1, 1, 0, 0, ...]]),\n",
    "        \"labels\": tensor([[-100,  1,  2,  2,  0, -100, -100, -100, ...]])\n",
    "        }\n",
    "tokens_list + labels_list\n",
    "        │\n",
    "        ▼\n",
    "tokenize_and_align()  → dict of tensors\n",
    "        │\n",
    "        ▼\n",
    "NERDataset(enc)       → single-sentence dicts\n",
    "        │\n",
    "        ▼\n",
    "DataLoader(dataset)   → batched dicts → feed into model\n",
    "\n",
    "┌─────────────────────────────┐\n",
    "│ Raw Sentences (word-segmented)\n",
    "│ [\"ရန်ကုန်\", \"မြို့\"]             │\n",
    "│ [\"မြန်မာနိုင်ငံ\", \"စာ\"]           │\n",
    "└───────────────┬─────────────┘\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────┐\n",
    "│ tokenize_and_align()        │\n",
    "│ - Tokenizer converts words to subwords │\n",
    "│ - Aligns NER labels to subwords      │\n",
    "│ - Pads/truncates to MAX_LEN          │\n",
    "│ - Special tokens ([CLS],[SEP],PAD) → -100 │\n",
    "└───────────────┬─────────────┘\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────┐\n",
    "│ NERDataset(enc)             │\n",
    "│ - Wraps each sentence into  │\n",
    "│   {\"input_ids\", \"attention_mask\", \"labels\"} │\n",
    "│ - __getitem__ returns 1 sentence │\n",
    "└───────────────┬─────────────┘\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────┐\n",
    "│ DataLoader(dataset, batch_size, shuffle) │\n",
    "│ - Stacks sentences into batch_size       │\n",
    "│ - Output shape: [batch_size, MAX_LEN]    │\n",
    "│ Example batch:                           │\n",
    "│ input_ids:      [[101,9011,8402,102,0...]] │\n",
    "│ attention_mask: [[1,1,1,1,0,...]]         │\n",
    "│ labels:         [[-100,3,4,-100,...]]     │\n",
    "└───────────────┬─────────────┘\n",
    "                │\n",
    "                ▼\n",
    "┌─────────────────────────────┐\n",
    "│ Feed batch into model       │\n",
    "│ - input_ids → embeddings    │\n",
    "│ - attention_mask → mask padding │\n",
    "│ - labels → compute loss (-100 ignored) │\n",
    "└─────────────────────────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:44:24.539659Z",
     "iopub.status.busy": "2025-08-18T13:44:24.539395Z",
     "iopub.status.idle": "2025-08-18T13:44:44.545303Z",
     "shell.execute_reply": "2025-08-18T13:44:44.544437Z",
     "shell.execute_reply.started": "2025-08-18T13:44:24.539639Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dc973e594c42f1a487eacf47da1e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f7ed910dc143c1b3152380b53f9b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52224ae08d7741b0ac1c774cb18530bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644aa10eea55433e93d73683a6ade5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 3) Tokenizer (FAST) ===\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_and_align(tokens_list, labels_list):\n",
    "    enc = tokenizer(\n",
    "        tokens_list,\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    all_label_ids = []\n",
    "    for i, labels in enumerate(labels_list):\n",
    "        word_ids = enc.word_ids(batch_index=i) # map subword → word index\n",
    "        prev = None\n",
    "        label_ids = []\n",
    "        for w in word_ids:\n",
    "            if w is None:\n",
    "                label_ids.append(-100)  # # ignore [CLS], [SEP], [PAD] during loss\n",
    "            elif w != prev:\n",
    "                # first subword of the word → keep original label\n",
    "                label_ids.append(label2id[labels[w]])      # first piece -> true label\n",
    "            else:\n",
    "                # subword -> give I- tag if B-*, # continuation subwords → switch B- to I-\n",
    "                curr = labels[w]\n",
    "                if curr.startswith(\"B-\"):\n",
    "                    curr = \"I-\" + curr[2:]\n",
    "                label_ids.append(label2id.get(curr, label2id[\"O\"]))\n",
    "            prev = w\n",
    "        all_label_ids.append(label_ids)\n",
    "    enc[\"labels\"] = all_label_ids\n",
    "    return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "train_enc = tokenize_and_align(train_tokens, train_labels)\n",
    "val_enc   = tokenize_and_align(val_tokens,   val_labels)\n",
    "test_enc  = tokenize_and_align(test_tokens,  test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NERDataset = wrapper around tokenized encodings → gives one sentence at a time.\n",
    "DataLoader = stacks sentences into batches automatically.\n",
    "\n",
    "DataLoader\n",
    "batch = {\n",
    "  \"input_ids\": tensor([\n",
    "    [ 101, 5001, 6205, 7123, 102,    0,    0,    0],\n",
    "    [ 101, 9011, 8402, 102,   0,    0,    0,    0]\n",
    "  ]),\n",
    "  \"attention_mask\": tensor([\n",
    "    [1, 1, 1, 1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 0, 0, 0, 0]\n",
    "  ]),\n",
    "  \"labels\": tensor([\n",
    "    [-100, 1, 2, 2, -100, -100, -100, -100],\n",
    "    [-100, 3, 4, -100, -100, -100, -100, -100]\n",
    "  ])\n",
    "  ....\n",
    "}\n",
    "\n",
    "\n",
    "Raw Sentences (word-segmented)\n",
    "───────────────────────────────\n",
    "[\"ရန်ကုန်\", \"မြို့\"]\n",
    "[\"မြန်မာနိုင်ငံ\", \"စာ\"]\n",
    "        │\n",
    "        ▼\n",
    "tokenize_and_align()\n",
    "───────────────────────────────\n",
    "- Tokenizer converts each word into subwords if needed\n",
    "- Aligns NER labels to subwords\n",
    "- Special tokens ([CLS], [SEP], PAD) get label = -100\n",
    "- Pads/truncates sentences to MAX_LEN\n",
    "Output:\n",
    "{\n",
    "  \"input_ids\":      tensor([num_sentences, MAX_LEN]),\n",
    "  \"attention_mask\": tensor([num_sentences, MAX_LEN]),\n",
    "  \"labels\":         tensor([num_sentences, MAX_LEN])\n",
    "}\n",
    "        │\n",
    "        ▼\n",
    "NERDataset(enc)\n",
    "───────────────────────────────\n",
    "- Wraps each sentence into a dictionary:\n",
    "{\n",
    "  \"input_ids\": tensor([MAX_LEN]),\n",
    "  \"attention_mask\": tensor([MAX_LEN]),\n",
    "  \"labels\": tensor([MAX_LEN])\n",
    "}\n",
    "- Allows indexing via __getitem__ (one sentence at a time)\n",
    "        │\n",
    "        ▼\n",
    "DataLoader(dataset, batch_size, shuffle)\n",
    "───────────────────────────────\n",
    "- Automatically stacks multiple sentences into a batch\n",
    "- Each batch has shape: [batch_size, MAX_LEN]\n",
    "Batch example (batch_size=2, MAX_LEN=8):\n",
    "{\n",
    "  \"input_ids\": tensor([\n",
    "      [101, 9011, 8402, 102, 0, 0, 0, 0],\n",
    "      [101, 5001, 6205, 7123, 102, 0, 0, 0]\n",
    "  ]),\n",
    "  \"attention_mask\": tensor([\n",
    "      [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "      [1, 1, 1, 1, 1, 0, 0, 0]\n",
    "  ]),\n",
    "  \"labels\": tensor([\n",
    "      [-100, 3, 4, -100, -100, -100, -100, -100],\n",
    "      [-100, 1, 2, 2, -100, -100, -100, -100]\n",
    "  ])\n",
    "}\n",
    "        │\n",
    "        ▼\n",
    "Feed batch into model (DistilBERT / mBERT)\n",
    "───────────────────────────────\n",
    "- input_ids → embeddings\n",
    "- attention_mask → tells model which tokens to attend\n",
    "- labels → used for loss calculation (ignore -100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:44:52.504010Z",
     "iopub.status.busy": "2025-08-18T13:44:52.503298Z",
     "iopub.status.idle": "2025-08-18T13:44:52.512586Z",
     "shell.execute_reply": "2025-08-18T13:44:52.511748Z",
     "shell.execute_reply.started": "2025-08-18T13:44:52.503975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 4) PyTorch Datasets ===\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, enc):\n",
    "        self.enc = enc\n",
    "    def __len__(self): return self.enc[\"input_ids\"].shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.enc[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.enc[\"attention_mask\"][idx],\n",
    "            \"labels\": self.enc[\"labels\"][idx]\n",
    "        }\n",
    "\n",
    "train_ds = NERDataset(train_enc)\n",
    "val_ds   = NERDataset(val_enc)\n",
    "test_ds  = NERDataset(test_enc)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:45:03.143763Z",
     "iopub.status.busy": "2025-08-18T13:45:03.143153Z",
     "iopub.status.idle": "2025-08-18T13:45:03.150882Z",
     "shell.execute_reply": "2025-08-18T13:45:03.150138Z",
     "shell.execute_reply.started": "2025-08-18T13:45:03.143737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "from torchcrf import CRF\n",
    "import time\n",
    "\n",
    "# -----------------------\n",
    "# 1️⃣ Define DistilBERT + CRF model\n",
    "# -----------------------\n",
    "class DistilBertCRF(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size # Number of hidden units from DistilBERT. Typically 768 for base models.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels)  # emission scores\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)  # (B, L, H)\n",
    "        emissions = self.classifier(sequence_output)              # (B, L, num_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            # Create mask for valid tokens\n",
    "            mask = labels.ne(-100)\n",
    "            mask[:, 0] = True  # Ensure first timestep is always unmasked\n",
    "\n",
    "            # Replace masked labels with a safe value 0\n",
    "            safe_labels = labels.clone()\n",
    "            safe_labels[~mask] = 0\n",
    "            safe_labels[:, 0] = safe_labels[:, 0].clamp(0, self.crf.num_tags-1)\n",
    "\n",
    "            log_likelihood = self.crf(emissions, safe_labels, mask=mask, reduction='mean') # CRF computes sequence-level log-likelihood over the batch.\n",
    "            return -log_likelihood # loss to minimize for training\n",
    "        else:\n",
    "            mask = attention_mask.bool() # Only attend to real tokens (ignore padding)\n",
    "            mask[:, 0] = True  # first timestep must be on\n",
    "            return self.crf.decode(emissions, mask=mask)  # Best label sequences, decoding to find most likely label sequence per sentence\n",
    "            # Output shape: [batch_size, seq_len] (list of predicted labels per token).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scheduler: Linear WarmupPurpose: control learning rate over training steps.\n",
    "\n",
    "Transformers usually train better if the learning rate starts small and gradually increases → warmup.\n",
    "\n",
    "Parameters:\n",
    "    num_training_steps\n",
    "    Total number of optimization steps = epochs * batches_per_epoch.\n",
    "    num_warmup_steps\n",
    "    Initial steps where learning rate linearly increases from 0 → lr\n",
    "    Here, 10% of total steps (0.1 * num_training_steps)\n",
    "\n",
    "How it works:\n",
    "    Warmup phase: LR grows linearly → prevents sudden large updates at the start.\n",
    "    Decay phase: After warmup, LR decreases linearly → stabilizes training and improves convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T13:45:22.884319Z",
     "iopub.status.busy": "2025-08-18T13:45:22.883773Z",
     "iopub.status.idle": "2025-08-18T14:45:33.373944Z",
     "shell.execute_reply": "2025-08-18T14:45:33.373122Z",
     "shell.execute_reply.started": "2025-08-18T13:45:22.884297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 13:45:36.250332: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755524736.590415      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755524736.688254      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4407014fb6b849aaacefc1578bed28f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/279614802.py:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
      "/tmp/ipykernel_36/279614802.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 10.4489 | Time: 11.97 min\n",
      "Epoch 2/5 | Loss: 3.5365 | Time: 11.93 min\n",
      "Epoch 3/5 | Loss: 2.3943 | Time: 11.91 min\n",
      "Epoch 4/5 | Loss: 1.6262 | Time: 11.89 min\n",
      "Epoch 5/5 | Loss: 1.0462 | Time: 11.91 min\n",
      "Training completed in 11.91 minutes\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 2️⃣ Prepare DataLoaders\n",
    "# -----------------------\n",
    "full_train_dataset = ConcatDataset([train_ds, val_ds])\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------\n",
    "# 3️⃣ Initialize model, optimizer, scheduler\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "num_labels = len(label2id)\n",
    "\n",
    "model = DistilBertCRF(model_name, num_labels).to(device)\n",
    "epochs = 5\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01) # penalizes large weights to reduce overfitting.\n",
    "num_training_steps = epochs * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "# -----------------------\n",
    "# 4️⃣ Training Loop (GPU-safe)\n",
    "# -----------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == \"cuda\")):\n",
    "            loss = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Time: {elapsed/60:.2f} min\")\n",
    "\n",
    "print(f\"Training completed in {(time.time() - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# -----------------------\n",
    "# 5️⃣ Save model\n",
    "# -----------------------\n",
    "torch.save(model.state_dict(), \"distilbert_crf_ner.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:47:04.014857Z",
     "iopub.status.busy": "2025-08-18T14:47:04.013908Z",
     "iopub.status.idle": "2025-08-18T14:49:31.087053Z",
     "shell.execute_reply": "2025-08-18T14:49:31.086417Z",
     "shell.execute_reply.started": "2025-08-18T14:47:04.014832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DATE       0.92      0.94      0.93      2614\n",
      "         LOC       0.84      0.86      0.85     10638\n",
      "        TIME       0.87      0.90      0.88       557\n",
      "\n",
      "   micro avg       0.86      0.88      0.87     13809\n",
      "   macro avg       0.87      0.90      0.89     13809\n",
      "weighted avg       0.86      0.88      0.87     13809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Make sure model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "true_labels_all = []\n",
    "pred_labels_all = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Get predictions from CRF\n",
    "        predictions = model(input_ids, attention_mask=attention_mask, labels=None)  # list of lists\n",
    "\n",
    "        # Convert label ids to actual labels\n",
    "        for i in range(len(labels)):\n",
    "            true_seq = []\n",
    "            pred_seq = []\n",
    "\n",
    "            for j in range(labels[i].size(0)):\n",
    "                if labels[i, j] == -100:\n",
    "                    continue  # skip padding\n",
    "                true_seq.append(id2label[labels[i, j].item()])\n",
    "                pred_seq.append(id2label[predictions[i][j]])\n",
    "\n",
    "            true_labels_all.append(true_seq)\n",
    "            pred_labels_all.append(pred_seq)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels_all, pred_labels_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T15:16:25.580713Z",
     "iopub.status.busy": "2025-08-18T15:16:25.580168Z",
     "iopub.status.idle": "2025-08-18T15:16:38.485990Z",
     "shell.execute_reply": "2025-08-18T15:16:38.485361Z",
     "shell.execute_reply.started": "2025-08-18T15:16:25.580690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.9431    0.9602    0.9516      2538\n",
      "       B-LOC     0.8913    0.9089    0.9000     10280\n",
      "      B-TIME     0.9293    0.9546    0.9418       551\n",
      "      I-DATE     0.9562    0.9664    0.9613     16296\n",
      "       I-LOC     0.8902    0.9098    0.8999     59371\n",
      "      I-TIME     0.9528    0.9804    0.9664      2758\n",
      "           O     0.9942    0.9927    0.9934   1162839\n",
      "\n",
      "    accuracy                         0.9876   1254633\n",
      "   macro avg     0.9367    0.9533    0.9449   1254633\n",
      "weighted avg     0.9877    0.9876    0.9877   1254633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# # Flatten sequences (BIO format preserved)\n",
    "# y_true = [label for seq in all_true for label in seq]\n",
    "# y_pred = [label for seq in all_pred for label in seq]\n",
    "\n",
    "y_true = [label for seq in true_labels_all for label in seq]\n",
    "y_pred = [label for seq in pred_labels_all for label in seq]\n",
    "\n",
    "# Print classification report with BIO tags\n",
    "print(classification_report(y_true, y_pred, digits=4, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
