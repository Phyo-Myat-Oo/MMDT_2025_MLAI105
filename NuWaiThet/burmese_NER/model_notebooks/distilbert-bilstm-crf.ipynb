{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install transformers torchcrf seqeval -q\n!pip install pytorch-crf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-18T17:31:14.508283Z","iopub.execute_input":"2025-08-18T17:31:14.508546Z","iopub.status.idle":"2025-08-18T17:31:20.312672Z","shell.execute_reply.started":"2025-08-18T17:31:14.508520Z","shell.execute_reply":"2025-08-18T17:31:20.311890Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\nDownloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\nfrom torchcrf import CRF\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nfrom transformers import DistilBertTokenizerFast, DistilBertModel, get_linear_schedule_with_warmup\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T17:36:42.992280Z","iopub.execute_input":"2025-08-18T17:36:42.992577Z","iopub.status.idle":"2025-08-18T17:36:42.998362Z","shell.execute_reply.started":"2025-08-18T17:36:42.992557Z","shell.execute_reply":"2025-08-18T17:36:42.997780Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# -----------------------\n# 1️⃣ Dataset & Tokenization\n# -----------------------\ndef parse_conll(path):\n    tokens, labels = [], []\n    with open(path, encoding=\"utf-8\") as f:\n        s_tok, s_lab = [], []\n        for line in f:\n            line = line.strip()\n            if not line:\n                if s_tok:\n                    tokens.append(s_tok)\n                    labels.append(s_lab)\n                    s_tok, s_lab = [], []\n                continue\n            parts = line.split(\"\\t\")\n            if len(parts) >= 3:\n                tok, _, ner = parts[0], parts[1], parts[2]\n                s_tok.append(tok)\n                s_lab.append(ner)\n        if s_tok:\n            tokens.append(s_tok)\n            labels.append(s_lab)\n    return tokens, labels\n\ndataset_path = \"/kaggle/input/myner-mmdt/\"\ntrain_tokens, train_labels = parse_conll(dataset_path + \"ner_train.conll\")\nval_tokens, val_labels = parse_conll(dataset_path + \"ner_val.conll\")\ntest_tokens, test_labels = parse_conll(dataset_path + \"ner_test.conll\")\n\n# Combine train + val for full training\ntrain_tokens += val_tokens\ntrain_labels += val_labels\n\n# Build label mapping\nuniq_labels = sorted({t for seq in train_labels for t in seq})\nif \"O\" not in uniq_labels: uniq_labels = [\"O\"] + [l for l in uniq_labels if l != \"O\"]\nlabel2id = {l: i for i, l in enumerate(uniq_labels)}\nid2label = {i: l for l, i in label2id.items()}\n\nnum_labels = len(label2id)\nprint(\"Labels:\", label2id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T17:31:41.007094Z","iopub.execute_input":"2025-08-18T17:31:41.007726Z","iopub.status.idle":"2025-08-18T17:31:43.589118Z","shell.execute_reply.started":"2025-08-18T17:31:41.007700Z","shell.execute_reply":"2025-08-18T17:31:43.588454Z"}},"outputs":[{"name":"stdout","text":"Labels: {'B-DATE': 0, 'B-LOC': 1, 'B-TIME': 2, 'I-DATE': 3, 'I-LOC': 4, 'I-TIME': 5, 'O': 6}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Tokenizer\nmodel_name = \"distilbert-base-multilingual-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\nMAX_LEN = 128\n\ndef tokenize_and_align(tokens_list, labels_list):\n    enc = tokenizer(\n        tokens_list,\n        is_split_into_words=True,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=MAX_LEN,\n        return_attention_mask=True\n    )\n    all_label_ids = []\n    for i, labels in enumerate(labels_list):\n        word_ids = enc.word_ids(batch_index=i)\n        prev = None\n        label_ids = []\n        for w in word_ids:\n            if w is None:\n                label_ids.append(-100)\n            elif w != prev:\n                label_ids.append(label2id[labels[w]])\n            else:\n                curr = labels[w]\n                if curr.startswith(\"B-\"):\n                    curr = \"I-\" + curr[2:]\n                label_ids.append(label2id.get(curr, label2id[\"O\"]))\n            prev = w\n        all_label_ids.append(label_ids)\n    enc[\"labels\"] = all_label_ids\n    return {k: torch.tensor(v) for k, v in enc.items()}\n\ntrain_enc = tokenize_and_align(train_tokens, train_labels)\ntest_enc  = tokenize_and_align(test_tokens, test_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T17:33:38.342244Z","iopub.execute_input":"2025-08-18T17:33:38.343284Z","iopub.status.idle":"2025-08-18T17:33:59.677429Z","shell.execute_reply.started":"2025-08-18T17:33:38.343217Z","shell.execute_reply":"2025-08-18T17:33:59.676670Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# -----------------------\n# 2️⃣ PyTorch Dataset\n# -----------------------\n# Suppose your unique labels are\n#unique_labels = sorted(set(l for seq in all_labels for l in seq))\n# label_ids = [label2id[label] for label in labels]\n# label2id = {label: idx for idx, label in enumerate(unique_labels)}\n# id2label = {idx: label for label, idx in label2id.items()}\n\nclass NERDataset(Dataset):\n    def __init__(self, tokens, labels, tokenizer, label2id, max_len=128):\n        self.tokens = tokens\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.label2id = label2id\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        words = self.tokens[idx]\n        labels = self.labels[idx]\n\n        # Convert labels to IDs\n        label_ids = [self.label2id[label] for label in labels]\n\n        encoding = self.tokenizer(\n            words,\n            is_split_into_words=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_tensors='pt'\n        )\n\n        word_ids = encoding.word_ids(batch_index=0)\n        aligned_labels = []\n\n        for i, word_idx in enumerate(word_ids):\n            if word_idx is None or word_idx >= len(label_ids):\n                aligned_labels.append(-100)  # ignore\n            else:\n                aligned_labels.append(label_ids[word_idx])\n\n        encoding['labels'] = torch.tensor(aligned_labels, dtype=torch.long)\n        return {k: v.squeeze(0) for k, v in encoding.items()}\n\n\n# train_ds = NERDataset(train_enc)\n# test_ds  = NERDataset(test_enc)\n\n# train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n# test_loader  = DataLoader(test_ds, batch_size=16, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:40:56.207395Z","iopub.execute_input":"2025-08-18T18:40:56.208339Z","iopub.status.idle":"2025-08-18T18:40:56.216744Z","shell.execute_reply.started":"2025-08-18T18:40:56.208298Z","shell.execute_reply":"2025-08-18T18:40:56.216130Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class DistilBertBiLSTMCRF(nn.Module):\n    def __init__(self, model_name, num_labels, hidden_dim=128):\n        super().__init__()\n        self.distilbert = DistilBertModel.from_pretrained(model_name)\n        self.lstm = nn.LSTM(\n            input_size=self.distilbert.config.hidden_size,\n            hidden_size=hidden_dim // 2,\n            num_layers=1,\n            bidirectional=True,\n            batch_first=True\n        )\n        self.classifier = nn.Linear(hidden_dim, num_labels)\n        self.crf = CRF(num_labels, batch_first=True)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state  # (batch, seq_len, hidden)\n\n        lstm_out, _ = self.lstm(sequence_output)\n        emissions = self.classifier(lstm_out)\n\n        # Ensure labels are >=0 for CRF\n        if labels is not None:\n            labels = torch.where(labels < 0, torch.tensor(0, device=labels.device), labels)\n            mask = attention_mask.bool() if attention_mask is not None else torch.ones_like(labels).bool()\n            loss = -self.crf(emissions, labels, mask=mask, reduction='mean')\n            return loss\n        else:\n            mask = attention_mask.bool() if attention_mask is not None else torch.ones(emissions.size()[:2], dtype=torch.bool, device=emissions.device)\n            return self.crf.decode(emissions, mask=mask)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T17:40:14.750724Z","iopub.execute_input":"2025-08-18T17:40:14.751402Z","iopub.status.idle":"2025-08-18T17:40:14.758064Z","shell.execute_reply.started":"2025-08-18T17:40:14.751378Z","shell.execute_reply":"2025-08-18T17:40:14.757308Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-multilingual-cased\")\n\n# tokens and labels are lists of lists from your .conll files\ntrain_dataset = NERDataset(train_tokens, train_labels, tokenizer, label2id, max_len=128)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n\nnum_labels = len(label2id)\nmodel = DistilBertBiLSTMCRF(\"distilbert-base-multilingual-cased\", num_labels).to(device)\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nepochs = 5\ngrad_accum_steps = 2\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\nnum_training_steps = epochs * len(train_loader) // grad_accum_steps\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=int(0.1*num_training_steps),\n    num_training_steps=num_training_steps\n)\n\n# -----------------------\n# 4️⃣ Training Loop\n# -----------------------\nbest_train_loss = float(\"inf\")\nstart_training = time.time()\n\nfor epoch in range(epochs):\n    model.train()\n    epoch_start = time.time()\n    total_loss = 0\n\n    for step, batch in enumerate(tqdm(train_loader)):\n        batch = {k:v.to(device) for k,v in batch.items()}\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            loss = model(**batch) / grad_accum_steps\n\n        scaler.scale(loss).backward()\n\n        if (step+1) % grad_accum_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        total_loss += loss.item() * grad_accum_steps\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Time: {time.time()-epoch_start:.2f} sec\")\n\n    if avg_loss < best_train_loss:\n        best_train_loss = avg_loss\n        torch.save(model.state_dict(), \"best_model.pt\")\n\nprint(f\"Total training time: {(time.time()-start_training)/60:.2f} min\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T17:41:55.856558Z","iopub.execute_input":"2025-08-18T17:41:55.856879Z","iopub.status.idle":"2025-08-18T18:38:37.272818Z","shell.execute_reply.started":"2025-08-18T17:41:55.856858Z","shell.execute_reply":"2025-08-18T18:38:37.271868Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3282867026.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n  0%|          | 0/3586 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/tmp/ipykernel_36/3282867026.py:38: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n100%|██████████| 3586/3586 [11:19<00:00,  5.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 | Loss: 19.5153 | Time: 679.09 sec\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3586 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|██████████| 3586/3586 [11:20<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 | Loss: 5.4560 | Time: 680.66 sec\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3586 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|██████████| 3586/3586 [11:20<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 | Loss: 3.9854 | Time: 680.17 sec\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3586 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|██████████| 3586/3586 [11:16<00:00,  5.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 | Loss: 3.1557 | Time: 676.35 sec\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3586 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|██████████| 3586/3586 [11:16<00:00,  5.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 | Loss: 2.5993 | Time: 676.93 sec\nTotal training time: 56.68 min\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# tokenizer = AutoTokenizer.from_pretrained(model_name)\n# print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n\n# print(\"Model embedding vocab size:\", model.bert.embeddings.word_embeddings.num_embeddings)\ntorch.save({\n    \"epoch\": epoch,\n    \"model_state_dict\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n    \"scheduler_state_dict\": scheduler.state_dict(),\n    \"best_train_loss\": best_train_loss,\n    \"label2id\": label2id\n}, \"best_model_full.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:54:26.759453Z","iopub.execute_input":"2025-08-18T18:54:26.760243Z","iopub.status.idle":"2025-08-18T18:54:29.736374Z","shell.execute_reply.started":"2025-08-18T18:54:26.760217Z","shell.execute_reply":"2025-08-18T18:54:29.735782Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from IPython.display import FileLink\n\n# Make sure current directory is /kaggle/working\n%cd /kaggle/working\n\n# Provide the relative path\nFileLink('best_model_full.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:56:00.369595Z","iopub.execute_input":"2025-08-18T18:56:00.370063Z","iopub.status.idle":"2025-08-18T18:56:00.376087Z","shell.execute_reply.started":"2025-08-18T18:56:00.370041Z","shell.execute_reply":"2025-08-18T18:56:00.375493Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/best_model_full.pt","text/html":"<a href='best_model_full.pt' target='_blank'>best_model_full.pt</a><br>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"# -----------------------\n# 6️⃣ Evaluation\n# -----------------------\ntest_dataset = NERDataset(test_tokens, test_labels, tokenizer, label2id, max_len=128)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nfrom seqeval.metrics import classification_report\n\nmodel.eval()\nall_true, all_pred = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        # Move tensors to device\n        batch = {k: v.to(device) for k, v in batch.items()}\n\n        # REMOVE labels so forward() returns decoded sequences\n        batch_eval = {k: v for k, v in batch.items() if k != \"labels\"}\n        preds = model(**batch_eval)  # now returns list of sequences\n\n        labels = batch[\"labels\"]\n        mask = labels != -100  # ignore padding tokens\n\n        # Ensure preds is a list of sequences\n        if isinstance(preds, torch.Tensor):\n            preds = preds.tolist()\n        elif isinstance(preds, list) and all(isinstance(p, int) for p in preds):\n            preds = [preds]  # single sequence edge case\n\n        batch_size = labels.size(0)\n        for i in range(batch_size):\n            pred_seq_ids = preds[i] if i < len(preds) else []\n\n            true_seq = [id2label[l.item()] for l, m in zip(labels[i], mask[i]) if m]\n            pred_seq = [id2label[p] for p, m in zip(pred_seq_ids, mask[i]) if m]\n\n            all_true.append(true_seq)\n            all_pred.append(pred_seq)\n\nprint(\"NER Classification Report:\")\nprint(classification_report(all_true, all_pred, digits=4, zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:48:18.119254Z","iopub.execute_input":"2025-08-18T18:48:18.120029Z","iopub.status.idle":"2025-08-18T18:50:33.586272Z","shell.execute_reply.started":"2025-08-18T18:48:18.120002Z","shell.execute_reply":"2025-08-18T18:50:33.585477Z"}},"outputs":[{"name":"stdout","text":"NER Classification Report:\n              precision    recall  f1-score   support\n\n        DATE     0.9402    0.9390    0.9396     10473\n         LOC     0.8874    0.9030    0.8951     46219\n        TIME     0.8658    0.8976    0.8814      1445\n\n   micro avg     0.8962    0.9094    0.9027     58137\n   macro avg     0.8978    0.9132    0.9054     58137\nweighted avg     0.8963    0.9094    0.9028     58137\n\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Flattened token-level evaluation\ntrue_labels_flat, pred_labels_flat = [], []\n\nmodel.eval()\nwith torch.no_grad():\n    for batch in test_loader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        batch_eval = {k: v for k, v in batch.items() if k != \"labels\"}\n        preds = model(**batch_eval)  # list of sequences\n\n        labels = batch[\"labels\"]\n        mask = labels != -100  # ignore padding tokens\n\n        # Ensure preds is list of sequences\n        if isinstance(preds, torch.Tensor):\n            preds = preds.tolist()\n        elif isinstance(preds, list) and all(isinstance(p, int) for p in preds):\n            preds = [preds]\n\n        batch_size = labels.size(0)\n        for i in range(batch_size):\n            pred_seq_ids = preds[i] if i < len(preds) else []\n\n            for true_id, pred_id, m in zip(labels[i], pred_seq_ids, mask[i]):\n                if m:  # only consider non-padding\n                    true_labels_flat.append(id2label[true_id.item()])\n                    pred_labels_flat.append(id2label[pred_id])\n\n# Token-level classification report\nprint(\"Token-level classification report:\")\nprint(classification_report(true_labels_flat, pred_labels_flat, digits=4, zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-18T18:52:05.420768Z","iopub.execute_input":"2025-08-18T18:52:05.421326Z","iopub.status.idle":"2025-08-18T18:54:05.569072Z","shell.execute_reply.started":"2025-08-18T18:52:05.421302Z","shell.execute_reply":"2025-08-18T18:54:05.568147Z"}},"outputs":[{"name":"stdout","text":"Token-level classification report:\n              precision    recall  f1-score   support\n\n      B-DATE     0.9506    0.9484    0.9495     10397\n       B-LOC     0.9026    0.9159    0.9092     45861\n      B-TIME     0.9060    0.9312    0.9184      1439\n      I-DATE     0.9468    0.9570    0.9518      8437\n       I-LOC     0.8330    0.8381    0.8356     23790\n      I-TIME     0.9340    0.9615    0.9476      1870\n           O     0.9933    0.9925    0.9929   1162839\n\n    accuracy                         0.9860   1254633\n   macro avg     0.9238    0.9349    0.9293   1254633\nweighted avg     0.9861    0.9860    0.9860   1254633\n\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}