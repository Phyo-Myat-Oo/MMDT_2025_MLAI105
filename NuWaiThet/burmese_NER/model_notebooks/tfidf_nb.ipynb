{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efad9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_numeric(token):\n",
    "    \"\"\"\n",
    "    Check if a token is numeric (supports Burmese and Western digits).\n",
    "\n",
    "    Args:\n",
    "        token (str): The token to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the token is numeric, False otherwise.\n",
    "    \"\"\"\n",
    "    burmese_digits = set(\"၁၂၃၄၅၆၇၈၉၀\")\n",
    "    return token.isdigit() or all(char in burmese_digits for char in token)\n",
    "\n",
    "def is_english(token):\n",
    "    \"\"\"\n",
    "    Check if a token consists only of English letters.\n",
    "\n",
    "    Args:\n",
    "        token (str): The token to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the token is English, False otherwise.\n",
    "    \"\"\"\n",
    "    return bool(re.match(r\"^[A-Za-z]+$\", token))\n",
    "\n",
    "def extract_features(sentence, index):\n",
    "    \"\"\"\n",
    "    Extract features for a token in a sentence for NER classification.\n",
    "\n",
    "    Args:\n",
    "        sentence (List[Tuple[str, str, str]]): List of (token, pos, ner) tuples.\n",
    "        index (int): Index of the token in the sentence.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of extracted features for the token.\n",
    "    \"\"\"\n",
    "    token = sentence[index][0]\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': token[0],\n",
    "        'prefix-2': token[:2],\n",
    "        'prefix-3': token[:3],\n",
    "        'prefix-4': token[:4],\n",
    "        'prefix-5': token[:5],\n",
    "        'suffix-1': token[-1],\n",
    "        'suffix-2': token[-2:],\n",
    "        'suffix-3': token[-3:],\n",
    "        'suffix-4': token[-4:],\n",
    "        'suffix-5': token[-5:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in token,\n",
    "        'is_numeric': is_numeric(token),\n",
    "        'is_english': is_english(token)\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# ---------- Convert token features to single string ----------\n",
    "def token_to_string(features):\n",
    "    \"\"\"\n",
    "    Combine token and features into a single string for TF-IDF\n",
    "    \"\"\"\n",
    "    s = features['word']\n",
    "    s += ' pre1_' + features['prefix-1']\n",
    "    s += ' pre2_' + features['prefix-2']\n",
    "    s += ' pre3_' + features['prefix-3']\n",
    "    s += ' suf1_' + features['suffix-1']\n",
    "    s += ' suf2_' + features['suffix-2']\n",
    "    s += ' suf3_' + features['suffix-3']\n",
    "    s += ' prev_' + features['prev_word']\n",
    "    s += ' next_' + features['next_word']\n",
    "    s += ' hyphen_' + str(features['has_hyphen'])\n",
    "    s += ' numeric_' + str(features['is_numeric'])\n",
    "    s += ' english_' + str(features['is_english'])\n",
    "    return s\n",
    "\n",
    "# ---------- Prepare data for TF-IDF + NB ----------\n",
    "def prepare_data(conll_data):\n",
    "    \"\"\"\n",
    "    conll_data: list of sentences, each sentence = list of tuples (token, pos, ner)\n",
    "    Returns:\n",
    "        X_strings: list of token feature strings\n",
    "        y_labels: list of NER labels\n",
    "    \"\"\"\n",
    "    X_strings = []\n",
    "    y_labels = []\n",
    "\n",
    "    for sentence in conll_data:\n",
    "        for i in range(len(sentence)):\n",
    "            feats = extract_features(sentence, i)\n",
    "            X_strings.append(token_to_string(feats))\n",
    "            y_labels.append(sentence[i][2])  # NER label\n",
    "\n",
    "    return X_strings, y_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953ac35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll(file_path):\n",
    "    \"\"\"\n",
    "    Load a .conll file and return a list of sentences.\n",
    "    Each sentence is a list of tuples: (token, pos, ner)\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): path to .conll file\n",
    "    \n",
    "    Returns:\n",
    "        List[List[Tuple[str, str, str]]]: list of sentences\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 3:\n",
    "                    token, pos, ner = parts[0], parts[1], parts[2]\n",
    "                    sentence.append((token, pos, ner))\n",
    "                else:\n",
    "                    # Handle lines with missing columns\n",
    "                    token = parts[0]\n",
    "                    pos = parts[1] if len(parts) > 1 else 'X'\n",
    "                    ner = parts[2] if len(parts) > 2 else 'O'\n",
    "                    sentence.append((token, pos, ner))\n",
    "    # Append last sentence if file does not end with blank line\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91a41bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE       0.72      0.55      0.62      5317\n",
      "       B-LOC       0.63      0.33      0.43     21967\n",
      "      B-TIME       0.96      0.26      0.41      1159\n",
      "      I-DATE       0.51      0.63      0.56      8583\n",
      "       I-LOC       0.43      0.17      0.24     16053\n",
      "      I-TIME       0.94      0.03      0.06      1472\n",
      "           O       0.97      0.99      0.98    993540\n",
      "\n",
      "    accuracy                           0.95   1048091\n",
      "   macro avg       0.74      0.42      0.47   1048091\n",
      "weighted avg       0.95      0.95      0.95   1048091\n",
      "\n",
      "Training and prediction took 5.1861 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_dir = \"../datasets/3entity_annotated_ner_cleaned.conll\"\n",
    "sentences = load_conll(input_dir)  # list of sentences, each sentence = [(token,pos,ner), ...]\n",
    "\n",
    "# Prepare data\n",
    "X_strings, y_labels = prepare_data(sentences)\n",
    "\n",
    "# TF-IDF + Naive Bayes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_strings, y_labels, test_size=0.4, random_state=42,stratify=y_labels)\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), lowercase=False)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "start_time = time.time()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "y_pred = nb.predict(X_test_tfidf)\n",
    "end_time = time.time()\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "print(f\"Training and prediction took {end_time - start_time:.4f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
