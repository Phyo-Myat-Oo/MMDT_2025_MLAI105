{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc0fdc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from TorchCRF import CRF\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592dc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1. Load CoNLL file\n",
    "# -----------------------\n",
    "def load_conll(file_path):\n",
    "    sentences, labels = [], []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentence, ner_tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(ner_tags)\n",
    "                    sentence, ner_tags = [], []\n",
    "            else:\n",
    "                parts = line.split(\"\\t\")\n",
    "                if len(parts) != 3:\n",
    "                    continue\n",
    "                token, _, ner = parts\n",
    "                sentence.append(token)\n",
    "                ner_tags.append(ner)\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(ner_tags)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e837b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2. Load FastText embeddings\n",
    "# -----------------------\n",
    "ft_model = KeyedVectors.load(\"fasttext_gensim.model\")  # pretrained FastText\n",
    "\n",
    "# -----------------------\n",
    "# 3. Build vocabs\n",
    "# -----------------------\n",
    "\"\"\"\n",
    "Word vocab → converts words → indices for embedding lookup.\n",
    "Char vocab → converts characters → indices for CharCNN embeddings.\n",
    "NER tag vocab → converts tags → indices for CRF.\n",
    "\"\"\"\n",
    "train_sents, train_labels = load_conll(\"ner_80train.conll\")\n",
    "test_sents, test_labels = load_conll(\"ner_20test.conll\")\n",
    "\n",
    "# Word vocab\n",
    "vocab = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for sent in train_sents:\n",
    "    for w in sent:\n",
    "        if w not in vocab:\n",
    "            vocab[w] = len(vocab)\n",
    "\n",
    "# Char vocab\n",
    "char_vocab = {\"<PAD>\":0, \"<UNK>\":1}\n",
    "for sent in train_sents:\n",
    "    for w in sent:\n",
    "        for c in w:\n",
    "            if c not in char_vocab:\n",
    "                char_vocab[c] = len(char_vocab)\n",
    "\n",
    "# Tag vocab\n",
    "ner_tag_to_ix = {\"<PAD>\":0}\n",
    "for tag_seq in train_labels:\n",
    "    for t in tag_seq:\n",
    "        if t not in ner_tag_to_ix:\n",
    "            ner_tag_to_ix[t] = len(ner_tag_to_ix)\n",
    "id2tag = {v:k for k,v in ner_tag_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e02bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4. Dataset and collate_fn\n",
    "# -----------------------\n",
    "max_word_len = 10  # max chars per word\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for NER with word-level and character-level representations.\n",
    "    Steps:\n",
    "        1. Stores tokenized sentences, labels, and vocabularies (word, char, tag).\n",
    "        2. Converts each sentence into:\n",
    "        - Word indices (for word embeddings)\n",
    "        - Character indices per word (for CharCNN/CharLSTM)\n",
    "        - Label indices (for CRF)\n",
    "        3. Pads characters per word up to `max_word_len`.\n",
    "        4. Returns tensors: word indices, char indices, and label indices for a single sentence.\n",
    "    Purpose:\n",
    "        - Provides a consistent way to access and convert raw NER sentences into tensors \n",
    "        suitable for a BiLSTM+CharCNN+CRF model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentences, labels, vocab, char_vocab, ner_tag_to_ix, max_word_len=10):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.char_vocab = char_vocab\n",
    "        self.ner_tag_to_ix = ner_tag_to_ix\n",
    "        self.max_word_len = max_word_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.sentences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Word indices\n",
    "        word_idx = [self.vocab.get(w, self.vocab[\"<UNK>\"]) for w in sent]\n",
    "\n",
    "        # Char indices\n",
    "        char_idx = []\n",
    "        for w in sent:\n",
    "            chars = [self.char_vocab.get(c, self.char_vocab[\"<UNK>\"]) for c in w][:self.max_word_len]\n",
    "            # pad chars\n",
    "            chars += [0]*(self.max_word_len - len(chars))\n",
    "            char_idx.append(chars)\n",
    "\n",
    "        # Label indices\n",
    "        label_idx = [self.ner_tag_to_ix[t] for t in label]\n",
    "\n",
    "        return torch.tensor(word_idx, dtype=torch.long), torch.tensor(char_idx, dtype=torch.long), torch.tensor(label_idx, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "        Collate function for batching sentences with dynamic padding.\n",
    "\n",
    "        Steps:\n",
    "        1. Receives a batch of sentences from NERDataset (word indices, char indices, tag indices).\n",
    "        2. Finds the length of the longest sentence in the batch.\n",
    "        3. Pads all sentences, character sequences, and tags to this maximum length.\n",
    "        - Words → pad with 0\n",
    "        - Characters → pad with 0 arrays of shape (max_word_len)\n",
    "        - Tags → pad with 0\n",
    "        4. Stacks padded tensors into batch tensors.\n",
    "\n",
    "        Returns:\n",
    "        - batch_words: Tensor of shape (batch_size, max_seq_len)\n",
    "        - batch_chars: Tensor of shape (batch_size, max_seq_len, max_word_len)\n",
    "        - batch_tags: Tensor of shape (batch_size, max_seq_len)\n",
    "\n",
    "        Purpose:\n",
    "        - Ensures all sequences in a batch have the same length for model input,\n",
    "        while preserving both word-level and character-level information.\n",
    "    \"\"\"\n",
    "\n",
    "    words, chars, tags = zip(*batch)\n",
    "    max_len = max(len(s) for s in words)\n",
    "    batch_words, batch_chars, batch_tags = [], [], []\n",
    "\n",
    "    for w, c, t in zip(words, chars, tags):\n",
    "        # pad words\n",
    "        pad_len = max_len - len(w)\n",
    "        batch_words.append(torch.cat([w, torch.zeros(pad_len, dtype=torch.long)]))\n",
    "        # pad chars\n",
    "        pad_chars = torch.zeros((pad_len, max_word_len), dtype=torch.long)\n",
    "        batch_chars.append(torch.cat([c, pad_chars], dim=0))\n",
    "        # pad tags\n",
    "        batch_tags.append(torch.cat([t, torch.zeros(pad_len, dtype=torch.long)]))\n",
    "\n",
    "    return torch.stack(batch_words), torch.stack(batch_chars), torch.stack(batch_tags)\n",
    "\n",
    "# -----------------------\n",
    "# 5. CharCNN + BiLSTM + CRF Model\n",
    "# -----------------------\n",
    "\"\"\"\n",
    "BiLSTM-CRF model with CharCNN for NER.\n",
    "\n",
    "Steps:\n",
    "    1. Word embeddings: map words to pretrained FastText vectors (with dropout)\n",
    "    2. Character embeddings: map characters to vectors, apply CNN + max-pooling + dropout\n",
    "    3. Combine word and char embeddings into a single representation\n",
    "    4. BiLSTM: contextualize each token using bidirectional LSTM\n",
    "    5. Linear layer: project LSTM outputs to tag scores\n",
    "    6. CRF: model valid tag sequences and compute loss or decode predictions\n",
    "\"\"\"\n",
    "\n",
    "class BiLSTM_CRF_CharCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, tagset_size,\n",
    "                 ft_model, char_vocab_size, char_embedding_dim=30,\n",
    "                 char_out_channels=30, max_word_len=10, dropout=0.1):\n",
    "        super(BiLSTM_CRF_CharCNN, self).__init__()\n",
    "\n",
    "        # Word embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        emb_weights = np.zeros((vocab_size, embedding_dim))\n",
    "        for w, idx in vocab.items():\n",
    "            if w in ft_model.wv:\n",
    "                emb_weights[idx] = ft_model.wv[w]\n",
    "            else:\n",
    "                emb_weights[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "        self.embedding.weight.data.copy_(torch.tensor(emb_weights, dtype=torch.float32))\n",
    "\n",
    "        self.word_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Char embedding + CNN\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim, padding_idx=0) # padding character is ignored\n",
    "        self.char_cnn = nn.Conv1d(char_embedding_dim, char_out_channels, kernel_size=3, padding=1) # keeps output length same as input length (so every char has a feature).\n",
    "        self.char_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # BiLSTM\n",
    "        self.bilstm = nn.LSTM(embedding_dim + char_out_channels,\n",
    "                              hidden_dim//2,\n",
    "                              num_layers=1,\n",
    "                              bidirectional=True,\n",
    "                              batch_first=True)\n",
    "        self.lstm_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Linear + CRF\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "        self.max_word_len = max_word_len\n",
    "\n",
    "    def forward(self, words, chars, tags=None, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for training or inference.\n",
    "\n",
    "        Steps:\n",
    "            1. Get word embeddings and apply dropout\n",
    "            2. Get char embeddings for each word, apply CNN + max-pooling + dropout\n",
    "            3. Concatenate word and char embeddings\n",
    "            4. Pass through BiLSTM + dropout\n",
    "            5. Project to tag scores via linear layer\n",
    "            6. If tags provided:\n",
    "                - compute CRF loss\n",
    "                - The CRF computes the log-likelihood of the correct tag sequence given the emission scores.\n",
    "                - But PyTorch minimizes loss, so we take the negative log-likelihood when calculating gradient descent.\n",
    "            Else:\n",
    "                - decode best tag sequence using CRF\n",
    "        \"\"\"\n",
    "\n",
    "        # Word embeddings\n",
    "        word_embeds = self.word_dropout(self.embedding(words))  # [B,L,E]\n",
    "\n",
    "        # Char embeddings + CNN\n",
    "        B, L, W = chars.size() # batch size, sentence length (#words), max characters per word\n",
    "        chars_flat = chars.view(B*L, W) # Flatten batch + sentence dims so we can process all words in the batch together.\n",
    "        char_embeds = self.char_embedding(chars_flat)  # [B*L, W, C], Map char indices → embeddings\n",
    "        char_embeds = char_embeds.transpose(1,2)       # [B*L, C, W], nn.Conv1d expects (batch, channels, seq_len), so we swap dim 1 and 2\n",
    "        char_cnn_out = F.relu(self.char_cnn(char_embeds)) # [B*L, char_out_channels, W]\n",
    "        char_cnn_out, _ = torch.max(char_cnn_out, dim=2) # Max pooling over the character dimension ,[B*L, char_out_channels]\n",
    "        char_cnn_out = self.char_dropout(char_cnn_out)\n",
    "        char_cnn_out = char_cnn_out.view(B, L, -1) # Reshape back to [batch_size, seq_len, char_out_channels], to be concatenated with word embeddings for BiLSTM input\n",
    "\n",
    "        # Combine\n",
    "        combined = torch.cat([word_embeds, char_cnn_out], dim=2)\n",
    "\n",
    "        # BiLSTM + dropout\n",
    "        lstm_out, _ = self.bilstm(combined)\n",
    "        lstm_out = self.lstm_dropout(lstm_out)\n",
    "\n",
    "        # Linear\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:\n",
    "            loss = -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            return self.crf.decode(emissions, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb3290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 3937.7961\n",
      "Epoch 2/5, Loss: 1666.0485\n",
      "Epoch 3/5, Loss: 1316.8349\n",
      "Epoch 4/5, Loss: 1120.6721\n",
      "Epoch 5/5, Loss: 992.3842\n",
      "Training completed in 3356.47 seconds\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 6. Hyperparameters\n",
    "# -----------------------\n",
    "embedding_dim = ft_model.vector_size\n",
    "hidden_dim = 128\n",
    "char_embedding_dim = 30\n",
    "char_out_channels = 30\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "lr = 0.001\n",
    "max_word_len = 10\n",
    "\n",
    "# -----------------------\n",
    "# 7. DataLoader\n",
    "# -----------------------\n",
    "train_dataset = NERDataset(train_sents, train_labels, vocab, char_vocab, ner_tag_to_ix, max_word_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = NERDataset(test_sents, test_labels, vocab, char_vocab, ner_tag_to_ix, max_word_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# -----------------------\n",
    "# 8. Initialize model\n",
    "# -----------------------\n",
    "model = BiLSTM_CRF_CharCNN(len(vocab), embedding_dim, hidden_dim, len(ner_tag_to_ix),\n",
    "                           ft_model, len(char_vocab), char_embedding_dim, char_out_channels,\n",
    "                           max_word_len, dropout=0.2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)  # ✅ weight decay\n",
    "\n",
    "# -----------------------\n",
    "# 9. Training\n",
    "# -----------------------\n",
    "start_time = time.time()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for Xw, Xc, y in train_loader:\n",
    "        mask = (Xw != 0)  # padding mask, ignore padded positions during loss computation.\n",
    "        optimizer.zero_grad() # Clears old gradients before computing new ones.\n",
    "        loss = model(Xw, Xc, tags=y, mask=mask)\n",
    "        loss.backward() # Computes gradients of loss w.r.t. model parameters.\n",
    "        # keeps gradients within a reasonable range, stabilizing training\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)  # Prevents exploding gradients in LSTM/BiLSTM models.\n",
    "        optimizer.step() # Applies gradient descent to update model parameters.\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{n_epochs}, Loss: {total_loss:.4f}\")\n",
    "print(f\"Training completed in {time.time()-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE     0.9675    0.9319    0.9493      2744\n",
      "       B-LOC     0.9155    0.8880    0.9016     10967\n",
      "      B-TIME     0.9356    0.9386    0.9371       635\n",
      "      I-DATE     0.9679    0.9415    0.9546      4361\n",
      "       I-LOC     0.8332    0.8392    0.8362      8158\n",
      "      I-TIME     0.9301    0.9543    0.9420       809\n",
      "           O     0.9945    0.9954    0.9950    500371\n",
      "\n",
      "    accuracy                         0.9899    528045\n",
      "   macro avg     0.9349    0.9270    0.9308    528045\n",
      "weighted avg     0.9898    0.9899    0.9899    528045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 10. Evaluation on test set\n",
    "# -----------------------\n",
    "model.eval()\n",
    "all_true, all_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for Xw, Xc, y in test_loader:\n",
    "        mask = (Xw != 0)\n",
    "        preds = model(Xw, Xc, mask=mask)\n",
    "        for i in range(len(preds)):\n",
    "            length = mask[i].sum().item()\n",
    "            all_pred.extend([id2tag[p] for p in preds[i][:length]])\n",
    "            all_true.extend([id2tag[t.item()] for t in y[i][:length]])\n",
    "\n",
    "print(\"NER Classification Report:\")\n",
    "print(classification_report(all_true, all_pred, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe34a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vocabs saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 11. Save model + vocabs\n",
    "# -----------------------\n",
    "torch.save(model.state_dict(), \"bilstm_crf_charcnn_fasttext.pth\")\n",
    "with open(\"word_vocab.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab,f,ensure_ascii=False,indent=2)\n",
    "with open(\"char_vocab.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(char_vocab,f,ensure_ascii=False,indent=2)\n",
    "with open(\"tag2id.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(ner_tag_to_ix,f,ensure_ascii=False,indent=2)\n",
    "print(\"Model and vocabs saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
