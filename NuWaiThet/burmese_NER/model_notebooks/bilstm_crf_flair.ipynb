{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "245691cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1Ô∏è‚É£ Imports\n",
    "# -----------------------\n",
    "# ===============================\n",
    "# 1Ô∏è‚É£ Imports\n",
    "# ===============================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "\n",
    "# ===============================\n",
    "# 2Ô∏è‚É£ Load CoNLL Data\n",
    "# ===============================\n",
    "def load_conll(path):\n",
    "    sentences, labels = [], []\n",
    "    sent, lab = [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # sentence boundary\n",
    "                if sent:\n",
    "                    sentences.append(sent)\n",
    "                    labels.append(lab)\n",
    "                    sent, lab = [], []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    token, tag = parts[0], parts[-1]\n",
    "                    sent.append(token)\n",
    "                    lab.append(tag)\n",
    "        if sent:  # last sentence\n",
    "            sentences.append(sent)\n",
    "            labels.append(lab)\n",
    "    return sentences, labels\n",
    "\n",
    "train_texts, train_labels = load_conll(\"ner_train.conll\")\n",
    "val_texts, val_labels = load_conll(\"ner_val.conll\")\n",
    "test_texts, test_labels = load_conll(\"ner_test.conll\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "049ea62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER tag vocab: {'B-DATE': 1, 'B-LOC': 2, 'B-TIME': 3, 'I-DATE': 4, 'I-LOC': 5, 'I-TIME': 6, 'O': 7, 'PAD': 0}\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 3Ô∏è‚É£ Build tag vocab\n",
    "# ===============================\n",
    "all_labels = train_labels + val_labels + test_labels\n",
    "unique_tags = sorted(set(tag for seq in all_labels for tag in seq))\n",
    "ner_tag_to_ix = {tag: i+1 for i, tag in enumerate(unique_tags)}  # 0 = PAD\n",
    "ner_tag_to_ix[\"PAD\"] = 0\n",
    "id2tag = {i: t for t, i in ner_tag_to_ix.items()}\n",
    "tagset_size = len(ner_tag_to_ix)\n",
    "\n",
    "print(\"NER tag vocab:\", ner_tag_to_ix)\n",
    "\n",
    "# ===============================\n",
    "# 4Ô∏è‚É£ Flair Embeddings\n",
    "# ===============================\n",
    "stacked_embeddings = StackedEmbeddings([\n",
    "    WordEmbeddings(\"glove\"),\n",
    "    FlairEmbeddings(\"news-forward\"),\n",
    "    FlairEmbeddings(\"news-backward\"),\n",
    "])\n",
    "embedding_dim = stacked_embeddings.embedding_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37c8afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5Ô∏è‚É£ Dataset\n",
    "# ===============================\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, embeddings, tag_to_ix, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.embeddings = embeddings\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.texts[idx]\n",
    "        tags = self.labels[idx]\n",
    "\n",
    "        # Flair Sentence (use given tokens, no re-tokenization)\n",
    "        sentence = Sentence(\" \".join(tokens), use_tokenizer=False)\n",
    "        self.embeddings.embed(sentence)\n",
    "\n",
    "        emb_list, tag_list = [], []\n",
    "        for tok, gold_tag in zip(sentence, tags):\n",
    "            emb_list.append(tok.embedding.detach().cpu().numpy())\n",
    "            tag_list.append(self.tag_to_ix[gold_tag])\n",
    "\n",
    "        # Pad or truncate\n",
    "        if len(emb_list) < self.max_len:\n",
    "            pad_len = self.max_len - len(emb_list)\n",
    "            emb_list.extend([[0.0]*self.embeddings.embedding_length]*pad_len)\n",
    "            tag_list.extend([self.tag_to_ix[\"PAD\"]]*pad_len)\n",
    "        else:\n",
    "            emb_list = emb_list[:self.max_len]\n",
    "            tag_list = tag_list[:self.max_len]\n",
    "\n",
    "        return torch.tensor(emb_list, dtype=torch.float32), torch.tensor(tag_list, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc4e8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  11%|‚ñà         | 302/2869 [2:19:12<19:43:18, 27.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     59\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     61\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     62\u001b[0m     mask \u001b[38;5;241m=\u001b[39m (y \u001b[38;5;241m!=\u001b[39m ner_tag_to_ix[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPAD\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[15], line 21\u001b[0m, in \u001b[0;36mNERDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Flair Sentence (use given tokens, no re-tokenization)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m sentence \u001b[38;5;241m=\u001b[39m Sentence(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens), use_tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m emb_list, tag_list \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok, gold_tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentence, tags):\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\flair\\embeddings\\token.py:97\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[1;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[0;32m     94\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sentences]\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings:\n\u001b[1;32m---> 97\u001b[0m     \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\flair\\embeddings\\base.py:51\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[1;34m(self, data_points)\u001b[0m\n\u001b[0;32m     48\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\flair\\embeddings\\token.py:808\u001b[0m, in \u001b[0;36mFlairEmbeddings._add_embeddings_internal\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    805\u001b[0m end_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# get hidden states from language model\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_chunk\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune:\n\u001b[0;32m    813\u001b[0m     all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m all_hidden_states_in_lm\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\flair\\models\\language_model.py:160\u001b[0m, in \u001b[0;36mLanguageModel.get_representation\u001b[1;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m    159\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 160\u001b[0m     rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     output_parts\u001b[38;5;241m.\u001b[39mappend(rnn_output)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# concatenate all chunks to make final output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\flair\\models\\language_model.py:88\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[1;34m(self, input, hidden, ordered_sequence_lengths, decode)\u001b[0m\n\u001b[0;32m     86\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (h,)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(output)\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 6Ô∏è‚É£ DataLoader\n",
    "# ===============================\n",
    "batch_size = 16\n",
    "train_dataset = NERDataset(train_texts, train_labels, stacked_embeddings, ner_tag_to_ix)\n",
    "val_dataset   = NERDataset(val_texts, val_labels, stacked_embeddings, ner_tag_to_ix)\n",
    "test_dataset  = NERDataset(test_texts, test_labels, stacked_embeddings, ner_tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ===============================\n",
    "# 7Ô∏è‚É£ BiLSTM + CRF Model\n",
    "# ===============================\n",
    "class BiLSTM_CRF_Flair(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size):\n",
    "        super(BiLSTM_CRF_Flair, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "        # CRF parameters\n",
    "        self.transitions = nn.Parameter(torch.randn(tagset_size, tagset_size))\n",
    "        self.transitions.data[:, ner_tag_to_ix[\"PAD\"]] = -10000\n",
    "        self.transitions.data[ner_tag_to_ix[\"PAD\"], :] = -10000\n",
    "\n",
    "    def forward(self, x, tags=None, mask=None):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "\n",
    "        if tags is not None:  # training\n",
    "            loss = self.neg_log_likelihood(emissions, tags, mask)\n",
    "            return loss\n",
    "        else:  # inference\n",
    "            return self.decode(emissions, mask)\n",
    "\n",
    "    def neg_log_likelihood(self, emissions, tags, mask):\n",
    "        # Simplified: use cross-entropy (for demo, replace with full CRF loss if needed)\n",
    "        emissions = emissions.view(-1, emissions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        return nn.CrossEntropyLoss(ignore_index=ner_tag_to_ix[\"PAD\"])(emissions, tags)\n",
    "\n",
    "    def decode(self, emissions, mask):\n",
    "        return torch.argmax(emissions, dim=-1)\n",
    "\n",
    "# ===============================\n",
    "# 8Ô∏è‚É£ Training\n",
    "# ===============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM_CRF_Flair(embedding_dim, 128, tagset_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 3\n",
    "start_time = time.time()\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs}\"):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        mask = (y != ner_tag_to_ix[\"PAD\"])\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(X, tags=y, mask=mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch}/{n_epochs}, Loss: {total_loss:.4f}\")\n",
    "print(f\"Training completed in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64199d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 9Ô∏è‚É£ Evaluation\n",
    "# ===============================\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        mask = (y != ner_tag_to_ix[\"PAD\"])\n",
    "        preds = model(X, mask=mask)\n",
    "        correct += ((preds == y) & mask).sum().item()\n",
    "        total += mask.sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# üîü Save model + vocab\n",
    "# ===============================\n",
    "torch.save(model.state_dict(), \"bilstm_crf_flair.pth\")\n",
    "import pickle\n",
    "with open(\"ner_tag_to_ix.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ner_tag_to_ix, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c212e146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
