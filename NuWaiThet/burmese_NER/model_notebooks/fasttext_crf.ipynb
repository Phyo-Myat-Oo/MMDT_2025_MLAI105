{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f160b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pretrained FastText embeddings using own courpus\n",
    "from gensim.models import FastText\n",
    "\n",
    "def load_conll_tokens(file_path):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                token = line.strip().split()[0]  # Take token only\n",
    "                current_sentence.append(token)\n",
    "            else:\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    return sentences\n",
    "\n",
    "# Load tokenized sentences from your .conll\n",
    "tokenized_sentences = load_conll_tokens(\"../datasets/3entity_annotated_ner_cleaned.conll\")\n",
    "\n",
    "\n",
    "\n",
    "# Train FastText model\n",
    "# ft_model = FastText(\n",
    "#     sentences=tokenized_sentences,\n",
    "#     vector_size=300,\n",
    "#     window=5,\n",
    "#     min_count=1,\n",
    "#     workers=4\n",
    "# )\n",
    "\n",
    "# # Save the model\n",
    "# ft_model.save(\"fasttext_gensim.model\")\n",
    "\n",
    "# # Example: Get vector for a token\n",
    "# print(ft_model.wv[\"မြန်မာ\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad24c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 71711\n",
      "Total tokens in corpus: 2620226\n",
      "Unique words in corpus: 25505\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of sentences into a single list of words\n",
    "all_tokens = [word for sent in tokenized_sentences for word in sent]\n",
    "print(f\"Total sentences: {len(tokenized_sentences)}\")\n",
    "\n",
    "print(f\"Total tokens in corpus: {len(all_tokens)}\")\n",
    "# Count unique words\n",
    "unique_words = set(all_tokens)\n",
    "\n",
    "print(f\"Unique words in corpus: {len(unique_words)}\")\n",
    "#print(f\"Vocab size in FastText: {len(ft_model.wv)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a81c141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 25505\n",
      "Vector dimension: 300\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load FastText model ----------\n",
    "ft_model = FastText.load(\"fasttext_gensim.model\")\n",
    "print(f\"Vocab size: {len(ft_model.wv)}\")\n",
    "print(f\"Vector dimension: {ft_model.wv.vector_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4945c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 700.60 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE      0.946     0.948     0.947      2609\n",
      "       B-LOC      0.918     0.862     0.889     11068\n",
      "      B-TIME      0.952     0.924     0.938       595\n",
      "      I-DATE      0.957     0.951     0.954      4205\n",
      "       I-LOC      0.838     0.764     0.800      8097\n",
      "      I-TIME      0.945     0.925     0.935       729\n",
      "           O      0.993     0.996     0.994    499009\n",
      "\n",
      "    accuracy                          0.989    526312\n",
      "   macro avg      0.936     0.910     0.922    526312\n",
      "weighted avg      0.988     0.989     0.988    526312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "def load_conll(file_path):\n",
    "    \"\"\"\n",
    "    Load a CoNLL file and return a list of sentences for NER.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CoNLL file.\n",
    "\n",
    "    Returns:\n",
    "        List[List[Tuple[str, str, str]]]: List of sentences, each sentence is a list of (token, pos, ner) tuples.\n",
    "    \"\"\"\n",
    "    sentences, sentence = [], []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    token, pos, ner = parts[0], parts[1], parts[2]\n",
    "                else:\n",
    "                    token, pos, ner = parts[0], \"X\", \"O\"\n",
    "                sentence.append((token, pos, ner))\n",
    "            else:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def is_numeric(token):\n",
    "    \"\"\"\n",
    "    Check if a token is numeric (supports Burmese and Western digits).\n",
    "\n",
    "    Args:\n",
    "        token (str): The token to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the token is numeric, False otherwise.\n",
    "    \"\"\"\n",
    "    burmese_digits = \"၁၂၃၄၅၆၇၈၉၀\"\n",
    "    return token.isdigit() or all(c in burmese_digits for c in token)\n",
    "\n",
    "def is_english(token):\n",
    "    \"\"\"\n",
    "    Check if a token consists only of English letters.\n",
    "\n",
    "    Args:\n",
    "        token (str): The token to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the token is English, False otherwise.\n",
    "    \"\"\"\n",
    "    return bool(re.match(r\"^[A-Za-z]+$\", token))\n",
    "\n",
    "def word2features(sentence, index, ft_model):\n",
    "    \"\"\"\n",
    "    Extract features for a token in a sentence, including FastText embedding average.\n",
    "\n",
    "    Args:\n",
    "        sentence (List[Tuple[str, str, str]]): List of (token, pos, ner) tuples.\n",
    "        index (int): Index of the token in the sentence.\n",
    "        ft_model (FastText): Pretrained FastText model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of extracted features for the token.\n",
    "    \"\"\"\n",
    "    token = sentence[index][0]\n",
    "    features = {\n",
    "        'word': token,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prefix-1': token[0],\n",
    "        'prefix-2': token[:2],\n",
    "        'prefix-3': token[:3],\n",
    "        'prefix-4': token[:4],\n",
    "        'prefix-5': token[:5],\n",
    "        'suffix-1': token[-1],\n",
    "        'suffix-2': token[-2:],\n",
    "        'suffix-3': token[-3:],\n",
    "        'suffix-4': token[-4:],\n",
    "        'suffix-5': token[-5:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1][0],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1][0],\n",
    "        'has_hyphen': '-' in token,\n",
    "        'is_numeric': is_numeric(token),\n",
    "        'is_english': is_english(token),\n",
    "    }\n",
    "    # Add FastText average embedding value\n",
    "    if token in ft_model.wv:\n",
    "        vec = ft_model.wv[token]  # vector for this token, to save time and to convert 1D scaler \n",
    "        features['ft_avg'] = float(np.mean(vec)) # average value of its dimensions\n",
    "    else:\n",
    "        features['ft_avg'] = 0.0\n",
    "    return features\n",
    "\n",
    "def sent2features(sentence, ft_model):\n",
    "    \"\"\"\n",
    "    Convert a sentence to a list of feature dicts for each token.\n",
    "\n",
    "    Args:\n",
    "        sentence (List[Tuple[str, str, str]]): List of (token, pos, ner) tuples.\n",
    "        ft_model (FastText): Pretrained FastText model.\n",
    "\n",
    "    Returns:\n",
    "        List[dict]: List of feature dicts for each token.\n",
    "        [ {feature_dict_of_word1}, {feature_dict_of_word2}, ... ]\n",
    "    \"\"\"\n",
    "    return [word2features(sentence, i, ft_model) for i in range(len(sentence))]\n",
    "\n",
    "def sent2labels(sentence):\n",
    "    \"\"\"\n",
    "    Extract NER labels from a sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (List[Tuple[str, str, str]]): List of (token, pos, ner) tuples.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of NER labels for the sentence.\n",
    "    \"\"\"\n",
    "    return [ner for _, _, ner in sentence]\n",
    "\n",
    "# ---------- Load FastText model ----------\n",
    "ft_model = FastText.load(\"fasttext_gensim.model\")\n",
    "\n",
    "# ---------- Load data ----------\n",
    "file_path = \"../datasets/3entity_annotated_ner_cleaned.conll\"\n",
    "data = load_conll(file_path)\n",
    "\n",
    "# ---------- Convert data to features & labels ----------\n",
    "X = [sent2features(s, ft_model) for s in data]\n",
    "y = [sent2labels(s) for s in data]\n",
    "\n",
    "# ---------- Split train/test ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------- Train CRF ----------\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "start_time = time.time()\n",
    "crf.fit(X_train, y_train)\n",
    "print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# ---------- Predict & report ----------\n",
    "y_pred = crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dcab872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the CRF model\n",
    "with open(\"fasttext_crf_ner_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(crf, f)\n",
    "\n",
    "print(\"CRF model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196dd8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\tf-env\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\tf-env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'c2': 0.01, 'c1': 0.46415888336127775}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE      0.949     0.946     0.947      5410\n",
      "       B-LOC      0.915     0.856     0.884     22067\n",
      "      B-TIME      0.943     0.913     0.928      1151\n",
      "      I-DATE      0.959     0.950     0.954      8733\n",
      "       I-LOC      0.830     0.764     0.796     16252\n",
      "      I-TIME      0.937     0.910     0.924      1406\n",
      "           O      0.993     0.996     0.994    998005\n",
      "\n",
      "    accuracy                          0.988   1053024\n",
      "   macro avg      0.932     0.905     0.918   1053024\n",
      "weighted avg      0.988     0.988     0.988   1053024\n",
      "\n",
      "Best CRF model saved to crf_fasttext_tuned.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "# ---------- Base CRF ----------\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "# ---------- Hyperparameter tuning ----------\n",
    "params_space = {\n",
    "    'c1': np.logspace(-3, 0, 10),  # L1\n",
    "    'c2': np.logspace(-3, 0, 10),  # L2\n",
    "}\n",
    "\n",
    "# Use weighted F1 score\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_iter=10,\n",
    "                        scoring='f1_weighted',\n",
    "                        n_jobs=-1,\n",
    "                        random_state=42)\n",
    "rs.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", rs.best_params_)\n",
    "best_crf = rs.best_estimator_\n",
    "\n",
    "# ---------- Predict & evaluate ----------\n",
    "y_pred = best_crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# ---------- Save model ----------\n",
    "import pickle\n",
    "with open(\"crf_fasttext_tuned.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_crf, f)\n",
    "print(\"Best CRF model saved to crf_fasttext_tuned.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
